<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Life in a Nutshell</title><link href="https://wugh.github.io/" rel="alternate"></link><link href="https://wugh.github.io/feeds/main.atom.xml" rel="self"></link><id>https://wugh.github.io/</id><updated>2016-03-13T21:22:00+08:00</updated><entry><title>CS224d笔记4续——RNN隐藏层计算之GRU和LSTM</title><link href="https://wugh.github.io/posts/2016/03/cs224d-notes4-recurrent-neural-networks-continue/" rel="alternate"></link><published>2016-03-13T21:22:00+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2016-03-13:posts/2016/03/cs224d-notes4-recurrent-neural-networks-continue/</id><summary type="html">&lt;p&gt;本篇文章主要介绍两种&lt;span class="caps"&gt;RNN&lt;/span&gt;的隐藏层信息计算方法&lt;span class="caps"&gt;GRU&lt;/span&gt;（Gated Recurrent Units）和
&lt;span class="caps"&gt;LSTM&lt;/span&gt;（Long-Short-Term-Memories），这两种隐藏层的计算方法通过引入门（Gate）
的机制来解决&lt;span class="caps"&gt;RNN&lt;/span&gt;的梯度消失问题，从而学习到长距离依赖。&lt;/p&gt;
&lt;p&gt;这里说的隐藏层计算方法指的是如何计算下个时刻的隐藏层信息，标准&lt;span class="caps"&gt;RNN&lt;/span&gt;中计算&amp;nbsp;方法是：
&lt;/p&gt;
&lt;div class="math"&gt;$$h_t=f(W^{(hh)}h_{t-1}+W^{(hx)}x_t)$$&lt;/div&gt;
&lt;p&gt;
而&lt;span class="caps"&gt;LSTM&lt;/span&gt;和&lt;span class="caps"&gt;GRU&lt;/span&gt;可以理解为计算&lt;span class="math"&gt;\(h_t\)&lt;/span&gt;的另一种方法。&lt;/p&gt;
&lt;h2&gt;&lt;span class="caps"&gt;LSTM&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;这篇文章&lt;/a&gt;详细 地解释了&lt;span class="caps"&gt;LSTM&lt;/span&gt;各个门的物理含义，
以及&lt;span class="caps"&gt;LSTM&lt;/span&gt;计算隐藏层的方法，这里简要的进行总结，
下图是&lt;span class="caps"&gt;LSTM&lt;/span&gt;网络的示意图，图中各个符号的含义参考&lt;a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;原文&lt;/a&gt;：
&lt;img alt="LSTM RNN" src="https://wugh.github.io/images/NLP/rnn-lstm-chain.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;p&gt;某个时刻&lt;span class="caps"&gt;LSTM&lt;/span&gt;计算隐藏层的方法参考下图：
&lt;img alt="LSTM 单元" src="https://wugh.github.io/images/NLP/rnn-lstm-unit-detail.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;LSTM&lt;/span&gt;最核心的部分是&lt;code&gt;cell state&lt;/code&gt;，即图中的&lt;span class="math"&gt;\(c_t\)&lt;/span&gt;。&lt;span class="math"&gt;\(c_t\)&lt;/span&gt;的信息贯穿整个&lt;span class="caps"&gt;LSTM&lt;/span&gt;，
在整个前向传播的过程中只在&lt;span class="math"&gt;\(c_t\)&lt;/span&gt;上进行一些简单的线性操作，通过门来控制
&lt;span class="math"&gt;\(c_t\)&lt;/span&gt;中信息的增减。&lt;span class="caps"&gt;LSTM&lt;/span&gt;中的门是通过一个sigmoid层来实现的，门输出的数值在
0~1之间，然后把门的取值向量和目标数据对应维相乘就可以达到控制数据流通的
效果。&lt;span class="caps"&gt;LSTM&lt;/span&gt;中有三个门，分别是&lt;code&gt;forget gate&lt;/code&gt;、&lt;code&gt;input gate&lt;/code&gt;和&lt;code&gt;output gate&lt;/code&gt;，
这三个门的计算方法公式一样，都是根据&lt;span class="math"&gt;\(x_t\)&lt;/span&gt;和&lt;span class="math"&gt;\(h_{t-1}\)&lt;/span&gt;来计算，&amp;nbsp;区别在于权重矩阵和偏置不同。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先是&lt;code&gt;forget gate&lt;/code&gt;&lt;span class="math"&gt;\(f_t\)&lt;/span&gt;，这个门主要控制要从&lt;code&gt;cell state&lt;/code&gt;中忘记哪些信息，计算方法如下:
&lt;div class="math"&gt;$$f_t=\sigma(W^fx_t+U^fh_{t-1})$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;其次是&lt;code&gt;input gate&lt;/code&gt;&lt;span class="math"&gt;\(i_t\)&lt;/span&gt;，这个门控制当前时刻的新信息（candidate hidden layer）有哪些需要添加进&lt;code&gt;cell state&lt;/code&gt;中，计算方法如下:
&lt;div class="math"&gt;$$i_t=\sigma(W^ix_t+U^ih_{t-1})$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;上一步提到的当前时刻新信息的计算方法如下：
&lt;div class="math"&gt;$$\tilde{c}_t=\tanh(W^cx_t+U^ch_{t-1})$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;然后&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻&lt;code&gt;cell state&lt;/code&gt;中的信息就变成&lt;span class="math"&gt;\(c_{t-1}\)&lt;/span&gt;中的部分信息再叠加上&lt;span class="math"&gt;\(\tilde{c}_t\)&lt;/span&gt;中的部分信息，计算方法如下：
&lt;div class="math"&gt;$$c_t=f_t\circ c_{t-1} + i_t\circ\tilde{c}_t$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;最后还需要根据&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻的&lt;code&gt;cell state&lt;/code&gt;输出&lt;span class="math"&gt;\(h_t\)&lt;/span&gt;，通过&lt;code&gt;output gate&lt;/code&gt;来控制&lt;code&gt;cell state&lt;/code&gt;中的哪些信息需要
输出，&lt;code&gt;output gate&lt;/code&gt;的计算方法如下：
&lt;div class="math"&gt;$$o_t=\sigma(W^ox_t+U^oh_{t-1})$$&lt;/div&gt;
将&lt;code&gt;cell state&lt;/code&gt;中的信息经过一个&lt;span class="math"&gt;\(\tanh\)&lt;/span&gt;层之后然后经过&lt;code&gt;output gate&lt;/code&gt;过滤得到&lt;span class="math"&gt;\(h_t\)&lt;/span&gt;：
&lt;div class="math"&gt;$$h_t=o_t\circ\tanh(c_t)$$&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果我们把&lt;span class="caps"&gt;LSTM&lt;/span&gt;的&lt;code&gt;forget gate&lt;/code&gt;全部置0（总是忘记之前的信息），&lt;code&gt;input gate&lt;/code&gt;全部
置1，&lt;code&gt;output gate&lt;/code&gt;全部置1（把&lt;code&gt;cell state&lt;/code&gt;中的信息全部输出），这样&lt;span class="caps"&gt;LSTM&lt;/span&gt;就变成一个标准的&lt;span class="caps"&gt;RNN&lt;/span&gt;。
&lt;a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;上文提到的文章&lt;/a&gt;中还提到一些&lt;span class="caps"&gt;LSTM&lt;/span&gt;的变种，比如根据&lt;span class="math"&gt;\(h_{t-1}\)&lt;/span&gt;、&lt;span class="math"&gt;\(x_t\)&lt;/span&gt;和&lt;span class="math"&gt;\(c_t\)&lt;/span&gt;来计算门信息。&lt;/p&gt;
&lt;h2&gt;&lt;span class="caps"&gt;GRU&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class="caps"&gt;GRU&lt;/span&gt;可以看成是&lt;span class="caps"&gt;LSTM&lt;/span&gt;的变种，&lt;span class="caps"&gt;GRU&lt;/span&gt;把&lt;span class="caps"&gt;LSTM&lt;/span&gt;中的&lt;code&gt;forget gate&lt;/code&gt;和&lt;code&gt;input gate&lt;/code&gt;用&lt;code&gt;update gate&lt;/code&gt;来替代。
把&lt;code&gt;cell state&lt;/code&gt;和隐状态&lt;span class="math"&gt;\(h_t\)&lt;/span&gt;进行合并，在计算当前时刻新信息的方法和&lt;span class="caps"&gt;LSTM&lt;/span&gt;有所不同。
下图是&lt;span class="caps"&gt;GRU&lt;/span&gt;更新&lt;span class="math"&gt;\(h_t\)&lt;/span&gt;的过程：
&lt;img alt="GRU" src="https://wugh.github.io/images/NLP/rnn-gru-unit.png" style="display:block;margin:0 auto" /&gt;&amp;nbsp;具体更新过程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先介绍&lt;span class="caps"&gt;GRU&lt;/span&gt;的两个门，分别是&lt;code&gt;reset gate&lt;/code&gt;&lt;span class="math"&gt;\(r_t\)&lt;/span&gt;和&lt;code&gt;update gate&lt;/code&gt;&lt;span class="math"&gt;\(z_t\)&lt;/span&gt;，计算方法和&lt;span class="caps"&gt;LSTM&lt;/span&gt;中
门的计算方法一致：
&lt;div class="math"&gt;$$\begin{align*}
r_t&amp;amp;=\sigma(W^rx_t+U^rh_{t-1})\\
z_t&amp;amp;=\sigma(W^zx_t+U^zh_{t-1})
\end{align*}$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;其次是计算候选隐藏层（candidate hidden layer）&lt;span class="math"&gt;\(\tilde{h}_t\)&lt;/span&gt;，这个候选隐藏层
和&lt;span class="caps"&gt;LSTM&lt;/span&gt;中的&lt;span class="math"&gt;\(\tilde{c}_t\)&lt;/span&gt;是类似，可以看成是当前时刻的新信息，其中&lt;span class="math"&gt;\(r_t\)&lt;/span&gt;用来控制需要
保留多少之前的记忆，如果&lt;span class="math"&gt;\(r_t\)&lt;/span&gt;为0，那么&lt;span class="math"&gt;\(\tilde{h}_t\)&lt;/span&gt;只包含当前词的信息：
&lt;div class="math"&gt;$$\tilde{h}_t=\tanh(Wx_t+r_tUh_{t-1})$$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;最后&lt;span class="math"&gt;\(z_t\)&lt;/span&gt;控制需要从前一时刻的隐藏层&lt;span class="math"&gt;\(h_{t-1}\)&lt;/span&gt;中遗忘多少信息，需要加入多少当前
时刻的隐藏层信息&lt;span class="math"&gt;\(\tilde{h}_t\)&lt;/span&gt;，最后得到&lt;span class="math"&gt;\(h_t\)&lt;/span&gt;，直接得到最后输出的隐藏层信息，
这里与&lt;span class="caps"&gt;LSTM&lt;/span&gt;的区别是&lt;span class="caps"&gt;GRU&lt;/span&gt;中没有&lt;code&gt;output gate&lt;/code&gt;：
&lt;div class="math"&gt;$$h_t=z_t\circ h_{t-1} + (1-z_t)\circ \tilde{h}_t$$&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果&lt;code&gt;reset gate&lt;/code&gt;接近0，那么之前的隐藏层信息就会丢弃，允许模型丢弃一些和未来无关
的信息；&lt;code&gt;update gate&lt;/code&gt;控制当前时刻的隐藏层输出&lt;span class="math"&gt;\(h_t\)&lt;/span&gt;需要保留多少之前的隐藏层信息，
若&lt;span class="math"&gt;\(z_t\)&lt;/span&gt;接近1相当于我们之前把之前的隐藏层信息拷贝到当前时刻，可以学习长距离依赖。
一般来说那些具有短距离依赖的单元&lt;code&gt;reset gate&lt;/code&gt;比较活跃（如果&lt;span class="math"&gt;\(r_t\)&lt;/span&gt;为1，而&lt;span class="math"&gt;\(z_t\)&lt;/span&gt;为0
那么相当于变成了一个标准的&lt;span class="caps"&gt;RNN&lt;/span&gt;，能处理短距离依赖），具有长距离依赖的单元&lt;code&gt;update gate&lt;/code&gt;比较活跃。&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;LSTM&lt;/span&gt;和&lt;span class="caps"&gt;RNN&lt;/span&gt;的&lt;code&gt;thenao&lt;/code&gt;实现可以参考&lt;a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/"&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="神经网络"></category><category term="RNN"></category><category term="循环神经网络"></category><category term="深度学习"></category><category term="GRU"></category><category term="LSTM"></category></entry><entry><title>CS224d笔记4——语言模型和循环神经网络（Recurrent Neural Network, RNN）</title><link href="https://wugh.github.io/posts/2016/03/cs224d-notes4-recurrent-neural-networks/" rel="alternate"></link><published>2016-03-05T11:13:00+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2016-03-05:posts/2016/03/cs224d-notes4-recurrent-neural-networks/</id><summary type="html">&lt;p&gt;这部分首先介绍语言模型，通过分析传统语言模型的问题引入循环神经网络（Recurrent Neural Network, &lt;span class="caps"&gt;RNN&lt;/span&gt;）以及
&lt;span class="caps"&gt;RNN&lt;/span&gt;的扩展（双向&lt;span class="caps"&gt;RNN&lt;/span&gt;，深度&lt;span class="caps"&gt;RNN&lt;/span&gt;），最后为了改善&lt;span class="caps"&gt;RNN&lt;/span&gt;对长距依存信息的捕获引入&lt;span class="caps"&gt;GRU&lt;/span&gt;（Gated Recurrent Units）和
&lt;span class="caps"&gt;LSTM&lt;/span&gt;（Long-Short-Term-Memories）。&lt;/p&gt;
&lt;h2&gt;语言模型&lt;/h2&gt;
&lt;p&gt;这里将介绍传统的n-gram语言模型，然后简单介绍神经概率语言模型，最后总结两者的问题所在引出循环神经网络。&lt;/p&gt;
&lt;p&gt;语言模型通常用来衡量一个单词序列出现的概率。假设词序列为&lt;span class="math"&gt;\(\{w_1,\ldots,w_T\}\)&lt;/span&gt;，
那么把这个序列出现的概率记为&lt;span class="math"&gt;\(P(w_1,\ldots,w_T)\)&lt;/span&gt;，计算词序列的概率在机器翻译中有着重要用途，
机器翻译需要给各个候选的词序列打分，这个分数可以用这个概率来衡量。
&lt;span class="math"&gt;\(P(w_1,\ldots,w_T)\)&lt;/span&gt;的计算可以用链式法则展开：
&lt;/p&gt;
&lt;div class="math"&gt;$$P(w_1,\ldots,w_T)=\Pi_{i=1}^{i=T}P(w_i|w_1,\ldots,w_{i-1})$$&lt;/div&gt;
&lt;p&gt;
这个概率估计起来比较复杂（因为&lt;span class="math"&gt;\(w_i\)&lt;/span&gt;出现的概率依赖于&lt;span class="math"&gt;\(w_i\)&lt;/span&gt;之前所有的词），
一般进行马尔可夫假设（假设&lt;span class="math"&gt;\(w_i\)&lt;/span&gt;出现的概率仅依赖&lt;span class="math"&gt;\(w_i\)&lt;/span&gt;之前的&lt;span class="math"&gt;\(n-1\)&lt;/span&gt;个词，称为为n-gram）。
&lt;/p&gt;
&lt;div class="math"&gt;$$P(w_1,\ldots,w_T)=\Pi_{i=1}^{i=T}P(w_i|w_1,\ldots,w_{i-1})=\Pi_{i=1}^{i=T}P(w_i|w_{i-(n-1)},\ldots,w_{i-1})$$&lt;/div&gt;
&lt;p&gt;
一般来说通过语料我们就可以估计出n-gram的概率，下面分别是bigram（2-gram）和trigram（3-gram）的概率计算方法：
&lt;img alt="n-gram计算" src="https://wugh.github.io/images/NLP/rnn-language-model-bigram-trigram.png" style="display:block;margin:0 auto" /&gt;
&lt;span class="math"&gt;\(n\)&lt;/span&gt;越大估计出来的序列的概率越准确，但是这种方法需要大量的内存，假设我们不做任何优化且词表大小为&lt;span class="math"&gt;\(|V|\)&lt;/span&gt;，
那么对于n-gram，需要一个&lt;span class="math"&gt;\(|V|^n\)&lt;/span&gt;规模的矩阵来存储，需要的内存随着&lt;span class="math"&gt;\(n\)&lt;/span&gt;增大指数增长，一般5-gram已经是非常大的规模（Google提供）。&lt;/p&gt;
&lt;p&gt;Bengio等人利用神经网络来表示语言模型，在语言模型的训练过程中可以得到单词的分布式表示，具体的神经概率语言模型图如下：
&lt;img alt="神经语言模型" src="https://wugh.github.io/images/NLP/rnn-neural-language-model.png" style="display:block;margin:0 auto" /&gt;
该网络的主要思想是用前&lt;span class="math"&gt;\(n-1\)&lt;/span&gt;个词的向量来估计当前词的概率，具体公式为：
&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}=\textit{softmax}(W^{(2)}\tanh(W^{(1)}+b{^{(1)}})+W^{(3)}+b^{(3)})$$&lt;/div&gt;
&lt;p&gt;可以看出传统语言模型在估计概率的时候需要固定&lt;span class="math"&gt;\(n\)&lt;/span&gt;的大小，否则无法统计概率&lt;span class="math"&gt;\(P(w_i|w_{i-(n-1)},\ldots,w_{i-1})\)&lt;/span&gt;。
而&lt;span class="caps"&gt;RNN&lt;/span&gt;的最大优势在于可以统计&lt;span class="math"&gt;\(P(w_i|w_1,\ldots,w_{i-1})\)&lt;/span&gt;的概率。&lt;/p&gt;
&lt;h2&gt;循环神经网络（Recurrent Neural Network, &lt;span class="caps"&gt;RNN&lt;/span&gt;）&lt;/h2&gt;
&lt;p&gt;&lt;span class="caps"&gt;RNN&lt;/span&gt;的优势在于当前词的概率依赖于之前出现的所有词，并且需要的内存并不会随着依赖的上下文长度增加而指数增长，
需要的内存和词表大小规模相关。&lt;span class="caps"&gt;RNN&lt;/span&gt;的网络结构图如下：
&lt;img alt="RNN结构" src="https://wugh.github.io/images/NLP/rnn-structure.png" style="display:block;margin:0 auto" /&gt;
可以看出&lt;span class="caps"&gt;RNN&lt;/span&gt;是一个序列模型，比较符合认知上人理解句子的顺序（从左到右）。在时刻&lt;span class="math"&gt;\(t\)&lt;/span&gt;，将&lt;span class="math"&gt;\(t-1\)&lt;/span&gt;
时刻的隐藏层输出&lt;span class="math"&gt;\(h_{t-1}\)&lt;/span&gt;和当前时刻的词向量&lt;span class="math"&gt;\(x_t\)&lt;/span&gt;输入隐藏层之后得到&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻的特征表示&lt;span class="math"&gt;\(h_t\)&lt;/span&gt;，
然后用这个特征表示得到&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻的预测输出&lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;。本节的重点是将&lt;span class="caps"&gt;RNN&lt;/span&gt;应用于语言模型，
下面来详细描述&lt;span class="caps"&gt;RNN&lt;/span&gt;语言模型中用到的参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x_1,\ldots,x_{t-1},x_t,x_{t+1},\ldots,x_T\)&lt;/span&gt;：&lt;span class="math"&gt;\(T\)&lt;/span&gt;长度的单词序列中每个时刻的词对应的词向量。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(h_t=\sigma(W^{(hh)}h_{t-1}+W^{(hx)}x_t)\)&lt;/span&gt;：如何计算&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻的隐藏层输出：&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x_t\in\mathbb{R}^d\)&lt;/span&gt;：&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻的单词对应的词向量&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(W^{(hx)}\in\mathbb{R}^{D_h\times d}\)&lt;/span&gt;：连接当前隐藏层和&lt;span class="math"&gt;\(x_t\)&lt;/span&gt;的权重矩阵&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(W^{(hh)}\in\mathbb{R}^{D_h\times D_h}\)&lt;/span&gt;：连接当前隐藏和前一个时刻隐藏层输出&lt;span class="math"&gt;\(h_{t-1}\)&lt;/span&gt;的权重矩阵&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(h_{t-1}\in\mathbb{R}^{D_h}\)&lt;/span&gt;：表示&lt;span class="math"&gt;\(t-1\)&lt;/span&gt;时刻隐藏层的输出，&lt;span class="math"&gt;\(h_0\in\mathbb{R}^{D_h}\)&lt;/span&gt;表示&lt;span class="math"&gt;\(t=0\)&lt;/span&gt;时刻随机初始化的隐藏层输出向量&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\sigma()\)&lt;/span&gt;：表示神经元使用的非线性函数为sigmoid，当然也可以采用tanh等函数&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\hat{y}_t=\text{softmax}(W^{(S)}h_t)\)&lt;/span&gt;：根据&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻的隐藏层输出进行softmax分类得到整个词汇表上的概率分布，
这里的&lt;span class="math"&gt;\(\hat{y}_t\)&lt;/span&gt;是对下个时刻（&lt;span class="math"&gt;\(t+1\)&lt;/span&gt;）词的预测，即根据已经给定的上下文信息&lt;span class="math"&gt;\(h_{t-1}\)&lt;/span&gt;和当前观测到的词向量&lt;span class="math"&gt;\(x_t\)&lt;/span&gt;
来预测&lt;span class="math"&gt;\(t+1\)&lt;/span&gt;时刻的词。其中&lt;span class="math"&gt;\(W^{(S)}\in\mathbb{R}^{|V|\times D_h}\)&lt;/span&gt;，&lt;span class="math"&gt;\(\hat{y}_t\in \mathbb{R}^{|V|}\)&lt;/span&gt;，&lt;span class="math"&gt;\(|V|\)&lt;/span&gt;为词表大小。&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\hat{P}(x_{t+1}=v_j|x_t,\ldots,x_1)=\hat{y}_{t,j}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class="caps"&gt;RNN&lt;/span&gt;语言模型中非常关键的一点是每个时刻采用的&lt;span class="math"&gt;\(W\)&lt;/span&gt;矩阵都是一个，所以参数规模不会随着依赖上下文的长度增加而指数增长。
通常来说采用交叉熵作为损失函数，那么在&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻的损失为：
&lt;/p&gt;
&lt;div class="math"&gt;$$J^{(t)}(\theta)=-\sum_{j=1}^{|V|}y_{t,j}\log \hat{y}_{t,j}$$&lt;/div&gt;
&lt;p&gt;
在一个长度为&lt;span class="math"&gt;\(T\)&lt;/span&gt;的序列上的总交叉熵为：
&lt;/p&gt;
&lt;div class="math"&gt;$$J=-\frac{1}{T}\sum_{t=1}^T J^{(t)}(\theta)=-\frac{1}{T}\sum_{t=1}^T \sum_{j=1}^{|V|}y_{t,j}\log \hat{y}_{t,j}$$&lt;/div&gt;
&lt;p&gt;
用来衡量语言模型的一个常用指标是困惑度（perplexity），困惑度越低表示预测下个词的置信度越高，困惑度和交叉熵的关系如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{Perplexity}=2^J$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="caps"&gt;RNN&lt;/span&gt;语言模型需要的内存正比于序列长度，因为需要将这个序列的词向量存储在内存中。&lt;/p&gt;
&lt;h2&gt;&lt;span class="caps"&gt;RNN&lt;/span&gt;训练&lt;/h2&gt;
&lt;p&gt;其实&lt;span class="caps"&gt;RNN&lt;/span&gt;本质上还是一个普通的多层神经网络，只是层与层之间使用的是同一个权重矩阵而已，
同样可以利用&lt;a href="https://wugh.github.io/posts/2016/03/cs224d-notes3-neural-networks-and-backward-propagation/"&gt;上一篇文章&lt;/a&gt;介绍的后向误差传播的原理来进行后向误差传播，
只需要把&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻的误差一直传播到&lt;span class="math"&gt;\(t=0\)&lt;/span&gt;时刻，但是在实际实现的时候一般只需要向后传播&lt;span class="math"&gt;\(\tau\approx 3-5\)&lt;/span&gt;个时间单位。&lt;/p&gt;
&lt;p&gt;对于&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻需要求的梯度如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial{J^{(t)}}}{\partial{W^{(S)}}}\qquad{}
\left. \frac{\partial{J^{(t)}}}{\partial{W^{(hh)}}}\right|_t\qquad{}
\left. \frac{\partial{J^{(t)}}}{\partial{W^{(hx)}}}\right|_t\qquad{}
\frac{\partial{J^{(t)}}}{\partial{x_t}}
$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(|_t\)&lt;/span&gt;表示求&lt;span class="math"&gt;\(t\)&lt;/span&gt;某个参数的梯度，因为&lt;span class="math"&gt;\(W^{^(hh)}\)&lt;/span&gt;和&lt;span class="math"&gt;\(W^{(hx)}\)&lt;/span&gt;两个参数在不同时刻用到，&amp;nbsp;需要求出不同时刻两个矩阵的梯度，然后进行累加。&lt;/p&gt;
&lt;p&gt;对于&lt;span class="math"&gt;\(t-1,\ldots,t-(\tau - 1)\)&lt;/span&gt;时刻需要求的梯度如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\left. \frac{\partial{J^{(t)}}}{\partial{W^{(hh)}}}\right|_{t-s}\qquad{}
\left. \frac{\partial{J^{(t)}}}{\partial{W^{(hx)}}}\right|_{t-s}\qquad{}
\frac{\partial{J^{(t)}}}{\partial{x_{t-s}}}
$$&lt;/div&gt;
&lt;p&gt;这里以&lt;span class="caps"&gt;RNN&lt;/span&gt;语言模型为例，将&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻的&lt;span class="caps"&gt;RNN&lt;/span&gt;展开一个多层神经网络，其中包含一个softmax层，进行后向传播，示意图如下：
&lt;img alt="RNN误差传播" src="https://wugh.github.io/images/NLP/rnn-gradient-bp.jpg" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;p&gt;假设&lt;span class="math"&gt;\(a_t=W^{(S)}h_t\)&lt;/span&gt;，那么&lt;span class="math"&gt;\(\hat{y}_t=\text{softmax}(a_t)\)&lt;/span&gt;，根据&lt;a href="https://wugh.github.io/posts/2016/02/cs224d-notes2-softmax-classification-and-window-classification/"&gt;之前文章&lt;/a&gt;可知
&lt;span class="math"&gt;\(J^{(t)}\)&lt;/span&gt;相对于&lt;span class="math"&gt;\(a_t\)&lt;/span&gt;的梯度为：
&lt;/p&gt;
&lt;div class="math"&gt;$$\nabla_{a_t}J^{(t)}=\hat{y}_t-y_t=\delta_t^{(S)}$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(J^{(t)}\)&lt;/span&gt;相对于&lt;span class="math"&gt;\(W^{(S)}\)&lt;/span&gt;的梯度如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial{J^{(t)}}}{\partial{W^{(S)}}}=\delta_t^{(S)}(h_t)^T$$&lt;/div&gt;
&lt;p&gt;假设&lt;span class="math"&gt;\(z_t=W^{(hh)}h_{t-1}+W^{(hx)}x_t\)&lt;/span&gt;，那么&lt;span class="math"&gt;\(h_t=\sigma(z_t)\)&lt;/span&gt;，现在我们需要把&lt;span class="math"&gt;\(a_t\)&lt;/span&gt;处的误差传播到&lt;span class="math"&gt;\(z_t\)&lt;/span&gt;处，
其实就是&lt;a href="https://wugh.github.io/posts/2016/03/cs224d-notes3-neural-networks-and-backward-propagation/"&gt;上一篇文章&lt;/a&gt;如何从&lt;span class="math"&gt;\(\delta^{(t+1)}\)&lt;/span&gt;得到&lt;span class="math"&gt;\(\delta^{(t)}\)&lt;/span&gt;，
&lt;span class="math"&gt;\(z_t\)&lt;/span&gt;的误差如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$\nabla_{z_t}J^{(t)}=(W^{(S)})^T\delta_t^{(S)}\circ \sigma'(z_t)=\delta_t$$&lt;/div&gt;
&lt;p&gt;
类似地可知&lt;span class="math"&gt;\(J^{(t)}\)&lt;/span&gt;相对于&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻&lt;span class="math"&gt;\(W^{(hh)}\)&lt;/span&gt;的梯度如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$\left. \frac{\partial{J^{(t)}}}{\partial{W^{(hh)}}}\right|_t=\delta_t (h_{t-1})^T$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(J^{(t)}\)&lt;/span&gt;相对于&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻&lt;span class="math"&gt;\(W^{(hx)}\)&lt;/span&gt;的梯度如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$\left. \frac{\partial{J^{(t)}}}{\partial{W^{(hx)}}}\right|_t=\delta_t (x_{t})^T$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(J^{(t)}\)&lt;/span&gt;相对于&lt;span class="math"&gt;\(x_t\)&lt;/span&gt;的梯度如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial{J^{(t)}}}{\partial{x_t}}=(W^{(hx)})^T\delta_t$$&lt;/div&gt;
&lt;p&gt;对于&lt;span class="math"&gt;\(t-s\)&lt;/span&gt;时刻来说，首先我们要得到&lt;span class="math"&gt;\(z_{t-s}\)&lt;/span&gt;处的误差，计算如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation}
\delta_{t-s}=(W^{(hh)})^T\delta_{t-s+1}\circ \sigma'(z_{t-s})
\label{eq:delta_bp}
\end{equation}$$&lt;/div&gt;
&lt;p&gt;
得到&lt;span class="math"&gt;\(\delta_{t-s}\)&lt;/span&gt;，求解
&lt;span class="math"&gt;\(\left. \frac{\partial{J^{(t)}}}{\partial{W^{(hh)}}}\right|_{t-s}\)&lt;/span&gt;，
&lt;span class="math"&gt;\(\left. \frac{\partial{J^{(t)}}}{\partial{W^{(hx)}}}\right|_{t-s}\)&lt;/span&gt;和
&lt;span class="math"&gt;\(\frac{\partial{J^{(t)}}}{\partial{x_{t-s}}}\)&lt;/span&gt;
都和&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻一致，直接表示成&lt;span class="math"&gt;\(\delta_{t-s}\)&lt;/span&gt;的式子。&lt;/p&gt;
&lt;h2&gt;梯度消失（vanishing gradient）和梯度爆炸（exploding&amp;nbsp;gradient）&lt;/h2&gt;
&lt;p&gt;当我们把&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻的误差往后传播的时候就可以得到&lt;span class="math"&gt;\(J^{(t)}\)&lt;/span&gt;对于各个参数的梯度，
在这个后向传播的过程中我们为了方便计算&lt;span class="math"&gt;\(t-s\)&lt;/span&gt;时刻的梯度引入了&lt;span class="math"&gt;\(\delta_{t-s}\)&lt;/span&gt;，
式&lt;span class="math"&gt;\(\ref{eq:delta_bp}\)&lt;/span&gt;是根据&lt;span class="math"&gt;\(\delta_{t-s+1}\)&lt;/span&gt;计算&lt;span class="math"&gt;\(\delta_{t-s}\)&lt;/span&gt;的公式，可以改写成：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\delta_{t-s}=
 \begin{pmatrix}
  {f'(z_{t-s})}_1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
  0 &amp;amp; {f'(z_{t-s})}_2 &amp;amp; \cdots &amp;amp; 0 \\
  \vdots  &amp;amp; \vdots  &amp;amp; \ddots &amp;amp; \vdots  \\
  0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; {f'(z_{t-s})}_{D_h}
 \end{pmatrix}
(W^{(hh)})^T\delta_{t-s+1}
$$&lt;/div&gt;
&lt;p&gt;
简化之后写成下面式子
&lt;/p&gt;
&lt;div class="math"&gt;$$
\delta_{t-s}=
  \text{diag}[f'(z_{t-s})]
(W^{(hh)})^T\delta_{t-s+1}
$$&lt;/div&gt;
&lt;p&gt;
假设&lt;span class="math"&gt;\({(W^{(hh)})}^T\)&lt;/span&gt;和&lt;span class="math"&gt;\(\text{diag}[f'(z_{t-s})]\)&lt;/span&gt;的范数上限分别为&lt;span class="math"&gt;\(\beta_W\)&lt;/span&gt;和&lt;span class="math"&gt;\(\beta_z\)&lt;/span&gt;，那么：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\lVert \delta_{t-s} \rVert\leq \lVert \text{diag}[f'(z_{t-s})]\rVert \lVert (W^{(hh)})^T\rVert \lVert \delta_{t-s+1}\rVert
\leq \beta_W \beta_z\lVert \delta_{t-s+1}\rVert
$$&lt;/div&gt;
&lt;p&gt;
那么可以看出&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻&lt;span class="math"&gt;\(z_t\)&lt;/span&gt;处的误差&lt;span class="math"&gt;\(\delta_t\)&lt;/span&gt;后向传播到&lt;span class="math"&gt;\(t-s\)&lt;/span&gt;时刻&lt;span class="math"&gt;\(\delta_{t-s}\)&lt;/span&gt;的误差有下面关系：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\lVert \delta_{t-s} \rVert \leq {(\beta_W \beta_z)}^s \lVert \delta_{t}\rVert
$$&lt;/div&gt;
&lt;p&gt;
可以看出&lt;span class="math"&gt;\(\delta_{t-s}\)&lt;/span&gt;随着&lt;span class="math"&gt;\(s\)&lt;/span&gt;增大会迅速增大或者减小，这种现象称为梯度爆炸或梯度消失。&lt;/p&gt;
&lt;p&gt;以语言模型为例来看一下梯度消失带来的问题，本来引入&lt;span class="caps"&gt;RNN&lt;/span&gt;的目的是为了能够表示长距离的上下文信息。
例如下面两个句子，我们要用已有的上下文信息预测句子的最后一个词：
&lt;img alt="梯度消失例子" src="https://wugh.github.io/images/NLP/rnn-gradient-vanishing-example.png" style="display:block;margin:0 auto" /&gt;
可以看出根据上下文信息，这个句子的答案应该都是“John”。理想状态下&lt;span class="caps"&gt;RNN&lt;/span&gt;应该也应该能够根据上下文中信息预测出下个词应该是“John”，
因为这个词在之前在上下文中出现过。但是实际情况是对于第一个句子&lt;span class="caps"&gt;RNN&lt;/span&gt;更容易预测正确，这是因为在后向误差传播过程中，
梯度传播到比较早的时刻就逐渐消失了。对于较长的句子（第二个句子），与答案有关的信息距离最后那个时刻比较远，&amp;nbsp;对预测答案的贡献很小，所以容易出错。&lt;/p&gt;
&lt;p&gt;为了解决梯度爆炸问题，Mikolov提出了一种启发式的解决方法，当梯度超过一个阈值的时候，将梯度裁剪到一个合适的值，算法流程如下：
&lt;img alt="clip gradient" src="https://wugh.github.io/images/NLP/rnn-clip-gradients.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;p&gt;对于梯度消失问题，这里介绍两个方法。第一个方法是将&lt;span class="math"&gt;\(W^{(hh)}\)&lt;/span&gt;初始化称一个单位矩；第二个方法是使用ReLU神经元，&amp;nbsp;因为这个神经元的局部梯度要不是0就是1，这样后向传播经过神经元的时候误差不会变小。&lt;/p&gt;
&lt;h2&gt;双向&lt;span class="caps"&gt;RNN&lt;/span&gt;和深度双向&lt;span class="caps"&gt;RNN&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;单向&lt;span class="caps"&gt;RNN&lt;/span&gt;的问题在于&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻进行分类的时候只能利用&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻之前的信息，
但是在&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻进行分类的时候可能也需要利用未来时刻的信息。双向&lt;span class="caps"&gt;RNN&lt;/span&gt;（bi-directional &lt;span class="caps"&gt;RNN&lt;/span&gt;）模型正是为了解决这个问题，
双向&lt;span class="caps"&gt;RNN&lt;/span&gt;在任意时刻&lt;span class="math"&gt;\(t\)&lt;/span&gt;都保持两个隐藏层，一个隐藏层用于从左往右的信息传播记作&lt;span class="math"&gt;\(\overrightarrow{h}_t\)&lt;/span&gt;，
另一个隐藏层用于从右往左的信息传播记作&lt;span class="math"&gt;\(\overleftarrow{h}_t\)&lt;/span&gt;。
双向&lt;span class="caps"&gt;RNN&lt;/span&gt;模型需要的内存是单向&lt;span class="caps"&gt;RNN&lt;/span&gt;的两倍，因为在每个时间点需要保存两个隐藏层，还需要保存从右往左方向的相关权重矩阵。
在&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻进行分类的时候同时使用两个隐藏层的信息作为输入进行分类，下图为网络结果示意：
&lt;img alt="双向RNN" src="https://wugh.github.io/images/NLP/bi-directional-rnn.png" style="display:block;margin:0 auto" /&gt;
下面是&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻两个隐藏层的计算方法，两个公式的区别仅仅在于计算方向不同：
&lt;img alt="隐藏层计算" src="https://wugh.github.io/images/NLP/bi-directional-rnn-hidden-layer.png" style="display:block;margin:0 auto" /&gt;
最后利用&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻的两个隐藏层信息进行分类，如下式：
&lt;img alt="双向RNN分类" src="https://wugh.github.io/images/NLP/bi-directional-rnn-classification.png" style="display:block;margin:0 auto" /&gt;
此时式中的&lt;span class="math"&gt;\(h_t\)&lt;/span&gt;同时表示了词左边和右边的上下文信息。&lt;/p&gt;
&lt;p&gt;有了单层的双向&lt;span class="caps"&gt;RNN&lt;/span&gt;之后，可以构建深度双向&lt;span class="caps"&gt;RNN&lt;/span&gt;，下图为多层双向&lt;span class="caps"&gt;RNN&lt;/span&gt;的网络结构图：
&lt;img alt="双向RNN" src="https://wugh.github.io/images/NLP/deep-bi-directional-rnn.png" style="display:block;margin:0 auto" /&gt;
可以看出&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻计算第&lt;span class="math"&gt;\(i\)&lt;/span&gt;层从左往右传播的隐藏层信息&lt;span class="math"&gt;\(\overrightarrow{h}_t^{(i)}\)&lt;/span&gt;时，输入有两部分，
第一部分是同一层之前一个时刻的隐藏层信息&lt;span class="math"&gt;\(\overrightarrow{h}_{t-1}^{(i)}\)&lt;/span&gt;，
第二部分是前一层同个时刻的两个方向隐藏层信息&lt;span class="math"&gt;\(h_t^{(i-1)}=[\overrightarrow{h}_{t}^{(i-1)};\overleftarrow{h}_{t}^{(i-1)}]\)&lt;/span&gt;。
下面给出第&lt;span class="math"&gt;\(i\)&lt;/span&gt;层&lt;span class="math"&gt;\(t\)&lt;/span&gt;时刻两个隐藏层的计算方法：
&lt;img alt="深度隐藏层计算" src="https://wugh.github.io/images/NLP/deep-bi-directional-rnn-hidden-layer.png" style="display:block;margin:0 auto" /&gt;
多层的双向&lt;span class="caps"&gt;RNN&lt;/span&gt;利用最后一个隐层的信息进行分类任务，假设深度&lt;span class="caps"&gt;RNN&lt;/span&gt;共有&lt;span class="math"&gt;\(L\)&lt;/span&gt;个&lt;span class="caps"&gt;RNN&lt;/span&gt;层，深度&lt;span class="caps"&gt;RNN&lt;/span&gt;会利用第&lt;span class="math"&gt;\(L\)&lt;/span&gt;层的隐藏层信息进行分类任务：
&lt;img alt="深度双向RNN分类" src="https://wugh.github.io/images/NLP/deep-bi-directional-rnn-classification.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span class="caps"&gt;RNN&lt;/span&gt;的应用&lt;/h2&gt;
&lt;p&gt;&lt;span class="caps"&gt;RNN&lt;/span&gt;是一个序列模型，跟&lt;span class="caps"&gt;CRF&lt;/span&gt;类似，可以应用于大量的序列标注问题上。
例如&lt;a href="http://aclweb.org/anthology/D15-1141"&gt;中文分词&lt;/a&gt;，
命名实体识别，词情感极性标注，&lt;a href="https://www.cs.cornell.edu/~oirsoy/files/emnlp14drnt.pdf"&gt;观点挖掘&lt;/a&gt;，
&lt;a href="http://arxiv.org/abs/1406.1078"&gt;机器翻译&lt;/a&gt;等。
由于普通&lt;span class="caps"&gt;RNN&lt;/span&gt;的隐藏层无法保存长距离的信息，&lt;a href="https://wugh.github.io/posts/2016/03/cs224d-notes4-recurrent-neural-networks-continue/"&gt;下篇文章&lt;/a&gt;将介绍两种新的&lt;span class="caps"&gt;RNN&lt;/span&gt;隐藏层计算方法来缓解这个问题，
一个是&lt;span class="caps"&gt;GRU&lt;/span&gt;，另一个是&lt;span class="caps"&gt;LSTM&lt;/span&gt;。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="神经网络"></category><category term="RNN"></category><category term="循环神经网络"></category><category term="语言模型"></category><category term="深度学习"></category></entry><entry><title>CS224d笔记3——神经网络</title><link href="https://wugh.github.io/posts/2016/03/cs224d-notes3-neural-networks-and-backward-propagation/" rel="alternate"></link><published>2016-03-01T09:00:00+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2016-03-01:posts/2016/03/cs224d-notes3-neural-networks-and-backward-propagation/</id><summary type="html">&lt;p&gt;这部分首先介绍神经元，接着介绍只有一个隐藏层的神经网络，这个神经网络要解决的
问题和&lt;a href="https://wugh.github.io/posts/2016/02/cs224d-notes2-softmax-classification-and-window-classification/"&gt;上一篇文章&lt;/a&gt;类似，这里对窗口分类任务进一步
简化，我们只需要判断中心词是否为地点。利用这个简单的神经网络说明前向传播和
后向误差传播算法，然后推出通用的后向误差传播算法。最后介绍一些神经网络在工程&amp;nbsp;实现中的一些技巧。&lt;/p&gt;
&lt;h2&gt;神经元&lt;/h2&gt;
&lt;p&gt;神经元是组成神经网络的基础单元，神经元通常接受&lt;span class="math"&gt;\(n\)&lt;/span&gt;个输入产生一个输出，运算过程如下图：
&lt;img alt="神经元" src="https://wugh.github.io/images/NLP/neural-network-neuron.png" style="display:block;margin:0 auto" /&gt;
神经元的参数包括一个&lt;span class="math"&gt;\(n\)&lt;/span&gt;维的权重向量&lt;span class="math"&gt;\(w\)&lt;/span&gt;和一个偏置&lt;span class="math"&gt;\(b\)&lt;/span&gt;（&lt;span class="math"&gt;\(b\)&lt;/span&gt;为标量），偏置可以看成判断类别的先验，
神经元的输出为&lt;span class="math"&gt;\(h{w,b}(x)=f(w^Tx + b)\)&lt;/span&gt;，&lt;span class="math"&gt;\(f\)&lt;/span&gt;被成为“激活函数”。本文默认使用的激活函数为sigmoid函数
（用符号&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;表示）：
&lt;/p&gt;
&lt;div class="math"&gt;$$f(z)=\sigma(z)=\frac{1}{1+\exp(-z)}$$&lt;/div&gt;
&lt;p&gt;
更多的激活函数将在后文讨论，这里不再展开，需要注意的是sigmoid函数的求导结果为
&lt;/p&gt;
&lt;div class="math"&gt;$$\sigma'(z)=\sigma(z)(1-\sigma(z))$$&lt;/div&gt;
&lt;h2&gt;单隐藏层神经网络&lt;/h2&gt;
&lt;p&gt;神经网络的特点在于把一个输入向量同时输入到一系列神经元，然后得到新的输出向量，
然后这个新的向量又可以作为下一层一系列神经元的输入，如下图所示：
&lt;img alt="神经网络例子" src="https://wugh.github.io/images/NLP/neural-network-3-layers-example.png" style="display:block;margin:0 auto" /&gt;
我们使用圆圈来表示神经网络的输入，标上“+1”的圆圈被称为偏置节点，也就是截距项。
神经网络最左边的一层叫做输入层，最右的一层叫做输出层（本例中，输出层只有一个节
点）。中间所有节点组成的一层叫做隐藏层，因为我们不能在训练样本集中观测到它们的
值。同时可以看到，以上神经网络的例子中有3个输入单元（偏置单元不计在内），3个隐
藏单元及一个输出单元。本文使用的符号参考&lt;a href="ufldl"&gt;&lt;span class="caps"&gt;UFLDL&lt;/span&gt;教程&lt;/a&gt;，计算隐藏层的过程如下图所示：
&lt;img alt="神经网络记号" src="https://wugh.github.io/images/NLP/neural-network-3-layers-example-notation.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;h2&gt;前向计算&lt;/h2&gt;
&lt;p&gt;可以把神经网络应用于&lt;a href="https://wugh.github.io/posts/2016/02/cs224d-notes2-softmax-classification-and-window-classification/"&gt;上一篇文章&lt;/a&gt;中提到的窗口分
类问题，这里我们主要只是为了用这个简单的神经网络说明前向计算和后向误差传播，所
以将任务限定为只需要判断中心词是否为地点，并且采用的网络只有一个隐藏层。
例如窗口内容为“meseums in Paris are amazing”，判断中心词“Paris”是否为地点，
假设词向量维度为4，那么窗口向量&lt;span class="math"&gt;\(x_{\text{window}}=x\in \mathbb{R}^{20}\)&lt;/span&gt;，
隐藏层用8个神经元，然后用8个神经元的输出来计算得分，最后利用这个得分判断中心词是否为实体。
下图为计算&lt;span class="math"&gt;\(\text{score(meseums in Paris are amazing)}\)&lt;/span&gt;的示意图，
从输入逐层向输出层传播最后得到输出的过程成为前向传播：
&lt;img alt="前向计算" src="https://wugh.github.io/images/NLP/neural-network-3-layers-ner-forward-computation.png" style="display:block;margin:0 auto" /&gt;
通过非线性的隐藏层可以学习到词向量之间的非线性关系来帮助分类任务，例如“第一个词是‘meseums’&amp;nbsp;并且第二个词是‘in’&amp;#8221;可以帮助判断第三个词是否为地点。&lt;/p&gt;
&lt;h2&gt;Max-margin目标函数&lt;/h2&gt;
&lt;p&gt;本文的窗口分类任务将使用Max-margin目标函数，Max-margin的思想是使得网络在“正确”
记作&lt;span class="math"&gt;\(s\)&lt;/span&gt;，数据点上的得分大于“错误”数据点上的得分记作&lt;span class="math"&gt;\(s_c\)&lt;/span&gt;（c代表corrupt），在窗
口分类任务中“正确”数据点指的是中心词为地点的窗口例如“meseums in Paris are amazing”，
“错误”数据点指的是中心词非地点的窗口例如“not all meseums in&amp;nbsp;Paris”。&lt;/p&gt;
&lt;p&gt;我们的优化目标就变成&lt;span class="math"&gt;\(\text{maxmize}(s-s_c)\)&lt;/span&gt;或者&lt;span class="math"&gt;\(\text{minimize}(s_c-s)\)&lt;/span&gt;，
这里对优化目标进行修改，只有当&lt;span class="math"&gt;\(s_c&amp;gt;s\)&lt;/span&gt;即&lt;span class="math"&gt;\(s_c-s&amp;gt;0\)&lt;/span&gt;时才计算梯度更新参数，主要思想是我们&amp;nbsp;比较关心那些“错误”数据点得分大于“正确”样本点的数据对，优化目标变成下面形式：
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{minimize}J=\max(s_c-s,0)$$&lt;/div&gt;
&lt;p&gt;
上述的目标生成的边缘不够安全，我们希望最后正确样本点的得分&lt;span class="math"&gt;\(s\)&lt;/span&gt;比任意错误样本点的得分高出一个正数&lt;span class="math"&gt;\(\Delta\)&lt;/span&gt;。
所以我们希望对于&lt;span class="math"&gt;\(s-s_c&amp;lt;\Delta\)&lt;/span&gt;即&lt;span class="math"&gt;\(s_c+\Delta-s&amp;gt;0\)&lt;/span&gt;的正负样本对计算梯度，如果取&lt;span class="math"&gt;\(\Delta=1\)&lt;/span&gt;可以得到下面的优化目标：
&lt;/p&gt;
&lt;div class="math"&gt;$$\text{minimize}J=\max(1+s_c-s,0)$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(s=U^Tf(Wx+b)\)&lt;/span&gt;，&lt;span class="math"&gt;\(s_c=U^Tf(Wx_c+b)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2&gt;后向传播（链式法则）&lt;/h2&gt;
&lt;p&gt;有了目标函数之后，优化神经网络的关键在于如何对&lt;span class="math"&gt;\(J\)&lt;/span&gt;求每个参数的导数。在我们的三层网络中参数有
&lt;span class="math"&gt;\(U\)&lt;/span&gt;，&lt;span class="math"&gt;\(W\)&lt;/span&gt;，&lt;span class="math"&gt;\(b\)&lt;/span&gt;，&lt;span class="math"&gt;\(x\)&lt;/span&gt;。假设&lt;span class="math"&gt;\(J=(1+s_c-s)&amp;gt;0\)&lt;/span&gt;，可得：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial J}{\partial s}=-\frac{\partial J}{\partial s_c}=-1$$&lt;/div&gt;
&lt;p&gt;
所以我们只要关心&lt;span class="math"&gt;\(s\)&lt;/span&gt;对于&lt;span class="math"&gt;\(U\)&lt;/span&gt;，&lt;span class="math"&gt;\(W\)&lt;/span&gt;，&lt;span class="math"&gt;\(b\)&lt;/span&gt;，&lt;span class="math"&gt;\(x\)&lt;/span&gt;的导数，求导过程中需要用到链式法则
&lt;span class="math"&gt;\(\frac{\partial y}{\partial x}=\frac{\partial y}{\partial u}\frac{\partial u}{\partial x}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;首先是&lt;span class="math"&gt;\(s\)&lt;/span&gt;对&lt;span class="math"&gt;\(U\)&lt;/span&gt;求导，由于&lt;span class="math"&gt;\(s=U^Ta=U^Tf(z)=U^Tf(Wx+b)\)&lt;/span&gt;，所以：
&lt;img alt="s对U求导" src="https://wugh.github.io/images/NLP/neural-network-3-layers-ner-derivate-wrt-U.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;p&gt;考虑&lt;span class="math"&gt;\(s\)&lt;/span&gt;对于&lt;span class="math"&gt;\(W_{ij}\)&lt;/span&gt;的导数，由前向传播过程可知&lt;span class="math"&gt;\(W_{ij}\)&lt;/span&gt;只在计算&lt;span class="math"&gt;\(a_i\)&lt;/span&gt;的过程中使用，例如下图说明了&lt;span class="math"&gt;\(W_{23}\)&lt;/span&gt;只在计算&lt;span class="math"&gt;\(a_2\)&lt;/span&gt;的时候用到：
&lt;img alt="网络示意图" src="https://wugh.github.io/images/NLP/neural-network-3-layers-ner-graph.png" style="display:block;margin:0 auto" /&gt;
所以导数计算过程如下：
&lt;img alt="s对W求导" src="https://wugh.github.io/images/NLP/neural-network-3-layers-ner-derivate-wrt-W.jpg" style="display:block;margin:0 auto" /&gt;
需要注意这个求导过程中我们将&lt;span class="math"&gt;\(s\)&lt;/span&gt;对于&lt;span class="math"&gt;\(z\)&lt;/span&gt;的导数记作&lt;span class="math"&gt;\(\delta=U\circ f'(z)\)&lt;/span&gt;，这个将在后面的求导过程中反复用到。&lt;/p&gt;
&lt;p&gt;考虑&lt;span class="math"&gt;\(s\)&lt;/span&gt;对于&lt;span class="math"&gt;\(b_i\)&lt;/span&gt;的导数，计算过程与&lt;span class="math"&gt;\(W_{ij}\)&lt;/span&gt;的导数类似：
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\frac{\partial s}{\partial W_{ij}}&amp;amp;=\frac{\partial s}{\partial a_i}\frac{\partial a_i}{\partial z_i}\frac{\partial z_i}{\partial b_i}\\
&amp;amp;=U_if'(z_i)\frac{\partial (W_{i\cdot}x+b_i)}{b_i}\\
&amp;amp;=\delta_i
\end{align*}&lt;/div&gt;
&lt;p&gt;
所以&lt;span class="math"&gt;\(s\)&lt;/span&gt;对于&lt;span class="math"&gt;\(b_i\)&lt;/span&gt;的导数为&lt;span class="math"&gt;\(\delta\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;现在考虑&lt;span class="math"&gt;\(s\)&lt;/span&gt;对于&lt;span class="math"&gt;\(x_j\)&lt;/span&gt;的导数，由于每个&lt;span class="math"&gt;\(a_i\)&lt;/span&gt;都和特定的&lt;span class="math"&gt;\(x_j\)&lt;/span&gt;相连，所以&lt;span class="math"&gt;\(x_j\)&lt;/span&gt;会影响所有隐藏层的输出，求导过程如下：
&lt;img alt="s对x求导" src="https://wugh.github.io/images/NLP/neural-network-3-layers-ner-derivate-wrt-x.png" style="display:block;margin:0 auto" /&gt;
所以&lt;span class="math"&gt;\(s\)&lt;/span&gt;对于&lt;span class="math"&gt;\(x\)&lt;/span&gt;的导数为&lt;span class="math"&gt;\(W^T\delta\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2&gt;后向传播（误差传播）&lt;/h2&gt;
&lt;p&gt;下面从误差（error）后向传播的角度来解释&lt;span class="math"&gt;\(s\)&lt;/span&gt;对于&lt;span class="math"&gt;\(W_{ij}^{(1)}\)&lt;/span&gt;的梯度。&lt;span class="math"&gt;\(W^{(k)}\)&lt;/span&gt;为第&lt;span class="math"&gt;\(k\)&lt;/span&gt;层和第&lt;span class="math"&gt;\(k+1\)&lt;/span&gt;层之间的权重矩阵，
图中&lt;span class="math"&gt;\(W^{(1)}\)&lt;/span&gt;等价于上文提到的&lt;span class="math"&gt;\(W\)&lt;/span&gt;，&lt;span class="math"&gt;\(W^{(2)}\)&lt;/span&gt;等价于上文提到的&lt;span class="math"&gt;\(U\)&lt;/span&gt;。&lt;span class="math"&gt;\(b_j^{(k)}\)&lt;/span&gt;为第&lt;span class="math"&gt;\(k+1\)&lt;/span&gt;层第j个神经元的偏置，
图中的&lt;span class="math"&gt;\(b^{(1)}\)&lt;/span&gt;等价于上文提到的&lt;span class="math"&gt;\(b\)&lt;/span&gt;。下图中&lt;span class="math"&gt;\(a_j^{(k)}\)&lt;/span&gt;表示第k层第j个神经元的输出，
下图中&lt;span class="math"&gt;\(z_j^{(k)}\)&lt;/span&gt;表示第k层第j个神经元的标量输入。
&lt;img alt="神经网络符号说明" src="https://wugh.github.io/images/NLP/neural-network-3-layers-bp-symbol.jpg" style="display:block;margin:0 auto" /&gt;
&lt;span class="math"&gt;\(\delta_1^{(2)}\)&lt;/span&gt;表示传播到&lt;span class="math"&gt;\(z_1^{(2)}\)&lt;/span&gt;位置的误差，即&lt;span class="math"&gt;\(s\)&lt;/span&gt;相对于&lt;span class="math"&gt;\(z_1^{(2)}\)&lt;/span&gt;的导数，如下图：
&lt;img alt="delta含义" src="https://wugh.github.io/images/NLP/neural-network-3-layers-bp-delta.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;p&gt;下图说明了如何利用反向误差来计算&lt;span class="math"&gt;\(W_{12}^{(1)}\)&lt;/span&gt;处的误差：
&lt;img alt="误差传播" src="https://wugh.github.io/images/NLP/neural-network-3-layers-bp-error-w12.jpg" style="display:block;margin:0 auto" /&gt;
可知最后&lt;span class="math"&gt;\(W_{12}^{(1)}\)&lt;/span&gt;处的误差为&lt;span class="math"&gt;\(\delta_1^{(2)}a_2^{(1)}\)&lt;/span&gt;，那么对于整个矩阵&lt;span class="math"&gt;\(W^{(1)}\)&lt;/span&gt;的误差可以写成&lt;span class="math"&gt;\({\delta^{(2)}}^Ta^{(1)}\)&lt;/span&gt;。&amp;nbsp;可以看出反向误差传播本质上也是链式法则，不过看起来更直观，下面总结了反向误差传播的两个要点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;误差&lt;span class="math"&gt;\(x\)&lt;/span&gt;向后传播穿过一个神经元时只需要将误差乘以当前神经元的局部梯度，如下图
&lt;img alt="误差传播" src="https://wugh.github.io/images/NLP/neural-network-3-layers-bp-delta-neuron.png" style="display:block;margin:0 auto" /&gt;&lt;/li&gt;
&lt;li&gt;误差&lt;span class="math"&gt;\(\delta\)&lt;/span&gt;沿着线性变换后向传播时，只需要将误差乘以前向传播时的对应线性变换权重，如下图（绿色为前向，黄色为后向）
&lt;img alt="误差传播" src="https://wugh.github.io/images/NLP/neural-network-3-layers-bp-delta-affine.png" style="display:block;margin:0 auto" /&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;整个后向传播过程可以看出我们定义的&lt;span class="math"&gt;\(\delta^{(k)}\)&lt;/span&gt;在计算误差的过程中十分重要，
需要知道如何从&lt;span class="math"&gt;\(\delta^{(k)}\)&lt;/span&gt;计算&lt;span class="math"&gt;\(\delta^{(k-1)}\)&lt;/span&gt;，即如何把&lt;span class="math"&gt;\(z^{(k)}\)&lt;/span&gt;处的误差传播到&lt;span class="math"&gt;\(z^{(k-1)}\)&lt;/span&gt;，
现在先考虑如何得到&lt;span class="math"&gt;\(z_j^{(k-1)}\)&lt;/span&gt;处的误差：
&lt;img alt="误差传播" src="https://wugh.github.io/images/NLP/neural-network-3-layers-bp-delta-to-delta.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对于第k层特定的&lt;span class="math"&gt;\(z_i^{(k)}\)&lt;/span&gt;处的误差首先需要传播到&lt;span class="math"&gt;\(a_j^{(k-1)}\)&lt;/span&gt;位置，在前向计算时从&lt;span class="math"&gt;\(a_j^{(k-1)}\)&lt;/span&gt;到
&lt;span class="math"&gt;\(z_i^{(k)}\)&lt;/span&gt;是线性变换，所以&lt;span class="math"&gt;\(a_j^{(k-1)}\)&lt;/span&gt;从&lt;span class="math"&gt;\(z_i^{(k)}\)&lt;/span&gt;处接收到的误差为&lt;span class="math"&gt;\(\delta_i^{(k)}W_{ij}^{(k-1)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;由于第k层每个&lt;span class="math"&gt;\(z_i^{(k)}\)&lt;/span&gt;处的误差都会传播到&lt;span class="math"&gt;\(a_j^{(k-1)}\)&lt;/span&gt;位置（层与层之间全连接），
所以&lt;span class="math"&gt;\(a_j^{(k-1)}\)&lt;/span&gt;从所有的&lt;span class="math"&gt;\(z_i^{(k)}\)&lt;/span&gt;处收到的总误差为&lt;span class="math"&gt;\(\sum_i&amp;nbsp;\delta_i^{(k)}W_{ij}^{(k-1)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;可以看到从&lt;span class="math"&gt;\(z_j^{(k-1)}\)&lt;/span&gt;到&lt;span class="math"&gt;\(a_j^{(k-1)}\)&lt;/span&gt;穿过了一个神经元，所以&lt;span class="math"&gt;\(a_j^{(k-1)}\)&lt;/span&gt;处的误差传播到&lt;span class="math"&gt;\(z_j^{(k-1)}\)&lt;/span&gt;
时需要乘以这个神经元的局部梯度&lt;span class="math"&gt;\(f'(z_j^{(k-1)})\)&lt;/span&gt;，得到&lt;span class="math"&gt;\(z_j^{(k-1)}\)&lt;/span&gt;处的误差&lt;span class="math"&gt;\(\delta_j^{(k-1)}\)&lt;/span&gt;
为&lt;span class="math"&gt;\(f'(z_j^{(k-1)})\sum_i&amp;nbsp;\delta_i^{(k)}W_{ij}^{(k-1)}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;根据&lt;span class="math"&gt;\(\delta_j^{(k-1)}=f'(z_j^{(k-1)})\sum_i \delta_i^{(k)}W_{ij}^{(k-1)}\)&lt;/span&gt;，
从&lt;span class="math"&gt;\(\delta^{(k)}\)&lt;/span&gt;计算&lt;span class="math"&gt;\(\delta^{(k-1)}\)&lt;/span&gt;的过程可以写成矩阵形式：
&lt;/p&gt;
&lt;div class="math"&gt;$$\delta^{(k-1)}=f'(z^{(k-1)})\circ({W^{(k-1)}}^T\delta^{(k)})$$&lt;/div&gt;
&lt;h2&gt;Tips和Tricks&lt;/h2&gt;
&lt;p&gt;剩下部分将讨论一下神经网络在工程实现中的一些技巧。&lt;/p&gt;
&lt;h3&gt;神经网络的使用策略&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;选择适当的网络结构&lt;ol&gt;
&lt;li&gt;结构：单个词，固定窗口，词袋，递归 vs 循环，&lt;span class="caps"&gt;CNN&lt;/span&gt;，基于句子 vs&amp;nbsp;基于文档&lt;/li&gt;
&lt;li&gt;非线性函数选择&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;用梯度检查来校验是否有实现bug&lt;/li&gt;
&lt;li&gt;参数初始化&lt;/li&gt;
&lt;li&gt;优化技巧&lt;/li&gt;
&lt;li&gt;检查模型是否能够在数据集上过拟合&lt;ol&gt;
&lt;li&gt;如果不能，那么需要改变模型结果或者让模型参数规模更大（例如增加隐藏层）&lt;/li&gt;
&lt;li&gt;如果可以，那么增加正则化项&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;神经元非线性函数选择&lt;/h3&gt;
&lt;p&gt;最常用的两个非线性函数是sigmoid和tanh函数，tanh可以看成对sigmoid函数进行了缩放和平移
&lt;/p&gt;
&lt;div class="math"&gt;$$\tanh(z)=2\sigma(2z)-1$$&lt;/div&gt;
&lt;p&gt;
sigmoid和tanh的函数图像和导数形式如下：
&lt;img alt="sigmoid和tanh" src="https://wugh.github.io/images/NLP/neural-network-tricks-non-linearities.png" style="display:block;margin:0 auto" /&gt;
tanh函数在深度网络中的表现通常要比sigmoid好，这里还有一些额外的非线性函数，
其中&lt;a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"&gt;ReLu函数&lt;/a&gt;在2015年成为了深度神经网络最受欢迎的非线性函数。
&lt;img alt="更多的非线性函数" src="https://wugh.github.io/images/NLP/neural-network-tricks-non-linearities-more.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;h3&gt;梯度检查（Gradient&amp;nbsp;Check）&lt;/h3&gt;
&lt;p&gt;前文描述的后向算法采用使用微积分来计算偏导数，但是其实我们可以根据导数的定义，&amp;nbsp;采用数值方法来估计偏导数，检查过程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;实现前向计算和后向误差传播&lt;/li&gt;
&lt;li&gt;对于网络中的每一个参数（假设记作&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;，&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;包含了网络所有的&lt;span class="math"&gt;\(W\)&lt;/span&gt;，&lt;span class="math"&gt;\(b\)&lt;/span&gt;），
然后按下列公式计算&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;每一维的梯度，&lt;span class="math"&gt;\(h\)&lt;/span&gt;一般取一个较小的数（例如0.00001）：
&lt;img alt="梯度检查" src="https://wugh.github.io/images/NLP/neural-network-tricks-non-gradient-check.png" style="display:block;margin:0 auto" /&gt;&lt;/li&gt;
&lt;li&gt;对比后向误差传播计算的梯度和第2步计算的梯度是否很接近&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;参数初始化&lt;/h3&gt;
&lt;p&gt;把偏置初始化成0，然后权重矩阵的初始化方式如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$W^{(l)}\sim U\left[-\frac{\sqrt{6}}{\sqrt{n^{(l)} + n^{(l+1)}}},\frac{\sqrt{6}}{\sqrt{n^{(l)}+n^{(l+1)}}}\right]$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(n^{(l)}\)&lt;/span&gt;表示第&lt;span class="math"&gt;\(l\)&lt;/span&gt;层的神经元个数，&lt;span class="math"&gt;\(n^{(l+1)}\)&lt;/span&gt;表示第&lt;span class="math"&gt;\(l+1\)&lt;/span&gt;层的神经元个数。&lt;/p&gt;
&lt;h3&gt;学习率&lt;/h3&gt;
&lt;p&gt;随机梯度下降一般在一个或者数个样本（mini-batch）上计算梯度，然后进行参数更新，
最简单的随机梯度下降采用固定的学习率&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;，学习率不能设置太大：
&lt;img alt="固定的alpha" src="https://wugh.github.io/images/NLP/neural-network-tricks-learning-rate-fixed-alpha.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;p&gt;较好的方法是让学习率随着迭代次数&lt;span class="math"&gt;\(t\)&lt;/span&gt;逐步下降：
&lt;img alt="alpha随时间下降" src="https://wugh.github.io/images/NLP/neural-network-tricks-learning-rate-decrease-alpha.png" style="display:block;margin:0 auto" /&gt;
其中&lt;span class="math"&gt;\(\alpha(0)\)&lt;/span&gt;表示初始的学习率是一个可调参数，&lt;span class="math"&gt;\(\tau\)&lt;/span&gt;是另一个参数表示从何时开始学习率开始降低。&amp;nbsp;在实践中，这种方法取得的结果通常不错。&lt;/p&gt;
&lt;p&gt;下面将讨论一种不需要手工设定学习率的方法，称之为Adagrad，这种方法的特点在于每个参数使用的学习率都不同。&lt;/p&gt;
&lt;h3&gt;Adagrad&lt;/h3&gt;
&lt;p&gt;不同参数使用不同的学习率，越少进行更新的参数采用的学习率大于那些更新频繁的参数，具体每个参数的学习率变化如下：
&lt;img alt="adagrad" src="https://wugh.github.io/images/NLP/neural-network-tricks-adagrad.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;h3&gt;正则化（Regularization）&lt;/h3&gt;
&lt;p&gt;当我们的模型在训练集上过拟合之后，需要对模型进行正则化，一般来说是将权重矩阵的&lt;span class="math"&gt;\(L_2\)&lt;/span&gt;正则项加入损失函数（偏置不加入）：
&lt;img alt="L2正则" src="https://wugh.github.io/images/NLP/neural-network-tricks-regularization.png" style="display:block;margin:0 auto" /&gt;
其中&lt;span class="math"&gt;\({\Vert W^{(i)}\Vert}_F\)&lt;/span&gt;称为弗罗贝尼乌斯范数（Frobenius norm）：
&lt;img alt="弗罗贝尼乌斯范数" src="https://wugh.github.io/images/NLP/neural-network-tricks-matrix-F-norm.png" style="display:block;margin:0 auto" /&gt;
可以看出正则项会惩罚大的参数，&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;是超参（越大则越认为&lt;span class="math"&gt;\(W\)&lt;/span&gt;应该接近0），加入&lt;span class="math"&gt;\(L_2\)&lt;/span&gt;正则项之后会使得&lt;span class="math"&gt;\(W\)&lt;/span&gt;接近0，
使得网络拟合的目标函数灵活性降低从而避免过拟合。
防止过拟合的方法除了加入正则项之外，还有&lt;code&gt;Early Stopping&lt;/code&gt;（根据模型在发展集上的性能停止训练）。&lt;/p&gt;
&lt;h3&gt;防止特征共同适应（Co-adaptation）&lt;/h3&gt;
&lt;p&gt;特征Co-adaptation指的是某个特征只有在另个特征出现时才有用。防止特征
Co-adaptation的方法是Dropout，顾名思义就是在训练模型的时候随机扔掉一部分的输入&amp;nbsp;（即随机把一部分输入变成0）
&lt;/p&gt;
&lt;div class="math"&gt;$$a^{(k+1)}=f\left (W^{(k)}(r\circ a^{(k)})+b\right )$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(p\)&lt;/span&gt;为超参，其中&lt;span class="math"&gt;\(r\)&lt;/span&gt;的每一维以概率&lt;span class="math"&gt;\(p\)&lt;/span&gt;置成1，以&lt;span class="math"&gt;\(1-p\)&lt;/span&gt;的概率置成0。测试的时候不需要Dropout，&amp;nbsp;采用Dropout可以也防止模型对于特定的特征共现过拟合。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="神经网络"></category><category term="后向传播"></category><category term="bp算法"></category><category term="深度学习"></category></entry><entry><title>CS224d笔记2——Softmax分类和窗口分类</title><link href="https://wugh.github.io/posts/2016/02/cs224d-notes2-softmax-classification-and-window-classification/" rel="alternate"></link><published>2016-02-29T14:18:00+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2016-02-29:posts/2016/02/cs224d-notes2-softmax-classification-and-window-classification/</id><summary type="html">&lt;p&gt;这部分内容主要总结Softmax分类，以及如何以交叉熵作为Softmax分类的损失度量标准来
训练Softmax的参数，最后将Softmax分类应用在一个简单的窗口分类任务上，窗口分类
指的是利用中心词向量以及中心词窗口范围内的向量拼接起来对中心词进行简单的分类&amp;nbsp;（例如把中心词按实体类型分成人名、地名、组织名、其他）。&lt;/p&gt;
&lt;h2&gt;Softmax分类&lt;/h2&gt;
&lt;p&gt;Softmax分类是Logistic回归的扩展版（Logistic回归用于二分类，Softmax分类用于类别
数量&amp;gt;2的分类任务），词向量&lt;span class="math"&gt;\(x\)&lt;/span&gt;（假设词向量维度是&lt;span class="math"&gt;\(d\)&lt;/span&gt;）属于类别&lt;span class="math"&gt;\(j\)&lt;/span&gt;的概率如下所示：
&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}_j = p(y_j=1|x)=\frac{\exp(W_{j\cdot}x)}{\sum_{c=1}^C\exp(W_{c\cdot}x)}$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(W\in\mathbb{R}^{C\times d}\)&lt;/span&gt;，&lt;span class="math"&gt;\(W_{j\cdot}x=\sum_{i=1}^d W_{ji}x_i=f_j\)&lt;/span&gt;。&amp;nbsp;以交叉熵作为损失函数，可以得到对于特定一个训练样本的损失如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$H(y,\hat{y})=-\sum_{j=1}^C y_j\log(\hat{y}_j)=-\sum_{j=1}^C y_j\log(\frac{\exp(W_{j\cdot}x)}{\sum_{c=1}^C\exp(W_{c\cdot}x)})$$&lt;/div&gt;
&lt;p&gt;
由于&lt;span class="math"&gt;\(y\)&lt;/span&gt;是一个one-hot向量，所以&lt;span class="math"&gt;\(y\)&lt;/span&gt;有&lt;span class="math"&gt;\(C-1\)&lt;/span&gt;个值为0，假设&lt;span class="math"&gt;\(y_k=1\)&lt;/span&gt;，那么上式可以写成：
&lt;/p&gt;
&lt;div class="math"&gt;$$-\log(\frac{\exp(W_{k\cdot}x)}{\sum_{c=1}^C\exp(W_{c\cdot}x)})$$&lt;/div&gt;
&lt;p&gt;
如果有&lt;span class="math"&gt;\(N\)&lt;/span&gt;个数据点，那么总的损失如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$-\sum_{i=1}^N \log(\frac{\exp(W_{k(i)\cdot}x)}{\sum_{c=1}^C\exp(W_{c\cdot}x)})$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(k(i)\)&lt;/span&gt;代表第&lt;span class="math"&gt;\(i\)&lt;/span&gt;个数据点正确的类别标签。&lt;/p&gt;
&lt;p&gt;这里有一个地方需要注意，就是我们把利用softmax对词向量分类的时候是否需要对词向量进行更新。
如果不需要更新词向量那么此时参数只有&lt;span class="math"&gt;\(W\)&lt;/span&gt;矩阵，参数规模为&lt;span class="math"&gt;\(C\cdot d\)&lt;/span&gt;；但是如果需要对词向量进行更新，
那么模型的参数就会变多，因为我们需要对词汇表中的向量进行更新，总的模型参数变成&lt;span class="math"&gt;\(C\cdot d + |V|\cdot d\)&lt;/span&gt;。
假设用&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;表示模型参数，此时损失函数对于&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;的导数如下：
&lt;img alt="softmax参数规模" src="https://wugh.github.io/images/NLP/softmax-parameters-W-and-word-vectors.png" /&gt;
如此大的参数规模很容易导致模型过拟合，可以通过加入&lt;span class="math"&gt;\(L2\)&lt;/span&gt;正则来减缓过拟合（使得模型参数接近0）：
&lt;/p&gt;
&lt;div class="math"&gt;$$J(\theta)=-\sum_{i=1}^N \log(\frac{\exp(W_{k(i)\cdot}x)}{\sum_{c=1}^C\exp(W_{c\cdot}x)})+\lambda\sum_{k=1}^{C\cdot d+|V|\cdot d} \theta_k^2$$&lt;/div&gt;
&lt;p&gt;这里对于单个训练样本的交叉熵损失进行一个初步的导数推导，假设该样本类比为k：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial J}{\partial f} = \frac{\partial -\log(\frac{\exp(f_k)}{\sum_{c=1}^C\exp(f_c)})}{\partial f}$$&lt;/div&gt;
&lt;p&gt;
求导结果如下：
&lt;img alt="softmax简单求导" src="https://wugh.github.io/images/NLP/softmax-parameters-f-gradient.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;窗口分类&lt;/h2&gt;
&lt;p&gt;窗口分类指的是利用中心词向量以及中心词窗口范围内的向量拼接起来对中心词进行简单的分类
（例如把中心词按实体类型分成人名、地名、组织名、其他）。下面例子取窗口长度为2，
对中心词“Paris”进行分类，最后令输入向量&lt;span class="math"&gt;\(x_{\text{windows}}=x\in \mathbb{R}^{5\cdot d}\)&lt;/span&gt;
&lt;img alt="窗口分类输入" src="https://wugh.github.io/images/NLP/window-classification-x-window.png" /&gt;&lt;/p&gt;
&lt;p&gt;整个模型就是一个简单的softmax分类，所以我们只要把softmax的损失函数对&lt;span class="math"&gt;\(W\)&lt;/span&gt;和&lt;span class="math"&gt;\(x\)&lt;/span&gt;求导就可以
得到梯度，其实求导结果和&lt;a href="https://wugh.github.io/posts/2016/02/cs224d-notes1-word2vec/"&gt;上一篇文章&lt;/a&gt;相同，
由于两篇文章在符号表示上略有区别，这里按这篇文章的符号重新推导一次，最后的结果是相同的。
下一篇文章将介绍神经网络，通过神经网络的后向误差传播过程可以更好地理解这个推导结果。
&lt;img alt="窗口分类求导结果" src="https://wugh.github.io/images/NLP/window-classification-gradient.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;最后把&lt;span class="math"&gt;\(x\)&lt;/span&gt;上的梯度分配给对应的词向量就可以更新词向量了。
&lt;img alt="词向量梯度" src="https://wugh.github.io/images/NLP/window-classification-gradient-x-word.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;Softmax分类最后得到的只是原始数据空间上的线性分类面，利用神经网络可以学习到非线性的决策边界，
将在&lt;a href="https://wugh.github.io/posts/2016/03/cs224d-notes3-neural-networks-and-backward-propagation/"&gt;下一篇&lt;/a&gt;笔记总结神经网络。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="softmax"></category><category term="窗口分类"></category><category term="深度学习"></category></entry><entry><title>CS224d笔记1——word2vec</title><link href="https://wugh.github.io/posts/2016/02/cs224d-notes1-word2vec/" rel="alternate"></link><published>2016-02-26T10:03:00+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2016-02-26:posts/2016/02/cs224d-notes1-word2vec/</id><summary type="html">&lt;p&gt;假期学习了斯坦福的&lt;a href="http://cs224d.stanford.edu/syllabus.html"&gt;CS224d&lt;/a&gt;课程，该课
程的主要内容是神经网络在自然语言处理领域的应用。 这里记录相关的学习笔记，大概分
成以下几个部分：word2vec，窗口分类，神经网络，循环神经网络，递归神经网络，卷积&amp;nbsp;神经网络。&lt;/p&gt;
&lt;h2&gt;为什么需要深度学习&lt;/h2&gt;
&lt;p&gt;传统的机器学习方法都是认为的设计特征或者表示，深度学习的目的是希望能够通过神经
网络让机器自动学习到有效的特征表示，这里所说的深度学习更偏向于关注各种类型的神
经网络。探索机器学习的&lt;a href="http://www.52nlp.cn/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%B8%80%E8%AE%B2%E5%BC%95%E8%A8%80"&gt;原因&lt;/a&gt;主要有以下几方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;人工设计的特征常常定义过多，不完整并且需要花费大量的时间去设计和验证&lt;/li&gt;
&lt;li&gt;自动学习的特征容易自适应，并且可以很快的学习&lt;/li&gt;
&lt;li&gt;深度学习提供了一个弹性的，通用的学习框架用来表征自然的，视觉的和语言的信息。&lt;/li&gt;
&lt;li&gt;深度学习可以用来学习非监督的（来自于生文本）和有监督的（带有特别标记的文本，&amp;nbsp;例如正向和负向标记）&lt;/li&gt;
&lt;li&gt;在2006年深度学习技术开始在一些任务中表现出众，为什么现在才热起来？&lt;ul&gt;
&lt;li&gt;深度学习技术受益于越来越多的数据&lt;/li&gt;
&lt;li&gt;更快的机器与更多核&lt;span class="caps"&gt;CPU&lt;/span&gt;/&lt;span class="caps"&gt;GPU&lt;/span&gt;对深度学习的普及起了很大的促进作用&lt;/li&gt;
&lt;li&gt;新的模型，算法和idea层出不穷&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;通过深度学习技术提升效果首先发生在语音识别和机器视觉领域，然后开始过渡到&lt;span class="caps"&gt;NLP&lt;/span&gt;领&amp;nbsp;域&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;深度学习在所有的&lt;span class="caps"&gt;NLP&lt;/span&gt;层次（音素、形态学、句法、语义）都得到了应用，在所有的&lt;span class="caps"&gt;NLP&lt;/span&gt;层&amp;nbsp;次的表示都涉及到向量（Vectors），下面主要讲如何用向量来表示词。&lt;/p&gt;
&lt;h2&gt;词向量&lt;/h2&gt;
&lt;p&gt;在传统意义上会使用&lt;a href="http://wordnet.princeton.edu/"&gt;WordNet&lt;/a&gt;来表示词的含义，通过WordNet可以查询词之间的
上下位关系、一个词的同义词、度量词之间的相似度等。但是WordNet是由人工维护，存在&amp;nbsp;一些问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;语义词典资源很棒但是可能在一些细微之处有缺失，例如这些同义词准确吗：adept,
  expert, good, practiced,&amp;nbsp;proficient,skillful?&lt;/li&gt;
&lt;li&gt;新词缺失（无法及时更新）&lt;/li&gt;
&lt;li&gt;人为构建，具有一定的主观性&lt;/li&gt;
&lt;li&gt;需要耗费大量的人力来构建&lt;/li&gt;
&lt;li&gt;很难用来计算词与词的相似度&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;One-hot向量&lt;/h3&gt;
&lt;p&gt;首先我们把词表中的词从0到&lt;span class="math"&gt;\(|V|-1\)&lt;/span&gt;进行编号，ont-hot向量把每个词表示成一个&lt;span class="math"&gt;\(|V|\)&lt;/span&gt;维
（词表大小为&lt;span class="math"&gt;\(|V|\)&lt;/span&gt;）的向量，该向量只有特定词的编号对应的位置为1，其他位置全部为0
。这种方法把每个词表示成独立的个体，无法通过one-hot向量直接到词之间的关系。解决&amp;nbsp;方法是通过一个词的上下文来表示一个词。&lt;/p&gt;
&lt;h3&gt;基于&lt;span class="caps"&gt;SVD&lt;/span&gt;分解的方法&lt;/h3&gt;
&lt;p&gt;这种方法的基本思想是通过大量的数据统计到词的累计共现矩阵&lt;span class="math"&gt;\(X\)&lt;/span&gt;，然后对&lt;span class="math"&gt;\(X\)&lt;/span&gt;进行奇异值
分解得到&lt;span class="math"&gt;\(USV^T\)&lt;/span&gt;，&lt;span class="math"&gt;\(U\)&lt;/span&gt;的每一行对应一个词的向量表示，&lt;span class="caps"&gt;SVD&lt;/span&gt;更多信息参考&lt;a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html"&gt;这里&lt;/a&gt;。&amp;nbsp;共现矩阵一般分为词-文档共现矩阵和词-词共现矩阵。&lt;/p&gt;
&lt;h4&gt;词-文档共现矩阵&lt;/h4&gt;
&lt;p&gt;这种共现矩阵的思想认为相关的词会出现在同一个文档中。假设词表大小为&lt;span class="math"&gt;\(|V|\)&lt;/span&gt;，文档数
量为&lt;span class="math"&gt;\(|M|\)&lt;/span&gt;，那么词-文档共现矩阵规模为&lt;span class="math"&gt;\(|V|\times|M|\)&lt;/span&gt;，矩阵元素&lt;span class="math"&gt;\(X_{ij}\)&lt;/span&gt;表示词i在文&amp;nbsp;档j中的出现次数，只要对所有文档循环一次就可以统计得到该矩阵&lt;/p&gt;
&lt;h4&gt;词-词共现矩阵&lt;/h4&gt;
&lt;p&gt;词-词共现矩阵的思想是词i的窗口内出现了j，那么认为i和j的共现次数加一，&lt;span class="math"&gt;\(X_{ij}\)&lt;/span&gt;表
示两个词的共现次数，&lt;span class="math"&gt;\(X\)&lt;/span&gt;是一个&lt;span class="math"&gt;\(|V|\times|V|\)&lt;/span&gt;的方阵。窗口一般是对称的，长度一般为&amp;nbsp;5-10。下面举一个简单的例子，例子中窗口大小为1，语料如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I enjoy&amp;nbsp;flying.&lt;/li&gt;
&lt;li&gt;I like &lt;span class="caps"&gt;NLP&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;I like deep&amp;nbsp;learning.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;得到共现矩阵如下：
&lt;img alt="词-词共现矩阵" src="https://wugh.github.io/images/NLP/word-word-matrix.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;p&gt;对该矩阵进行&lt;span class="caps"&gt;SVD&lt;/span&gt;分解：
&lt;img alt="svd" src="https://wugh.github.io/images/NLP/word-word-matrix-svd.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;p&gt;之后区&lt;span class="math"&gt;\(U\)&lt;/span&gt;的前&lt;span class="math"&gt;\(k\)&lt;/span&gt;列作为所有单词的&lt;span class="math"&gt;\(k\)&lt;/span&gt;维向量表示。
&lt;img alt="向量表示" src="https://wugh.github.io/images/NLP/word-word-matrix-embeding.png" style="display:block;margin:0 auto" /&gt;&lt;/p&gt;
&lt;p&gt;这种基于共现矩阵进行&lt;span class="caps"&gt;SVD&lt;/span&gt;分解的方法存在问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;矩阵的维度经常发生变化（新词和新文档的加入）&lt;/li&gt;
&lt;li&gt;矩阵非常稀疏&lt;/li&gt;
&lt;li&gt;矩阵过大&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;SVD&lt;/span&gt;分解的计算复杂度大，对于&lt;span class="math"&gt;\(n\times m\)&lt;/span&gt;的矩阵进行分解的复杂度为&lt;span class="math"&gt;\(O(mn^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;需要对&lt;span class="math"&gt;\(X\)&lt;/span&gt;矩阵进行一些修正来修复词频分布不均匀问题&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对&lt;span class="math"&gt;\(X\)&lt;/span&gt;矩阵的一些修正：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;功能词(the, he,&amp;nbsp;has)过于频繁，对语法有很大影响，解决办法是降低使用或完全忽略功能词&lt;/li&gt;
&lt;li&gt;采用带权重的窗口，距离当前词距离越近共现权重越大&lt;/li&gt;
&lt;li&gt;用皮尔逊相关系数代替计数，并置负数为0&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Word2vec&lt;/h3&gt;
&lt;p&gt;Word2vec的基本思想与共现计数不同，word2vec主要分为两种，采用当前词来预测窗口中&amp;nbsp;的其他词（skip-gram），另一种是用窗口中的词来预测当前词（cbow）。&lt;/p&gt;
&lt;h4&gt;&lt;span class="caps"&gt;CBOW&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span class="caps"&gt;CBOW&lt;/span&gt;（Continuous Bag of Words）的基本思想是用窗口中的词的向量求平均之后来预测中心词
。训练语料是上下文和对应的中心词的对，上下文窗口内的每个词都用一个one-hot向量
&lt;span class="math"&gt;\(x^(i)\)&lt;/span&gt;表示，中心词用one-hot向量&lt;span class="math"&gt;\(y^(i)\)&lt;/span&gt;表示，&lt;span class="caps"&gt;CBOW&lt;/span&gt;中预测的中心词只有一个所以直接把
输出向量表示成&lt;span class="math"&gt;\(y\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;随机初始化两个矩阵（一般用正态分布进行初始化）&lt;span class="math"&gt;\(W^{(1)}\in \mathbb{R}^{n\times |V|}\)&lt;/span&gt;和
&lt;span class="math"&gt;\(W^{(2)}\in \mathbb{R}^{|V|\times n}\)&lt;/span&gt;分别用来存储输入向量和输出向量，最后训练完每个词
有两个向量，一个是当作输入时的向量，一个是当作输出时的向量。&lt;span class="math"&gt;\(n\)&lt;/span&gt;为词向量的维度；&lt;span class="math"&gt;\(W^{(1)}\)&lt;/span&gt;
的第&lt;span class="math"&gt;\(i\)&lt;/span&gt;列表示词&lt;span class="math"&gt;\(w^(i)\)&lt;/span&gt;当作输入时的向量表示，记作&lt;span class="math"&gt;\(u^{(i)}\)&lt;/span&gt;；&lt;span class="math"&gt;\(W^{(2)}\)&lt;/span&gt;的第&lt;span class="math"&gt;\(j\)&lt;/span&gt;行表
示词&lt;span class="math"&gt;\(w^(j)\)&lt;/span&gt;当作输出时的向量表示，记作&lt;span class="math"&gt;\(v^{(j)}\)&lt;/span&gt;。利用上下文预测中心词的步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;把大小为&lt;span class="math"&gt;\(C\)&lt;/span&gt;的上下文用one-hot向量表示
   &lt;span class="math"&gt;\((x^{(i-C)},\ldots,x^{(i-1)},x^{(i+1)},\ldots,x^{(i+C)})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;把&lt;span class="math"&gt;\(W^{(1)}\)&lt;/span&gt;分别和&lt;span class="math"&gt;\(2C\)&lt;/span&gt;个one-hot向量相乘，得到上下文的输入向量，例如
   &lt;span class="math"&gt;\(x^{(i-C)}\)&lt;/span&gt;作为输入时的向量为&lt;span class="math"&gt;\(u^{(i-C)}=W^{(1)}x^{(i-C)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;将上下文中的向量进行平均&lt;span class="math"&gt;\(h=\frac{u^{(i-C)}+u^{(i-C+1)}+\ldots&amp;nbsp;+u^{(i+C)}}{2C}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;生成得分向量&lt;span class="math"&gt;\(z=W^{(2)}h\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;利用softmax函数将得分转换成概率&lt;span class="math"&gt;\(\hat{y}=\text{softmax}(z)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;我们的目标是希望预测的概率&lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;和真实的中心词one-hot向量&lt;span class="math"&gt;\(y\)&lt;/span&gt;一致&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们希望模型输出的概率分布和真实分布的尽量相似，可以利用信息论中的交叉熵来衡量两个概率分布
的距离，离散情况下两个概率分布的交叉熵&lt;span class="math"&gt;\(H(\hat{y},y)\)&lt;/span&gt;如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$H(y,\hat{y})=-\sum_{j=1}^{|V|}y_j\log(\hat{y}_j)$$&lt;/div&gt;
&lt;p&gt;
考虑&lt;span class="caps"&gt;CBOW&lt;/span&gt;中的情况，此时&lt;span class="math"&gt;\(y\)&lt;/span&gt;是一个one-hot向量，假设&lt;span class="math"&gt;\(y\)&lt;/span&gt;的第&lt;span class="math"&gt;\(i\)&lt;/span&gt;维为1，那么交叉熵可以简化成：
&lt;/p&gt;
&lt;div class="math"&gt;$$H(y,\hat{y})=-y_i\log(\hat{y}_i)=-\log(\hat{y}_i)$$&lt;/div&gt;
&lt;p&gt;
可以看出交叉熵的最小值为0，优化目标就是最小化交叉熵：
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\min J&amp;amp;=-\log P(w^{(i)}|w^{(i-C)},\ldots,w^{(i-1)},w^{(i+1)},\ldots,w^{(i+C)})\\
&amp;amp;=-\log P(v^{(i)}|h)\\
&amp;amp;=-\log \frac{\exp{(v^{(i)}\cdot h)}}{\sum_{j=1}^{|V|}\exp(v^{(j)}\cdot h)}\\
&amp;amp;=-v^{(i)}\cdot h + \log{\sum_{j=1}^{|V|}\exp(v^{(j)}\cdot h)}
\end{align*}&lt;/div&gt;
&lt;p&gt;
由上描述可知整个模型的未知参数就是&lt;span class="math"&gt;\(W^{(1)}\)&lt;/span&gt;和&lt;span class="math"&gt;\(W^{(2)}\)&lt;/span&gt;，即对于每个输出向量&lt;span class="math"&gt;\(v^{(j)}\)&lt;/span&gt;和
上下文中的输入向量&lt;span class="math"&gt;\((u^{(i-C)},\ldots,u^{(i-1)},u^{(i+1)},\ldots,u^{(i+C)})\)&lt;/span&gt;求导。&lt;/p&gt;
&lt;p&gt;首先是&lt;span class="math"&gt;\(\frac{\partial J}{\partial v^{(j)}}\)&lt;/span&gt;
&lt;img alt="J对输出向量求导" src="https://wugh.github.io/images/NLP/cbow-gradient-w2.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;然后对窗口内的其中一个输入向量求导&lt;span class="math"&gt;\(\frac{\partial J}{\partial u^{(i-C)}}\)&lt;/span&gt;，其他输入向量
求导方法与之相同，最后结果相等
&lt;img alt="J对输出向量求导" src="https://wugh.github.io/images/NLP/cbow-gradient-w1-ui.jpg" /&gt;&lt;/p&gt;
&lt;h4&gt;Skip-Gram&lt;/h4&gt;
&lt;p&gt;Skip-Gram的基本思想用当前词预测窗口长度为&lt;span class="math"&gt;\(C\)&lt;/span&gt;内的其他词，此时模型的输入是中心词one-hot
向量&lt;span class="math"&gt;\(x\)&lt;/span&gt;，窗口内的词one-hot向量为&lt;span class="math"&gt;\((y^{(i-C)},\ldots,y^{(i-1)},y^{(i+1)},\ldots,y^{(i+C)})\)&lt;/span&gt;。&amp;nbsp;利用中心词预测周边词的过程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;输入one-hot向量为&lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x\)&lt;/span&gt;的输入向量表示为&lt;span class="math"&gt;\(u^{(i)}=W^{(1)}x\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;只有一个输入所以不需要像&lt;span class="caps"&gt;CBOW&lt;/span&gt;一样对向量进行平均，&lt;span class="math"&gt;\(h=u^{(i)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;生成得分向量&lt;span class="math"&gt;\(z=W^{(2)}h\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;利用softmax函数将得分转换成概率&lt;span class="math"&gt;\(\hat{y}=\text{softmax}(z)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;我们的目标是希望预测的概率&lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;和真实的概率&lt;span class="math"&gt;\((y^{(i-C)},\ldots,y^{(i-1)},y^{(i+1)},\ldots,y^{(i+C)})\)&lt;/span&gt;一致&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这里把&lt;span class="math"&gt;\(P(w^{(i-C)},\ldots,w^{(i-1)},w^{(i+1)},\ldots,w^{(i+C)}|w^{(i)})\)&lt;/span&gt;整体看成一个分布，&amp;nbsp;然后用朴素贝叶斯假设来简化这个条件概率的求解，即：
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\min J&amp;amp;=-\log P(w^{(i-C)},\ldots,w^{(i-1)},w^{(i+1)},\ldots,w^{(i+C)}|w^{(i)})\\
&amp;amp;=-\log \Pi_{j=0,j\neq C}^{2C}P(w^{(i-C+j)}|w^{(i)})\\
&amp;amp;=-\log \Pi_{j=0,j\neq C}^{2C}P(v^{(i-C+j)}|u^{(i)})\\
&amp;amp;=-\log \Pi_{j=0,j\neq C}^{2C}\frac{\exp{(v^{(i-C+j)}\cdot h)}}{\sum_{k=1}^{|V|}\exp(v^{(k)}\cdot h)}\\
&amp;amp;=-\sum_{j=0,j\neq C}^{2C}v^{(i-C+j)}\cdot h + 2C\log{\sum_{k=1}^{|V|}\exp(v^{(k)}\cdot h)}\\
\end{align*}&lt;/div&gt;
&lt;p&gt;
对于这个损失函数我们可以先不考虑对窗口内的所有词求和，假设我们现在只针对窗口内的特定词&lt;span class="math"&gt;\(w^{(j)}\)&lt;/span&gt;进行预测，&amp;nbsp;此时
&lt;/p&gt;
&lt;div class="math"&gt;$$J=-v^{(j)}\cdot h + \log{\sum_{k=1}^{|V|}\exp(v^{(k)}\cdot h)}$$&lt;/div&gt;
&lt;p&gt;
该损失函数对于每个输出向量的导数的求解结果与&lt;span class="caps"&gt;CBOW&lt;/span&gt;类似，损失函数对于输入向量的求导结果也和&lt;span class="caps"&gt;CBOW&lt;/span&gt;类似。唯一的
区别在于skip-gram的输入向量只有一个，所以&lt;span class="math"&gt;\(J\)&lt;/span&gt;对于&lt;span class="math"&gt;\(u^{(i)}\)&lt;/span&gt;的导数直接为&lt;span class="math"&gt;\({W^{(2)}}^T(\hat{y}-y)\)&lt;/span&gt;不需要除以&lt;span class="math"&gt;\(2C\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4&gt;负采样&lt;/h4&gt;
&lt;p&gt;上述的&lt;span class="caps"&gt;CBOW&lt;/span&gt;和Skip-Gram模型都存在一个问题，就是损失函数都有一个求和是&lt;span class="math"&gt;\(|V|\)&lt;/span&gt;规模的，这个计算非常耗时，英文的词汇表
规模大概是1300万。负采样是基于Skip-Gram模型，但是和优化目标和Skip-Gram不同。给定一对词&lt;span class="math"&gt;\((w,c)\)&lt;/span&gt;，其中&lt;span class="math"&gt;\(w\)&lt;/span&gt;是中心词，
&lt;span class="math"&gt;\(c\)&lt;/span&gt;代表&lt;span class="math"&gt;\(w\)&lt;/span&gt;的上下文窗口内出现的另一个词，用&lt;span class="math"&gt;\(P(D=1|w,c)\)&lt;/span&gt;表示&lt;span class="math"&gt;\((w,c)\)&lt;/span&gt;确实出现在语料中的概率，相应的&lt;span class="math"&gt;\(P(D=0|w,c)\)&lt;/span&gt;表示&lt;span class="math"&gt;\((w,c)\)&lt;/span&gt;
没有出现在语料中的概率。用sigmoid函数（下文用&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;表示sigmoid函数）来表示&lt;span class="math"&gt;\(P(D=1|w,c)\)&lt;/span&gt;这个概率，这里用
&lt;span class="math"&gt;\(v_c\)&lt;/span&gt;表示输出向量，&lt;span class="math"&gt;\(u_w\)&lt;/span&gt;表示输入向量。
&lt;/p&gt;
&lt;div class="math"&gt;$$P(D=1|w,c,\theta)=\frac{1}{1+\exp(-v_c^Tu_w)}$$&lt;/div&gt;
&lt;p&gt;
我们期望优化的目标是希望真实存在语料中&lt;span class="math"&gt;\((w,c)\)&lt;/span&gt;对&lt;span class="math"&gt;\(P(D=1|w,c)\)&lt;/span&gt;最大化，同时不存在语料中的&lt;span class="math"&gt;\((w,c)\)&lt;/span&gt;对&lt;span class="math"&gt;\(P(D=0|w,c)\)&lt;/span&gt;概率最大化，
公式中的&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;包括了上面提到的两个矩阵&lt;span class="math"&gt;\(W^{(1)}\)&lt;/span&gt;和&lt;span class="math"&gt;\(W^{(2)}\)&lt;/span&gt;，我们希望&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;满足下列公式：
&lt;img alt="负采样优化目标" src="https://wugh.github.io/images/NLP/neg-sampling-object.jpg" /&gt;
上述推导中用到需要用到了一个等式&lt;span class="math"&gt;\(1-\sigma(x)=\sigma(-x)\)&lt;/span&gt;，这个等式比较好验证，其中&lt;span class="math"&gt;\(\tilde{D}\)&lt;/span&gt;为负样本。由此可得新的&amp;nbsp;目标函数
&lt;/p&gt;
&lt;div class="math"&gt;$$\min J=-\log\sigma(v_c^Tu_w)-\sum_{k=1}^{K}\log\sigma(-v_k^Tu_w)$$&lt;/div&gt;
&lt;p&gt;
在这个目标函数中&lt;span class="math"&gt;\(v_k\)&lt;/span&gt;是从&lt;span class="math"&gt;\(P_n(w)\)&lt;/span&gt;从采样出来的，这个&lt;span class="math"&gt;\(P_n(w)\)&lt;/span&gt;是unigram概率&lt;span class="math"&gt;\(U(w)\)&lt;/span&gt;的&lt;span class="math"&gt;\(3/4\)&lt;/span&gt;次方，&amp;nbsp;这样可以增大一些概率很小的词被采样出来的概率。&lt;/p&gt;
&lt;p&gt;可以求得&lt;span class="math"&gt;\(\frac{\partial J}{\partial u_w}\)&lt;/span&gt;
&lt;img alt="负采样输入向量梯度" src="https://wugh.github.io/images/NLP/neg-sampling-gradient-w1.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;然后是&lt;span class="math"&gt;\(J\)&lt;/span&gt;对相关的输出向量求导
&lt;img alt="负采样输出向量梯度" src="https://wugh.github.io/images/NLP/neg-sampling-gradient-w2.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;求得梯度之后可以采用随机梯度下降优化目标函数，得到向量表示。
上述三种方法训练完成之后对于每个词都会得到两个向量，一个是作为模型输入时的向量，
一个是作为模型输出时的向量，最后的向量表示是把这两种向量相加使用，即对于词&lt;span class="math"&gt;\(w^{(i)}\)&lt;/span&gt;
的向量为&lt;span class="math"&gt;\(v^{(i)}+u^{(i)}\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2&gt;词向量评测&lt;/h2&gt;
&lt;p&gt;词向量的评测方法可以分为内部（intrinsic）评测和外部（extrinsic）评测，两种评测的对比如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;内部评测：&lt;ul&gt;
&lt;li&gt;在一个特定的子任务中进行评测&lt;/li&gt;
&lt;li&gt;计算迅速&lt;/li&gt;
&lt;li&gt;有助于理解相关的系统&lt;/li&gt;
&lt;li&gt;不太清楚是否有助于真实任务除非和实际的&lt;span class="caps"&gt;NLP&lt;/span&gt;任务的相关性已经建立起来&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;外部评测：&lt;ul&gt;
&lt;li&gt;在一个真实任务中进行评测&lt;/li&gt;
&lt;li&gt;需要花很长的实际来计算精度&lt;/li&gt;
&lt;li&gt;不太清楚是否是这个子系统或者其他子系统引起的问题&lt;/li&gt;
&lt;li&gt;如果用这个子系统替换原有的系统后获得精度提升–&amp;gt;有效(Winning!)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;词向量内部评测方法&lt;/h3&gt;
&lt;p&gt;词向量内部评测方法主要有词向量类比、词向量相关度，这两种方法有相应的数据集。&lt;/p&gt;
&lt;p&gt;词向量类比的基本思想如下
&lt;img alt="词向量类比" src="https://wugh.github.io/images/NLP/word-vector-analogy.png" /&gt;&lt;/p&gt;
&lt;p&gt;目前评测的数据集主要是word2vec项目提供的&lt;a href="https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt"&gt;数据集&lt;/a&gt;
包含了语义类比和语法类比两种。语义类比的数据有州名包含城市名、首都和国家，&amp;nbsp;语法类比的数据是比较级类比和时态类比。&lt;/p&gt;
&lt;p&gt;词向量相关度的数据集例如&lt;a href="http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/"&gt;WordSim353&lt;/a&gt;，
该数据集是人为地给两个词的相关度打分（从0-10），然后通过计算词向量的Cosine相似度与&amp;nbsp;这个相关度进行对比。&lt;/p&gt;
&lt;h3&gt;词向量外部评测方法&lt;/h3&gt;
&lt;p&gt;简单来说就是把词向量应用于具体的任务中来评测不同的词向量对于任务整体性能的影响。
这里需要注意的问题是，应用于具体任务的时候是否还需要调整词向量，&amp;nbsp;一般来说调整词向量会降低向量的范化能力。所以一般具体任务的训练集足够大时才考虑调整词向量。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="word2vec"></category><category term="深度学习"></category></entry><entry><title>Pelican自定义Jinja过滤器</title><link href="https://wugh.github.io/posts/2015/09/pelican-custom-jinja-filter/" rel="alternate"></link><published>2015-09-28T10:18:00+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2015-09-28:posts/2015/09/pelican-custom-jinja-filter/</id><summary type="html">&lt;p&gt;使用&lt;code&gt;Pelican&lt;/code&gt;的时候发现需要在模板中使用&lt;code&gt;Hash&lt;/code&gt;函数，但是&lt;code&gt;Jinja&lt;/code&gt;并没有提供&lt;code&gt;Hash&lt;/code&gt;
过滤器，需要自己实现一个&lt;code&gt;Hash&lt;/code&gt;过滤器，这里以&lt;code&gt;Hash&lt;/code&gt;过滤器为例子讲讲如何在
&lt;code&gt;Pelican&lt;/code&gt;中使用自定义过滤器。&lt;/p&gt;
&lt;h3&gt;Hash函数功能实现&lt;/h3&gt;
&lt;p&gt;通过&lt;code&gt;Python&lt;/code&gt;的标准库函数实现一个&lt;code&gt;md5&lt;/code&gt;的哈希函数，函数的功能的是传入一个字符串
返回该字符串的&lt;code&gt;md5&lt;/code&gt;串，实现如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# (c) 2012-2014, Michael DeHaan &amp;lt;michael.dehaan@gmail.com&amp;gt;&lt;/span&gt;
&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;span class="c1"&gt;# This file is part of Ansible&lt;/span&gt;
&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;span class="c1"&gt;# Ansible is free software: you can redistribute it and/or modify&lt;/span&gt;
&lt;span class="c1"&gt;# it under the terms of the GNU General Public License as published by&lt;/span&gt;
&lt;span class="c1"&gt;# the Free Software Foundation, either version 3 of the License, or&lt;/span&gt;
&lt;span class="c1"&gt;# (at your option) any later version.&lt;/span&gt;
&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;span class="c1"&gt;# Ansible is distributed in the hope that it will be useful,&lt;/span&gt;
&lt;span class="c1"&gt;# but WITHOUT ANY WARRANTY; without even the implied warranty of&lt;/span&gt;
&lt;span class="c1"&gt;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&lt;/span&gt;
&lt;span class="c1"&gt;# GNU General Public License for more details.&lt;/span&gt;
&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;span class="c1"&gt;# You should have received a copy of the GNU General Public License&lt;/span&gt;
&lt;span class="c1"&gt;# along with Ansible.  If not, see &amp;lt;http://www.gnu.org/licenses/&amp;gt;.&lt;/span&gt;

&lt;span class="c1"&gt;# Make coding more python3-ish&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;__future__&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;absolute_import&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;division&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;print_function&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;__metaclass__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;


&lt;span class="c1"&gt;# Note, sha1 is the only hash algorithm compatible with python2.4 and with&lt;/span&gt;
&lt;span class="c1"&gt;# FIPS-140 mode (as of 11-2014)&lt;/span&gt;
&lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;hashlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;sha1&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sha1&lt;/span&gt;
&lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ImportError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sha&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;sha&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sha1&lt;/span&gt;

&lt;span class="c1"&gt;# Backwards compat only&lt;/span&gt;
&lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;hashlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;md5&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;_md5&lt;/span&gt;
&lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ImportError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;md5&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;md5&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;_md5&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ImportError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Assume we&amp;#39;re running in FIPS mode here&lt;/span&gt;
        &lt;span class="n"&gt;_md5&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;secure_hash_s&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hash_func&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sha1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Return a secure hash hex digest of data. &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="n"&gt;digest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hash_func&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;basestring&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;
        &lt;span class="n"&gt;digest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;UnicodeEncodeError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;digest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;digest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hexdigest&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="c1"&gt;# The checksum algorithm must match with the algorithm in ShellModule.checksum() method&lt;/span&gt;
&lt;span class="n"&gt;checksum_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;secure_hash_s&lt;/span&gt;

&lt;span class="c1"&gt;# Backwards compat functions.  Some modules include md5s in their return values&lt;/span&gt;
&lt;span class="c1"&gt;# Continue to support that for now.  As of ansible-1.8, all of those modules&lt;/span&gt;
&lt;span class="c1"&gt;# should also return &amp;quot;checksum&amp;quot; (sha1 for now)&lt;/span&gt;
&lt;span class="c1"&gt;# Do not use md5 unless it is needed for:&lt;/span&gt;
&lt;span class="c1"&gt;# 1) Optional backwards compatibility&lt;/span&gt;
&lt;span class="c1"&gt;# 2) Compliance with a third party protocol&lt;/span&gt;
&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;span class="c1"&gt;# MD5 will not work on systems which are FIPS-140-2 compliant.&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;md5s&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;_md5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MD5 not available.  Possibly running in FIPS mode&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;secure_hash_s&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_md5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这里的&lt;code&gt;md5s&lt;/code&gt;实现了我们需要的功能。&lt;/p&gt;
&lt;h3&gt;向Pelican注册Jinja过滤器&lt;/h3&gt;
&lt;p&gt;通过配置&lt;code&gt;Pelican&lt;/code&gt;的配置文件&lt;code&gt;pelicanconf.py&lt;/code&gt;注册过滤器的名字和对应的函数，如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# add customer filter&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;curdir&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;hashing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;md5s&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;md5hash&lt;/span&gt;
&lt;span class="n"&gt;JINJA_FILTERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;md5hash&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;md5hash&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;代码中注册了一个名字为&lt;code&gt;md5hash&lt;/code&gt;的过滤器，并且对应的函数名字也叫&lt;code&gt;md5hash&lt;/code&gt;。&lt;/p&gt;
&lt;h3&gt;在Jinja模板中使用自定义过滤器&lt;/h3&gt;
&lt;p&gt;下面的模板中通过文章的&lt;code&gt;url&lt;/code&gt;产生一个唯一的&lt;code&gt;key&lt;/code&gt;给多说插件使用
&lt;code&gt;data-thread-key="{{ article.url|md5hash }}"&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{%if DUOSHUO %}
&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;div&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;row&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;div&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;alpha span9&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
            &lt;span class="c"&gt;&amp;lt;!-- 多说评论框 start --&amp;gt;&lt;/span&gt;
            &lt;span class="c"&gt;&amp;lt;!--data-thread-key=&amp;quot;的ID&amp;quot;--&amp;gt;&lt;/span&gt;
            &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;div&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ds-thread&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;data-thread-key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;{{ article.url|md5hash }}&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;data-title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;{{ article.title|striptags }}&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;data-url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;{{article.url}}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;div&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
            &lt;span class="c"&gt;&amp;lt;!-- 多说评论框 end --&amp;gt;&lt;/span&gt;
            &lt;span class="c"&gt;&amp;lt;!-- 多说公共JS代码 start (一个网页只需插入一次) --&amp;gt;&lt;/span&gt;
            &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt; &lt;span class="na"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;text/javascript&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
            &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;duoshuoQuery&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;short_name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;{{DUOSHUO}}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
                &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;ds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;document&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;createElement&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;script&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
                    &lt;span class="nx"&gt;ds&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;text/javascript&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="nx"&gt;ds&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;async&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
                    &lt;span class="nx"&gt;ds&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;src&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;document&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;location&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;protocol&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;https:&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;https:&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;http:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;//static.duoshuo.com/embed.js&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
                    &lt;span class="nx"&gt;ds&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;charset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;UTF-8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
                    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;document&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;getElementsByTagName&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;head&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; 
                     &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="nb"&gt;document&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;getElementsByTagName&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;body&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]).&lt;/span&gt;&lt;span class="nx"&gt;appendChild&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ds&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
                &lt;span class="p"&gt;})();&lt;/span&gt;
                &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
            &lt;span class="c"&gt;&amp;lt;!-- 多说公共JS代码 end --&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;div&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;div&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
{%endif%}
&lt;/pre&gt;&lt;/div&gt;</summary><category term="Pelican"></category><category term="Jinja"></category><category term="Filter"></category></entry><entry><title>Linux通过Pulse混合麦克风和音频输出</title><link href="https://wugh.github.io/posts/2015/01/linux-pulse-mix-mic-and-computer-audio/" rel="alternate"></link><published>2015-01-08T21:08:00+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2015-01-08:posts/2015/01/linux-pulse-mix-mic-and-computer-audio/</id><summary type="html">&lt;p&gt;由于想在语音识别时候给麦克风加噪声，所以就在思考有没有办法通过混合麦克风输入和
电脑的音频输出混合起来实现加噪声的效果呢？经过Google找到以下的&lt;a href="https://www.youtube.com/watch?v=8hkCE7ETJRM"&gt;解决办法
&lt;/a&gt;，这种方法依赖
&lt;a href="http://zh.wikipedia.org/wiki/PulseAudio" title="PulseAudio"&gt;PulseAudio&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;安装工具&lt;/h3&gt;
&lt;p&gt;首先需要安装
&lt;a href="http://freedesktop.org/software/pulseaudio/pavucontrol/"&gt;pavucontrol&lt;/a&gt;用来管理&amp;nbsp;Pulse。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;emerge -a media-sound/pavucontrol
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;修改配置文件&lt;/h3&gt;
&lt;p&gt;修改配置文件&lt;code&gt;/etc/pulse/default.pa&lt;/code&gt;，添加以下模块&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load-module module-null-sink 
load-module module-loopback
load-module module-loopback
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;然后重启电脑或者重启Pulse服务器。&lt;/p&gt;
&lt;p&gt;上面的步骤也可以不重启Pulse服务器，直接通过&lt;code&gt;pacmd&lt;/code&gt;这个命令行工具配置Pulse服务&amp;nbsp;器，可以在这个命令行的交互工具中，执行以上三个命令。&lt;/p&gt;
&lt;h3&gt;通过pavucontrol配置音频混合&lt;/h3&gt;
&lt;p&gt;我们添加了两个loopback，这时候就可以在pavucontrol的Playback标签页下面看到
两个新的Loopback，需要把&lt;code&gt;Show&lt;/code&gt;这个过滤器选择为&lt;code&gt;All Streams&lt;/code&gt;，结果如下图所示：
&lt;img alt="Playback" src="https://wugh.github.io/images/Linux/playback.png" title="Playback" /&gt;
这两个Loopback一个是麦克风（Loopback from Bulit-in Audio Analog Stero）另一个
是电脑的音频输出（Loopback of Monitor of Bult-in Audio Analog Stero），需要把
这两个的输出设定成&lt;code&gt;Null Output&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;接着需要设定一下我们录音的时候使用哪个声音源，设置如下图
&lt;img alt="Record" src="https://wugh.github.io/images/Linux/record.png" title="Record" /&gt;
可以看到&lt;code&gt;ALSA Capture from&lt;/code&gt;被我们设置成&lt;code&gt;Monitor of Null Output&lt;/code&gt;，相当于把Null
这个设备的输出当作声音源进行捕获，而Null这个设备输出是麦克风和电脑声音的混合，&amp;nbsp;所以可以正常工作。&lt;/p&gt;</summary><category term="pulse"></category><category term="录音"></category></entry><entry><title>最大熵</title><link href="https://wugh.github.io/posts/2014/11/maxent/" rel="alternate"></link><published>2014-11-14T00:00:00+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2014-11-14:posts/2014/11/maxent/</id><summary type="html">&lt;h3&gt;最大熵原理&lt;/h3&gt;
&lt;p&gt;最大熵原理指的是当我们在估计概率分布的时候，这个概率分布符合已知信息的约束并且&amp;nbsp;该分布是最均匀的。从熵的角度考虑就是要让这个分布符合约束并且熵最大。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The principle of maximum entropy states that, subject to precisely stated
prior data (such as a proposition that expresses testable information), the
probability distribution which best represents the current state of knowledge
is the one with largest&amp;nbsp;entropy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;现在考虑一个文本分类的例子，假设我们有4个类别的文本分别是：economics、
sports、politics和art。因为文本只能被分成4个类别，假设现在没有额外的信息，&amp;nbsp;那么约束只有以下的1个
&lt;/p&gt;
&lt;div class="math"&gt;$$p(economics)+p(sports)+p(politics)+p(art)=1$$&lt;/div&gt;
&lt;p&gt;
那么我们希望得到得概率分布尽量均匀，就会得到下面结果
&lt;/p&gt;
&lt;div class="math"&gt;$$p(economics)=p(sports)=p(politics)=p(art)=0.25$$&lt;/div&gt;
&lt;p&gt;
现在假设我们有一个先验信息是60%的文档是economics或者sports类别的，那么我们的&amp;nbsp;概率分布就会有以下两个约束
&lt;/p&gt;
&lt;div class="math"&gt;$$p(economics)+p(sports)=0.6$$&lt;/div&gt;
&lt;div class="math"&gt;$$p(economics)+p(sports)+p(politics)+p(art)=1$$&lt;/div&gt;
&lt;p&gt;
考虑以上两个约束又希望使得我们的分布均匀，将会得到下面的结果
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
\notag
p(economics)&amp;amp;=0.30\cr
\notag
p(sports)&amp;amp;=0.30\cr
\notag
p(politics)&amp;amp;=0.20\cr
\notag
p(art)&amp;amp;=0.20 
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
但是随着对数据的观察，可能又会对引入其他约束。这时候需要解决两个问题，首先是如
何来定量描述分布的均匀；其次是如何在考虑约束的条件下使得分布均匀。最大熵的基本
思路就是选择一个与给定事实一致的模型（满足约束），并且要使得模型对未知事实不做&amp;nbsp;假设（使得分布尽量均匀）。&lt;/p&gt;
&lt;h3&gt;最大熵建模&lt;/h3&gt;
&lt;p&gt;我们继续考虑上面提到的文本分类的例子，假设用最简单的&lt;a href="http://en.wikipedia.org/wiki/Bag-of-words_model" title="Bag-of-words model"&gt;词袋模型&lt;/a&gt;来表示文档，&amp;nbsp;每个文档都可以表示成一个词频向量，例如下面的简单的例子&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Doc1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;John&lt;/span&gt; &lt;span class="n"&gt;likes&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;watch&lt;/span&gt; &lt;span class="n"&gt;movies&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Mary&lt;/span&gt; &lt;span class="n"&gt;likes&lt;/span&gt; &lt;span class="n"&gt;movies&lt;/span&gt; &lt;span class="n"&gt;too&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Doc2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;John&lt;/span&gt; &lt;span class="n"&gt;also&lt;/span&gt; &lt;span class="n"&gt;likes&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;watch&lt;/span&gt; &lt;span class="n"&gt;football&lt;/span&gt; &lt;span class="n"&gt;games&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;得到词表如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{
    &amp;quot;John&amp;quot;: 1,
    &amp;quot;likes&amp;quot;: 2,
    &amp;quot;to&amp;quot;: 3,
    &amp;quot;watch&amp;quot;: 4,
    &amp;quot;movies&amp;quot;: 5,
    &amp;quot;also&amp;quot;: 6,
    &amp;quot;football&amp;quot;: 7,
    &amp;quot;games&amp;quot;: 8,
    &amp;quot;Mary&amp;quot;: 9,
    &amp;quot;too&amp;quot;: 10
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;所以可以把Doc1和Doc2表示成两个10维的向量，如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Doc1: [1, 2, 1, 1, 2, 0, 0, 0, 1, 1]
Doc2: [1, 1, 1, 1, 0, 1, 1, 1, 0, 0]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;我们把当个文档用&lt;span class="math"&gt;\(x\)&lt;/span&gt;表示，文档类别用&lt;span class="math"&gt;\(y\)&lt;/span&gt;表示，根据上面的描述&lt;span class="math"&gt;\(y\)&lt;/span&gt;会有4个类别，跟
&lt;a href="http://en.wikipedia.org/wiki/Logistic_regression" title="Logistic regression"&gt;Logistic Regression&lt;/a&gt;类似，我们要建模一个判别模型&lt;span class="math"&gt;\(p(y|x)\)&lt;/span&gt;，即给定
一个文档&lt;span class="math"&gt;\(x\)&lt;/span&gt;，我们的模型可以得出这篇文档属于特定类别&lt;span class="math"&gt;\(y\)&lt;/span&gt;的概率。如果用&lt;span class="math"&gt;\(\mathcal{P}\)&lt;/span&gt;
表示所有的条件概率分布，那么&lt;span class="math"&gt;\(p(y|x)\)&lt;/span&gt;就是&lt;span class="math"&gt;\(\mathcal{P}\)&lt;/span&gt;中的一个元素。&lt;/p&gt;
&lt;h4&gt;训练数据&lt;/h4&gt;
&lt;p&gt;我们可以把的&lt;span class="math"&gt;\(N\)&lt;/span&gt;个训练数据表示成&lt;span class="math"&gt;\(\\{(x_1,y_1),\cdots,(x_N,y_N)\\}\)&lt;/span&gt;，且&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x_i\in X\)&lt;/span&gt;，其中&lt;span class="math"&gt;\(X\)&lt;/span&gt;表示所有的文档类别&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(y_i\in Y\)&lt;/span&gt;，其中&lt;span class="math"&gt;\(Y\)&lt;/span&gt;表示训练文档集合，每个文档用词频向量表示&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从训练数据样本中我们可以估计出一个经验的联合概率分布
&lt;/p&gt;
&lt;div class="math"&gt;$$\tilde{p}(x,y)=\frac{1}{N}\times \text{the number of times (x,y) appears}$$&lt;/div&gt;
&lt;h4&gt;特征和约束&lt;/h4&gt;
&lt;p&gt;最大熵建模的过程中特征函数（简称特征）主要是为了描述训练样本的统计量，例如&amp;nbsp;建模的时候我们可能会考虑当ball这个词出现的时候，文档属于sports的概率是9/10。&lt;/p&gt;
&lt;p&gt;为了描述ball出现时文档属于sports类别的事实我们引入特征函数（指示函数或者特征）
&lt;/p&gt;
&lt;div class="math"&gt;$$
f_{ball,sports} (x,y)=\begin{cases}
1 &amp;amp; \text{if y=sports and ball appears in d} \cr
0 &amp;amp; \text{otherwise}
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;
考虑经验分布&lt;span class="math"&gt;\(\tilde{p}(x,y)\)&lt;/span&gt;下&lt;span class="math"&gt;\(f\)&lt;/span&gt;的期望值，这个期望值就是我们关注的统计量（如
果对上面那个特征函数按照经验分布求期望，得到的东西就是当ball这个词出现的时候文&amp;nbsp;档属于sports的概率），记为
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
\tilde{p}(f)\equiv \sum_{x,y}{\tilde{p}(x,y)f(x,y)}
\end{equation}
\label{eq:empricalException}
$$&lt;/div&gt;
&lt;p&gt;
其实样本的任何统计量都可以通过特征函数的期望值来表示。&lt;/p&gt;
&lt;p&gt;如果我们发现了一些有用的统计量，我们就可以要求我们的模型也要遵循这个信息。我们
通过约束模型在特征函数&lt;span class="math"&gt;\(f\)&lt;/span&gt;上的期望值来遵循这个统计量，对于模型&lt;span class="math"&gt;\(p(y|x)\)&lt;/span&gt;特征函数
&lt;span class="math"&gt;\(f\)&lt;/span&gt;的期望值为
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
p(f)\equiv \sum_{x,y}{\tilde{p}(x)p(y|x)f(x,y)}
\label{eq:modelException}
\end{equation}
$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(\tilde{p}(x)\)&lt;/span&gt;是训练样本中&lt;span class="math"&gt;\(x\)&lt;/span&gt;的经验分布。我们通过约束模型下特征函数的期望&amp;nbsp;值等于样本中特征函数的期望值，如下所示
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
p(f) = \tilde{p}(f)
\label{eq:constriant}
\end{equation}
$$&lt;/div&gt;
&lt;p&gt;把前面的&lt;span class="math"&gt;\(\ref{eq:empricalException}\)&lt;/span&gt;，&lt;span class="math"&gt;\(\ref{eq:modelException}\)&lt;/span&gt;和
&lt;span class="math"&gt;\(\ref{eq:constriant}\)&lt;/span&gt;综合起来得意得到下面的约束&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{x,y}{\tilde{p}(x)p(y|x)f(x,y)} = \sum_{x,y}{\tilde{p}(x,y)f(x,y)}
$$&lt;/div&gt;
&lt;p&gt;这样我们只需要考虑那些满足&lt;span class="math"&gt;\(\ref{eq:constriant}\)&lt;/span&gt;的模型&lt;span class="math"&gt;\(p(y|x)\)&lt;/span&gt;。总的来说
我们用&lt;span class="math"&gt;\(\tilde{p}(f)\)&lt;/span&gt;表示样本数据中的统计特征，同时也要求我们的模型要表示出
这种特征（&lt;span class="math"&gt;\(p(f)=\tilde{p}(f)\)&lt;/span&gt;）。&lt;/p&gt;
&lt;p&gt;假设我们现在有&lt;span class="math"&gt;\(n\)&lt;/span&gt;个我们认为很重要的特征函数&lt;span class="math"&gt;\(f_i\)&lt;/span&gt;，我们希望我们的模型遵循这些
特征在训练数据中所表现出的统计信息，那么&lt;span class="math"&gt;\(p\)&lt;/span&gt;就应该是&lt;span class="math"&gt;\(\mathcal{P}\)&lt;/span&gt;的一个子集
&lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt;，定义如下
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
\mathcal{C} = \big\{p \in P \text{ | } p(f_i)=\tilde{p}(f_i) \text{ for i } \in \{1,2,\cdots,n\}\big\}
\label{eq:distsubset}
\end{equation}
$$&lt;/div&gt;
&lt;p&gt;
下图解释为概率分布添加约束的过程，图中&lt;span class="math"&gt;\(\mathcal{P}\)&lt;/span&gt;表示一个在3个变量上的概率分
布，如果我们没有设置任何约束那么所有的概率分布都是可以的如图(a)所示；如果我们
添加一个线性约束&lt;span class="math"&gt;\(\mathcal{C}_1\)&lt;/span&gt;那么我们的概率分布只能落在图(b)中&lt;span class="math"&gt;\(mathcal{C}_1\)&lt;/span&gt;
这个线上面；此时如果再添加一个约束我们就能确定概率分布&lt;span class="math"&gt;\(p\)&lt;/span&gt;，如果第二个线性约束
&lt;span class="math"&gt;\(\mathcal{C}_2\)&lt;/span&gt;和&lt;span class="math"&gt;\(\mathcal{C}_1\)&lt;/span&gt;不冲突（有交点），那么这个交点就是我们要求的概
率分布&lt;span class="math"&gt;\(p\)&lt;/span&gt;，如图(c)所示；如果两个约束冲突，例如第一个约束要求第1点的概率是&lt;span class="math"&gt;\(1/3\)&lt;/span&gt;
而第二个约束是要求第3点的概率是&lt;span class="math"&gt;\(3/4\)&lt;/span&gt;，那么会得到图(d)的结果。由于我们的约束都
是从训练样本中抽取的，所以约束之间不可能冲突，而且我们的约束无法像图(c)一样唯
一确定&lt;span class="math"&gt;\(p\)&lt;/span&gt;，换句话说&lt;span class="math"&gt;\(\mathcal{C}=\mathcal{C}_1\cap\mathcal{C}_2\cdots\cap\mathcal{C}_n\)&lt;/span&gt;
所确定的模型有无数个。
&lt;img alt="simplex" src="https://wugh.github.io/images/ML/simplex.png" title="simplex" /&gt;&lt;/p&gt;
&lt;p&gt;在所有的模型&lt;span class="math"&gt;\(p\in\mathcal{C}\)&lt;/span&gt;中我们需要根据最大熵原理选择一个最均匀的，我们用
&lt;a href="http://en.wikipedia.org/wiki/Conditional_entropy" title="条件熵"&gt;条件熵&lt;/a&gt;量化度量条件分布&lt;span class="math"&gt;\(p(y|x)\)&lt;/span&gt;的均匀程度
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
H(p)\equiv -\sum_{x,y}{\tilde{p}(x)p(y|x)\log{p(y|x)}}
\label{eq:conditionalentropy}
\end{equation}
$$&lt;/div&gt;
&lt;p&gt;
条件熵的取值其下界是0（没有不确定性），上界是&lt;span class="math"&gt;\(\log{|Y|}\)&lt;/span&gt;（在所有&lt;span class="math"&gt;\(y\)&lt;/span&gt;的取值上均
匀分布）。我们的目的就是要从&lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt;里面找到一个模型&lt;span class="math"&gt;\(p^*\in\mathcal{C}\)&lt;/span&gt;使
得&lt;span class="math"&gt;\(H(p)\)&lt;/span&gt;最大，即
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
p^*= \mathop{\arg\,\max}\limits_{p\in\mathcal{C}}H(p)
\end{equation}
$$&lt;/div&gt;
&lt;h3&gt;指数形式&lt;/h3&gt;
&lt;p&gt;经过上节的分析其实可以知道最大熵模型的目的就是要找到一个模型
&lt;span class="math"&gt;\(p^*\in\mathcal{C}\)&lt;/span&gt;使得&lt;span class="math"&gt;\(H(p)\)&lt;/span&gt;最大。这其实就是一个有约束条件下的最优化问题，可
以用&lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier" title="拉格朗日数乘"&gt;拉格朗日乘数法&lt;/a&gt;来解，原始优化问题形式如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp; max_p &amp;amp;&amp;amp; H(p) \\
&amp;amp; s.t. &amp;amp;&amp;amp; p(y|x) \leq 0\text{ for all }x,y.\\
&amp;amp;&amp;amp;&amp;amp; \sum_yp(y|x)=1\text{ for all }x. \\
&amp;amp;&amp;amp;&amp;amp; \sum_{x,y}\tilde{p}(x)p(y|x)f(x,y)=\sum_{x,y}\tilde{p}(x,y)f(x,y)\text{ for }
i\in\left\{1,2,...,n\right\}. 
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;
前两个约束保证模型是一个条件概率分布，第三个约束值得是模型需要满足的统计量。该
问题等价于在相同约束下最小化&lt;span class="math"&gt;\(-H(p)\)&lt;/span&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation}\begin{split}
&amp;amp; min_p &amp;amp;&amp;amp; -H(p) \\
&amp;amp; s.t. &amp;amp;&amp;amp; p(y|x) \leq 0\text{ for all }x,y.\\
&amp;amp;&amp;amp;&amp;amp; \sum_yp(y|x)=1\text{ for all }x.\\
&amp;amp;&amp;amp;&amp;amp; \sum_{x,y}\tilde{p}(x)p(y|x)f(x,y)=\sum_{x,y}\tilde{p}(x,y)f(x,y)\text{ for }
i\in\left\{1,2,...,n\right\}.
\end{split}
\label{eq:primal}
\end{equation}$$&lt;/div&gt;
&lt;p&gt;
用拉格朗日乘数法将有约束问题转换成无约束问题，拉格朗日方程如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation}\begin{split}
\mathcal{L}(p,\Lambda,\gamma)=&amp;amp;\sum_{x,y}\tilde{p}(x)p(y|x)\log p(y|x)\\
&amp;amp;+\sum_i^n\lambda_i\left(sum_{x,y}\tilde{p}(x,y)f_i(x,y)-\sum_{x,y}\tilde{p}(x)p(y|x)f_i(x,y)\right)\\
&amp;amp;+\gamma(\sum_yp(y|x) - 1)
\end{split}
\label{eq:lagrangian}
\end{equation}$$&lt;/div&gt;
&lt;p&gt;
对于&lt;span class="math"&gt;\(\ref{eq:lagrangian}\)&lt;/span&gt;这个拉格朗日方程原问题如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation}
\min_w\max_{\Lambda,\gamma}\mathcal{L}(p,\Lambda,\gamma)
\end{equation}$$&lt;/div&gt;
&lt;p&gt;
对偶问题为：
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation}
\max_{\Lambda,\gamma}\min_w\mathcal{L}(p,\Lambda,\gamma)
\label{eq:dual}
\end{equation}$$&lt;/div&gt;
&lt;p&gt;
由于&lt;span class="math"&gt;\(p\)&lt;/span&gt;是一个凸函数，并且两个约束都和&lt;span class="math"&gt;\(p\)&lt;/span&gt;呈线性关系，所以原始问题的解和对偶问题
的解是等价的，下面求如何最大化对偶问题&lt;span class="math"&gt;\(\ref{eq:dual}。首先固定\)&lt;/span&gt;Lambda&lt;span class="math"&gt;\(和
$\gamma\)&lt;/span&gt;求&lt;span class="math"&gt;\(\mathcal{L}(p,\Lambda,\gamma)\)&lt;/span&gt;的最小值，将&lt;span class="math"&gt;\(\ref{eq:lagrangian}\)&lt;/span&gt;对
&lt;span class="math"&gt;\(p\)&lt;/span&gt;求导，另求导结果等于0，得到：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathcal{L}(p,\Lambda,\gamma)}{\partial
p(y|x)}=\tilde{p}(x)\left(1+\log
p(y|x)\right)-\sum_i\lambda_i\tilde{p}(x)f_i(x,y) + \gamma=0
$$&lt;/div&gt;
&lt;p&gt;
可以求得最优的&lt;span class="math"&gt;\(p(y|x)\)&lt;/span&gt;具有如下形式：
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation}\begin{split}
&amp;amp;\log p^*(y|x)=\sum_i\lambda_if_i(x,y)-\frac{\gamma}{\tilde{p}(x)}-1\\
\Rightarrow&amp;amp;p*(y|x)=\exp\left(\sum_i\lambda_if_i(x,y)\right)\exp\left(-\frac{\gamma)}{\tilde{p}(x)}-1\right)
\end{split}
\label{eq:optimalp}
\end{equation}$$&lt;/div&gt;
&lt;p&gt;
这样我们就找到了&lt;span class="math"&gt;\(p^*\)&lt;/span&gt;的最优化形式，现在的目标就是要去求解&lt;span class="math"&gt;\(\gamma^*\)&lt;/span&gt;和
&lt;span class="math"&gt;\(\Lambda^*\)&lt;/span&gt;。注意到&lt;span class="math"&gt;\(\ref{eq:optimalp}\)&lt;/span&gt;的第二项实际上对应的就是原始束问题
&lt;span class="math"&gt;\(\ref{eq:primal}\)&lt;/span&gt;的第二个约束，可以把&lt;span class="math"&gt;\(\ref{eq:optimalp}\)&lt;/span&gt;写成如下形式：
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation}\begin{split}
p*(y|x)&amp;amp;=\frac{p^*(y|x)}{\sum_yp^*(y|x)}\\
&amp;amp;=\frac{\exp\left(\sum_i\lambda_if_i(x,y)\right)\exp\left(-\frac{\gamma)}{\tilde{p}(x)}-1\right)}{\sum_y{\exp\left(\sum_i\lambda_if_i(x,y)\right)\exp\left(-\frac{\gamma)}{\tilde{p}(x)}-1\right)}}\\
&amp;amp;=\frac{\exp\left(\sum_i\lambda_if_i(x,y)\right)}{Z(x)}
\end{split}
\label{eq:optimalpnew}
\end{equation}$$&lt;/div&gt;
&lt;p&gt;
其中
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation}
Z(x)=\sum_y\exp\left(\sum_i\lambda_if_i(x,y)\right)
\end{equation}$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(\ref{eq:optimalpnew}\)&lt;/span&gt;就是最终&lt;span class="math"&gt;\(p^*\)&lt;/span&gt;的参数形式，并且满足&lt;span class="math"&gt;\(\ref{eq:primal}\)&lt;/span&gt;的第二
个约束，此时相当于我们已经找到了最优的&lt;span class="math"&gt;\(p^*\)&lt;/span&gt;和&lt;span class="math"&gt;\(\gamma^*\)&lt;/span&gt;，&lt;span class="math"&gt;\(p^*\)&lt;/span&gt;带入
拉格朗日方程&lt;span class="math"&gt;\(\ref{eq:lagrangian}\)&lt;/span&gt;中，得到对偶函数：
&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{align}
\Psi(\Lambda)&amp;amp;=\mathcal{L}(p^*,\Lambda,\gamma^*)\nonumber\\
&amp;amp;=\sum_{x,y}\tilde{p}(x)p^*(y|x)\log p^*(y|x)+\sum_i^n\lambda_i\left(\sum_{x,y}\tilde{p}(x,y)f_i(x,y)-\sum_{x,y}\tilde{p}(x)p^*(y|x)f_i(x,y)\right)\nonumber\\
&amp;amp;=\sum_{x,y}\tilde{p}(x,y)\sum_i\lambda_i f_i(x,y)+\sum_{x,y}\tilde{p}(x)p^*(y|x)\left(\log p^*(y|x)-\sum_i\lambda_i f_i(x,y)\right)\nonumber\\
&amp;amp;=\sum_{x,y}\tilde{p}(x,y)\sum_i\lambda_i f_i(x,y)-\sum_{x,y}\tilde{p}(x)p^*(y|x)\log Z(x)\nonumber\\
&amp;amp;=\sum_{x,y}\tilde{p}(x,y)\sum_i\lambda_i f_i(x,y)-\sum_{x}\tilde{p}(x)\log Z(x)
\label{eq:optimallambda}
\end{align} $$&lt;/div&gt;
&lt;p&gt;
所以现在的对偶问题&lt;span class="math"&gt;\(\ref{eq:dual}\)&lt;/span&gt;相当于是要优化如下目标：
&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{align}
\max_{\Lambda}\Psi(\Lambda)=\max_{\Lambda}\left[\sum_{x,y}\tilde{p}(x,y)\sum_i\lambda_i f_i(x,y)-\sum_{x}\tilde{p}(x)\log Z(x)\right]
\end{align} $$&lt;/div&gt;
&lt;p&gt;
最优的&lt;span class="math"&gt;\(\Lambda^*\)&lt;/span&gt;需要满足：
&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{align}
\DeclareMathOperator*{\argmax}{arg\,max}
\Lambda^*=\argmax_{\Lambda}\Psi(\Lambda)=\argmax_{\Lambda}\left[\sum_{x,y}\tilde{p}(x,y)\sum_i\lambda_i f_i(x,y)-\sum_{x}\tilde{p}(x)\log Z(x)\right]
\end{align} $$&lt;/div&gt;
&lt;h3&gt;最大似然&lt;/h3&gt;
&lt;p&gt;已知训练数据的经验分布&lt;span class="math"&gt;\(\tilde{p}(x,y)\)&lt;/span&gt;，模型&lt;span class="math"&gt;\(p(y|x)\)&lt;/span&gt;的对数似然函数表示为：
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
L_{\tilde{p}}(p) &amp;amp;\equiv \log\Pi_{x,y}p(y|x)^{\tilde{p}(x,y)}=\sum_{x,y}\tilde{p}(x,y)\log p(y|x) \nonumber \\
&amp;amp;=\sum_{x,y}\tilde{p}(x,y)\sum_i\lambda_i f_i(x,y)-\sum_{x,y}\tilde{p}(x,y)\log Z(x) \nonumber \\G
&amp;amp;=\sum_{x,y}\tilde{p}(x,y)\sum_i\lambda_i f_i(x,y)-\sum_{x,y}\tilde{p}(x)\log Z(x)
\label{eq:likehood}
\end{align}$$&lt;/div&gt;
&lt;p&gt;
可以看出对偶函数&lt;span class="math"&gt;\(\Psi(\Lambda)\)&lt;/span&gt;形式&lt;span class="math"&gt;\(\ref{eq:optimallambda}\)&lt;/span&gt;和模型&lt;span class="math"&gt;\(p\)&lt;/span&gt;的对数似然
结果是等价的。所以整个对偶问题的求解找到的熵最大的模型&lt;span class="math"&gt;\(p*\)&lt;/span&gt;其实也最大化了模型在&amp;nbsp;训练样本上的似然。&lt;/p&gt;
&lt;h3&gt;参数求解&lt;/h3&gt;
&lt;p&gt;对于一般的问题一般无法用数学分析的方法求解出最大化&lt;span class="math"&gt;\(\Psi(\Lambda)\)&lt;/span&gt;的&lt;span class="math"&gt;\(\Lambda^*\)&lt;/span&gt;
，一般需要用数值方法来求解。因为&lt;span class="math"&gt;\(\Psi(\Lambda)\)&lt;/span&gt;是一个光滑的凸函数，所以有很多
方法都可以用来求&lt;span class="math"&gt;\(\Lambda^*\)&lt;/span&gt;，例如梯度下降、共轭梯度、坐标上升等方法。这里介绍
的是专门针对最大熵问题设计的&lt;code&gt;改进的尺度迭代算法（improved iterative scaling,
IIS）&lt;/code&gt;，该算法要求所有的特征函数&lt;span class="math"&gt;\(f_i(x,y)\)&lt;/span&gt;必须非负。
&lt;img alt="IIS算法" src="https://wugh.github.io/images/ML/iis.png" /&gt;
算法的关键在于求解第3步的&lt;span class="math"&gt;\(\Delta\lambda_i\)&lt;/span&gt;，如果这时候&lt;span class="math"&gt;\(f^\#(x,y)\)&lt;/span&gt;（表示某个样
本&lt;span class="math"&gt;\(x,y\)&lt;/span&gt;激活的特征函数个数）对于所有的&lt;span class="math"&gt;\(x,y\)&lt;/span&gt;都一样，即&lt;span class="math"&gt;\(f^\#(x,y)\)&lt;/span&gt;是一个常数&lt;span class="math"&gt;\(M\)&lt;/span&gt;，
那么&lt;span class="math"&gt;\(\Delta\lambda_i\)&lt;/span&gt;可以按下面式子求解
&lt;/p&gt;
&lt;div class="math"&gt;$$
\Delta\lambda_i = \frac{1}{M}\log\frac{\tilde{p}(f_i)}{p_\Lambda(f_i)}
$$&lt;/div&gt;
&lt;p&gt;
如果&lt;span class="math"&gt;\(f^\#(x,y)\)&lt;/span&gt;不是一个常数，那么&lt;span class="math"&gt;\(\Delta\lambda_i\)&lt;/span&gt;需要通过数值方法计算。一个简
单快速的方法是通过&lt;a href="https://en.wikipedia.org/wiki/Newton%27s_method" title="牛顿法"&gt;牛顿法&lt;/a&gt;来求解，相当于这时候的目标函数
&lt;span class="math"&gt;\(g(\Delta\lambda_i)\)&lt;/span&gt;就是算法第3步那个方程把右边那一项移到左边的函数，现在的目
标就是要求&lt;span class="math"&gt;\(g(\Delta\lambda_i)=0\)&lt;/span&gt;的&lt;span class="math"&gt;\(\Delta\lambda_i\)&lt;/span&gt;，可以按下面的更新公式求
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{equation}
\Delta\lambda_i^{n+1}=\Delta\lambda_i^n-\frac{g(\Delta\lambda_i^n)}{g'(\Delta\lambda_i^n)}
\end{equation}$$&lt;/div&gt;
&lt;p&gt;
通过选取适当的初始&lt;span class="math"&gt;\(\Delta\lambda_i^0\)&lt;/span&gt;，由于&lt;span class="math"&gt;\(g(\Delta\lambda_i)=0\)&lt;/span&gt;有单根，牛顿&amp;nbsp;法可以快速收敛。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="MaxEnt"></category></entry><entry><title>MDP入门</title><link href="https://wugh.github.io/posts/2014/04/an-introduction-to-mdp/" rel="alternate"></link><published>2014-04-25T19:19:36+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2014-04-25:posts/2014/04/an-introduction-to-mdp/</id><summary type="html">&lt;h3&gt;介绍&lt;/h3&gt;
&lt;p&gt;&lt;span class="caps"&gt;MDP&lt;/span&gt;（Markov Decision
Process）由5元组构成&lt;span class="math"&gt;\(MDP(S,A,{P_{sa}},\gamma,R)\)&lt;/span&gt;，具体的&amp;nbsp;参数介绍如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(S\)&lt;/span&gt;：状态集合&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(A\)&lt;/span&gt;：动作集合&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P_{sa}\)&lt;/span&gt;：状态转移概率分布，&lt;span class="math"&gt;\(P_{sa}(s')\)&lt;/span&gt;表示在&lt;span class="math"&gt;\(s\)&lt;/span&gt;状态下采取
  &lt;span class="math"&gt;\(s\)&lt;/span&gt;动作，转移到&lt;span class="math"&gt;\(s'\)&lt;/span&gt;的概率，&lt;span class="math"&gt;\(P_{sa}(s')\geq0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;：折扣系数取值范围&lt;span class="math"&gt;\(0\leq\gamma\le1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(R\)&lt;/span&gt;：回报函数，&lt;span class="math"&gt;\(R:S\mapsto&amp;nbsp;\mathbb{R}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面举一个例子来说明&lt;span class="caps"&gt;MDP&lt;/span&gt;的参数，假设一个机器人在如所描述的
网格中走动，灰色代表障碍物，当机器人走到&lt;span class="math"&gt;\((4,3)\)&lt;/span&gt;位置将获得&lt;span class="math"&gt;\(+1\)&lt;/span&gt;的回报，走到
&lt;span class="math"&gt;\((4,2)\)&lt;/span&gt;位置回报为&lt;span class="math"&gt;\(-1\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="MDP简单例子" src="https://wugh.github.io/images/ML/simple-pomdp.png" /&gt;
&lt;img alt="动作执行受噪声干扰" src="https://wugh.github.io/images/ML/noisy.png" /&gt;&lt;/p&gt;
&lt;p&gt;如果用&lt;span class="caps"&gt;MDP&lt;/span&gt;来描述这个例子，那么&lt;span class="math"&gt;\(S\)&lt;/span&gt;就有&lt;span class="math"&gt;\(11\)&lt;/span&gt;中取值，机器人可能处在除了障碍物的其他
位置；&lt;span class="math"&gt;\(A\)&lt;/span&gt;就有&lt;span class="math"&gt;\(4\)&lt;/span&gt;种取值，机器人可以往&lt;span class="math"&gt;\({N,S,E,W}\)&lt;/span&gt;四个方向走；假设现在处于&lt;span class="math"&gt;\([3,1]\)&lt;/span&gt;
位置，采取动作&lt;span class="math"&gt;\(N\)&lt;/span&gt;（虽然命令机器人向前走，但是由于噪声的影响，可能机器人会向左
或者向右走，如），假设&lt;span class="math"&gt;\(P_{[3,1]N}\)&lt;/span&gt;分布如下
&lt;span class="math"&gt;\(P_{[3,1]N}([3,2])=0.8\)&lt;/span&gt;，&lt;span class="math"&gt;\(P_{[3,1]N}([2,1])=0.1\)&lt;/span&gt;，&lt;span class="math"&gt;\(P_{[3,1]N}([4,1])=0.1\)&lt;/span&gt;，
&lt;span class="math"&gt;\(P_{[3,1]N}([1,1])=0\)&lt;/span&gt;等（除了相邻的位置，其他位置都无法到达，所以为&lt;span class="math"&gt;\(0\)&lt;/span&gt;）；回报
函数&lt;span class="math"&gt;\(R\)&lt;/span&gt;，&lt;span class="math"&gt;\(R([4,2])=-1\)&lt;/span&gt;，&lt;span class="math"&gt;\(R([4,3])=+1\)&lt;/span&gt;，对于其他位置而言&lt;span class="math"&gt;\(R(s)=-0.02\)&lt;/span&gt;，因为当机器&amp;nbsp;人每走动一步都需要消耗电量，所以对于中间状态回报是一个比较小的负数。&lt;/p&gt;
&lt;p&gt;对于的描述，状态的变化过程如下描述，假设&lt;span class="math"&gt;\(0\)&lt;/span&gt;时刻状态是&lt;span class="math"&gt;\(s_0\)&lt;/span&gt;
，然后选择一个动作&lt;span class="math"&gt;\(a_0\)&lt;/span&gt;，根据&lt;span class="math"&gt;\(s\_1 \thicksim P_{s_0a_0}\)&lt;/span&gt;分布选择目标状态&lt;span class="math"&gt;\(s_1\)&lt;/span&gt;，再选择
动作&lt;span class="math"&gt;\(a_1\)&lt;/span&gt;，根据&lt;span class="math"&gt;\(s\_2 \thicksim P_{s_1a_1}\)&lt;/span&gt;选择目标状态&lt;span class="math"&gt;\(s_2\)&lt;/span&gt;，依此类推状态序列。
对于这个状态变化序列，可以计算总的回报值（Total&amp;nbsp;Payoff）。&lt;/p&gt;
&lt;p&gt;初始状态是&lt;span class="math"&gt;\(s_0\)&lt;/span&gt;的总回报定义如下，&lt;span class="math"&gt;\(0\leq \gamma \le 1\)&lt;/span&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots
    \label{eq:totalpayoff}$$&lt;/div&gt;
&lt;p&gt;
总的回报是当前的回报，加上未来的回报，但是距离当前越远回报值权重越小，为了使得
总的回报值最大，我们需要选择一组最优动作序列&lt;span class="math"&gt;\((a_0,a_1,\dots)\)&lt;/span&gt;使得总回报的期望最大：&lt;/p&gt;
&lt;div class="math"&gt;$$E[R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots]
  \label{eq:expectedpayoff}$$&lt;/div&gt;
&lt;p&gt;最后还需要引入一个定义&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;：策略&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;指的是，在给定状态选择一个动作，映射关系
为：&lt;span class="math"&gt;\(S\mapsto A\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;所以选择一个最优的动作序列，就是说要找到一个最优的&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;，对于
能够求解出如的最优&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;，下面的章节会解释如何求解&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3&gt;&lt;span class="caps"&gt;MDP&lt;/span&gt;求解&lt;/h3&gt;
&lt;p&gt;本节我们需要定义几个辅助变量：&lt;span class="math"&gt;\(V^{\pi}\)&lt;/span&gt;，&lt;span class="math"&gt;\(V^*\)&lt;/span&gt;和&lt;span class="math"&gt;\(\pi^*\)&lt;/span&gt;，下面将逐步介绍&amp;nbsp;定义。&lt;/p&gt;
&lt;h4&gt;&lt;span class="math"&gt;\(V^\pi\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;对于任意&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;都可以定义一个值函数&lt;span class="math"&gt;\(V^{\pi}\)&lt;/span&gt;（映射关系是&lt;span class="math"&gt;\(S\mapsto \mathbb{R}\)&lt;/span&gt;）
，&lt;span class="math"&gt;\(V^{\pi}\)&lt;/span&gt;指的是从状态&lt;span class="math"&gt;\(s\)&lt;/span&gt;开始并执行策略&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;之后所得到的总回报值的期望：&lt;/p&gt;
&lt;div class="math"&gt;$$V^{\pi}=E\Big[R(s_0) + \gamma R(s_1) + \dots|\pi, s_0=s\Big]
    \label{eq:vpi}$$&lt;/div&gt;
&lt;p&gt;下面是一个具体的例子，如是一个&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;，是与之
对应的&lt;span class="math"&gt;\(V^\pi\)&lt;/span&gt;，实际上这个策略&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;并不是非常好，因为对于很多状态执行该策略
后趋向于走到&lt;span class="math"&gt;\(-1\)&lt;/span&gt;位置而不是&lt;span class="math"&gt;\(+1\)&lt;/span&gt;位置。中下面两行执行的动作
使得机器人偏向走到&lt;span class="math"&gt;\(-1\)&lt;/span&gt;位置，所以他们的总回报的期望值是负数，对于最上面一行偏向
于走到&lt;span class="math"&gt;\(+1\)&lt;/span&gt;位置，所以总回报都是正的。所以对于下两行的位置这个策略非常差，但是&amp;nbsp;对于最上面那行这个策略就显得不错。&lt;/p&gt;
&lt;p&gt;&lt;img alt="其中一个pi" src="https://wugh.github.io/images/ML/one-pi.png" /&gt;
&lt;img alt="pi对应的V" src="https://wugh.github.io/images/ML/v-pi.png" /&gt;&lt;/p&gt;
&lt;p&gt;下面要对&lt;span class="math"&gt;\(V^\pi\)&lt;/span&gt;做一个推导使得&lt;span class="math"&gt;\(V\pi\)&lt;/span&gt;更容易计算，这里假设当前状态&lt;span class="math"&gt;\(s\)&lt;/span&gt;会转移到状态
&lt;span class="math"&gt;\(s'\)&lt;/span&gt;。中的&lt;span class="math"&gt;\(P_{s\pi(s)}(s')\)&lt;/span&gt;描述的是&lt;span class="math"&gt;\(s\)&lt;/span&gt;状态下采取一个动作
（这个动作由&lt;span class="math"&gt;\(\pi(s)\)&lt;/span&gt;来确定）转移到&lt;span class="math"&gt;\(s'\)&lt;/span&gt;状态的概率分布，因此式子中的求和描述的就
是一个求期望的过程，总回报值的期望是当前回报加上未来回报值的期望，
也称作贝尔曼方程（Bellman’s&amp;nbsp;Equations）。&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{align}
  V^\pi(s) &amp;amp;= E\Big[R(s_0) + \gamma \Big(R(s_1) + \gamma R(s_2) + \dots\Big)\Big|\pi,s_0=s\Big] \cr
  &amp;amp;= R(s) + \gamma \sum_{s'}{P_{s\pi(s)}(s')V^\pi(s')}
\label{eq:bellman}
\end{align}$$&lt;/div&gt;
&lt;p&gt;对于的例子，如果针对每个状态都写出方程，
那么就可以得到&lt;span class="math"&gt;\(11\)&lt;/span&gt;个线性方程组，并且有&lt;span class="math"&gt;\(11\)&lt;/span&gt;个未知数（每个状态都有一个&lt;span class="math"&gt;\(V^pi(s)\)&lt;/span&gt;），
可以通过求解这个方程组得到&lt;span class="math"&gt;\(V^\pi\)&lt;/span&gt;。按照的策略，我们可以计算
&lt;span class="math"&gt;\([3,1]\)&lt;/span&gt;位置的&lt;span class="math"&gt;\(V^\pi\)&lt;/span&gt;： &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{split}
  V^\pi([3,1]) = &amp;amp;R([3,1]) + \cr
  &amp;amp; \gamma[0.8V\pi([3,2]) + 0.1V\pi([4,1]) + 0.1V\pi([2,1])]
\end{split}$$&lt;/div&gt;
&lt;h4&gt;&lt;span class="math"&gt;\(V^*\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\pi^*\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;最优值函数&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;定义如下：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;是最优值函数，值得是找到一个&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;使得对于所有的状态&lt;span class="math"&gt;\(V^\pi(s)\)&lt;/span&gt;最大。&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
V^*(s) = \max_x V^\pi(s)
    \label{eq:vstar}
\end{equation}&lt;/div&gt;
&lt;p&gt;集合和可以得到&lt;span class="math"&gt;\(V^*\)&lt;/span&gt;的贝尔曼方程：&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
V^*(s) = R(s) + \max_a \gamma \sum_{s'}{P_{sa}(s')V^*(s')} 
  \label{eq:vstarbellman}
\end{equation}&lt;/div&gt;
&lt;p&gt;根据最优值函数的贝尔曼方程，把中的常数项&lt;span class="math"&gt;\(R(s)\)&lt;/span&gt;和
常数系数&lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;去掉（处于状态&lt;span class="math"&gt;\(s\)&lt;/span&gt;时，对于所有的&lt;span class="math"&gt;\(a\)&lt;/span&gt;这两个系数都相等），
就可以得到最优策略&lt;span class="math"&gt;\(\pi^*(s)\)&lt;/span&gt;的求解公式：&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\DeclareMathOperator*{\argmax}{arg\,max}
\pi^*(s) = \argmax_a \sum_{s'}{P_{sa}(s')V^*(s')} 
  \label{eq:bestpi}
\end{equation}&lt;/div&gt;
&lt;p&gt;由可以看出&lt;span class="math"&gt;\(\pi^*(s)\)&lt;/span&gt;其实依赖于&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;，所以现在的主要目标是要想办法求解
&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;。根据的定义，最直接的方法就是穷举所有的&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;，但是穷举的情况会非常多
，例如有&lt;span class="math"&gt;\(11\)&lt;/span&gt;个状态，&lt;span class="math"&gt;\(4\)&lt;/span&gt;个动作那么就有&lt;span class="math"&gt;\(4^{11}\)&lt;/span&gt;种组合，搜索空间呈指数增长，不大
合理，下面将介绍值迭代（Value Iteration）和策略迭代（Policy Iteration）方法来
求解&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;算法所描述的是值迭代的过程，初始化时对于所有的&lt;span class="math"&gt;\(s\)&lt;/span&gt;对应的
&lt;span class="math"&gt;\(V(s)\)&lt;/span&gt;为&lt;span class="math"&gt;\(0\)&lt;/span&gt;，接着对于每个&lt;span class="math"&gt;\(V(s)\)&lt;/span&gt;，这里的&lt;span class="math"&gt;\(V(s)\)&lt;/span&gt;有两种更新方式。&lt;/p&gt;
&lt;p&gt;&lt;img alt="值迭代" src="https://wugh.github.io/images/ML/mdp-vi.png" /&gt;&lt;/p&gt;
&lt;p&gt;第一种是对于所有的状态计算出式子右边的部分，然后同时更新所有的&lt;span class="math"&gt;\(V(s)\)&lt;/span&gt;,这种称作
同步更新（Synchronous Update）；另一种叫做异步更新（Asynchronous
Update），假设我们按照固定的状态顺序更新&lt;span class="math"&gt;\(V(s)\)&lt;/span&gt;，那么首先会更新第1个状态
的&lt;span class="math"&gt;\(V(s)\)&lt;/span&gt;，接着是第2个状态的&lt;span class="math"&gt;\(V(s)\)&lt;/span&gt;、第3个状态的&lt;span class="math"&gt;\(V(s)\)&lt;/span&gt;、第4个状态的&lt;span class="math"&gt;\(V(s)\)&lt;/span&gt;
，如果在更新第5个状态的&lt;span class="math"&gt;\(V(s)\)&lt;/span&gt;用到的&lt;span class="math"&gt;\(V(s')\)&lt;/span&gt;恰好是第1、2、3、4状态的，
那么我们使用的&lt;span class="math"&gt;\(V(s')\)&lt;/span&gt;是前面几次迭代更新的版本。两种方法中异步更新会
收敛地稍微快一点，值迭代会使得&lt;span class="math"&gt;\(V(s)\)&lt;/span&gt;不断地向&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;接近，如
是最后求解出来的&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="左图是\(V^*\)且\gamma=.99，右图是对应的\(\pi^*\)" src="https://wugh.github.io/images/ML/bestv-and-pi.png" /&gt;&lt;/p&gt;
&lt;p&gt;求解出&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;之后，根据就可以计算&lt;span class="math"&gt;\(\pi^*(s)\)&lt;/span&gt;，
下面举一个例子计算&lt;span class="math"&gt;\(\pi([3,1])\)&lt;/span&gt;的最优策略，结合，可以
计算出采取各个动作的未来总回报的期望，假设机器人碰到墙壁之后会回到
原来的位置，所以机器人向&lt;span class="math"&gt;\(E\)&lt;/span&gt;走的时候有&lt;span class="math"&gt;\(0.1\)&lt;/span&gt;的可能性会碰到墙壁然后又
返回到&lt;span class="math"&gt;\([3,1]\)&lt;/span&gt;位置。 &lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    E: &amp;amp; \sum_{s'}{P_{sa}(s')V^*(s')} = 0.8*0.75 + 0.1*0.69 + 0.1*0.71 = 0.74\cr
    N: &amp;amp; \sum_{s'}{P_{sa}(s')V^*(s')} = 0.8*0.69 + 0.1*0.75 + 0.1*0.49 = 0.676\cr
    W: &amp;amp; \sum_{s'}{P_{sa}(s')V^*(s')} = 0.8*0.49 + 0.1*0.69 + 0.1*0.71 = 0.532\cr
    S: &amp;amp; \sum_{s'}{P_{sa}(s')V^*(s')} = 0.8*0.71 + 0.1*0.75 + 0.1*0.49 = 0.692
\end{aligned}&lt;/div&gt;
&lt;p&gt;对比&lt;span class="math"&gt;\(4\)&lt;/span&gt;个方向的未来总回报的期望值之后，发现采取&lt;span class="math"&gt;\(E\)&lt;/span&gt;动作之后得到的值最大，
所以在&lt;span class="math"&gt;\([3,1]\)&lt;/span&gt;位置会采取动作&lt;span class="math"&gt;\(E\)&lt;/span&gt;。对每个状态都计算最优动作之后就可以得到如&amp;nbsp;所示的结果。&lt;/p&gt;
&lt;p&gt;描述完值迭代之后，下面简单描述一下策略迭代求解&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;，策略迭代会使得最后
&lt;span class="math"&gt;\(V(s)\)&lt;/span&gt;趋近于&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;并且&lt;span class="math"&gt;\(\pi(s)\)&lt;/span&gt;趋近于&lt;span class="math"&gt;\(\pi^*(s)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="策略迭代" src="https://wugh.github.io/images/ML/mdp-pi.png" /&gt;&lt;/p&gt;
&lt;p&gt;当状态数量少的时候可以采用策略迭代，因为这时候求解贝尔曼方程比较快速，但是当
状态数非常多，例如有100万个状态，那么求解贝尔曼方程的代价可能会太大，就应该&amp;nbsp;考虑使用值迭代。&lt;/p&gt;
&lt;p&gt;这里还需要讨论一下如何求解&lt;span class="math"&gt;\(P_{sa}\)&lt;/span&gt;，一般来说可以用最大似然估计来估计。&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\begin{split}
    P_{sa}(s') &amp;amp;= \frac{\text{在}s\text{状态下采取动作a到达状态}s'\text{的次数}}
    {\text{在}s\text{状态下采取动作}a\text{的次数}} \cr
    &amp;amp;\Big(\text{如果得到}\frac{0}{0}\text{的情况就用}\frac{1}{|s|}\text{替换}\Big)
  \end{split}
  \label{eq:psa}
\end{equation}&lt;/div&gt;
&lt;h4&gt;求解过程总结&lt;/h4&gt;
&lt;p&gt;把上文提到的求解&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;、&lt;span class="math"&gt;\(\pi^*(s)\)&lt;/span&gt;和&lt;span class="math"&gt;\(P_{sa}\)&lt;/span&gt;的方法结合起来就可以构成一个完整
的求解&lt;span class="caps"&gt;MDP&lt;/span&gt;的方法。首先采取策略&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;之后可以观测到一些状态转移的数据，用这些
数据来重新估计&lt;span class="math"&gt;\(P_{sa}\)&lt;/span&gt;，接着用值迭代的方式来求解当前&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;和&lt;span class="math"&gt;\(P_{sa}\)&lt;/span&gt;前提
下的&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;（值迭代的初始&lt;span class="math"&gt;\(V(s)\)&lt;/span&gt;可以使用上一轮迭代的&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;），
最后再利用这个&lt;span class="math"&gt;\(V^*(s)\)&lt;/span&gt;来更新&lt;span class="math"&gt;\(\pi\)&lt;/span&gt;。
&lt;img alt="整个过程" src="https://wugh.github.io/images/ML/put-together.png" /&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="MDP"></category></entry><entry><title>文本分类</title><link href="https://wugh.github.io/posts/2013/11/text-classification/" rel="alternate"></link><published>2013-11-25T09:08:37+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2013-11-25:posts/2013/11/text-classification/</id><summary type="html">&lt;h3&gt;1&amp;nbsp;概述&lt;/h3&gt;
&lt;p&gt;最近做了一个简单的文本分类程序，在这篇博客里面记录具体的分类流程。一般来说包括&amp;nbsp;以下几个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;语料获取&lt;/li&gt;
&lt;li&gt;文本预处理&lt;/li&gt;
&lt;li&gt;特征值提取&lt;/li&gt;
&lt;li&gt;训练分类器&lt;/li&gt;
&lt;li&gt;评估分类性能&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下文将分析上面每个步骤的具体实现过程。&lt;/p&gt;
&lt;h3&gt;2&amp;nbsp;语料获取&lt;/h3&gt;
&lt;p&gt;语料获取可以采用爬虫来获得，在实现的过程中从网易上获取了十个类别的新闻语料，分
别是：时政、军事、教育、娱乐、房产、女人、财经、体育、科技和旅游。关于爬虫怎么
写可以参考&lt;a href="http://blog.pluskid.org/?p=366" title="Scrapy 轻松
定制网络爬虫"&gt;Scrapy 轻松定制网络爬虫&lt;/a&gt;，这篇文章对于学习&lt;a href="http://scrapy.org" title="scrapy"&gt;Scrapy&lt;/a&gt;框架非常有
用，但是文章较老，很多接口可能很多都被丢弃，新的接口参考&lt;a href="http://doc.scrapy.org/en/latest/intro/tutorial.html"&gt;Scrapy
Tutorial&lt;/a&gt;。如果实在懒得爬
语料可以用我获取好的数据，对数据做一个简单说明，总共有测试语料10万篇，每种新闻
各1万篇；测试语料1万篇，每种新闻1千篇，点击&lt;a href="https://drive.google.com/file/d/0BywGHFgHsfRbSGVSbDhGLXVrUzg/edit?usp=sharing" title="新闻语料"&gt;这里
&lt;/a&gt;下载，语料示例如下，&lt;code&gt;URL::&lt;/code&gt;和&lt;code&gt;END&lt;/code&gt;之间是一篇文章：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;URL::http://politics.people.com.cn/n/2012/1120/c14562-19635416.html 2012-11-20
贵州毕节5名男孩取暖致死 副区长等8人被免职
“中国网事”记者获悉，毕节市委19日晚研究决定，对5名男孩意外死亡事件负有领导和管理责任的七星关区分管民政工作的副区长唐兴全、分管教育工作的副区长高守军等8人分别进行停职或免职处理。(记者王丽)
END::
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;3&amp;nbsp;语料预处理&lt;/h3&gt;
&lt;p&gt;得到语料之后需要对语料做预处理，预处理的过程包括：分词、去停用词、去低频词等。&lt;/p&gt;
&lt;h4&gt;3.1&amp;nbsp;分词和去停用词&lt;/h4&gt;
&lt;p&gt;现在有很多可以免费使用的分词工具，包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://code.google.com/p/fudannlp/" title="FudanNLP"&gt;FudanNLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ictclas.nlpir.org/" title="nlpir"&gt;中科院分词&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ansjsun/ansj_seg" title="ansj seg"&gt;ansj分词&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fxsjy/jieba" title="jieba"&gt;jieba分词&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本文使用的是&lt;a href="http://code.google.com/p/fudannlp/" title="FudanNLP"&gt;FudanNLP&lt;/a&gt;中文自然语言处理工具包，如何分词请参考
&lt;a href="https://code.google.com/p/fudannlp/wiki/QuickStart" title="QuickStart"&gt;QuickStart&lt;/a&gt;和
&lt;a href="https://fudannlp.googlecode.com/svn/FudanNLP-1.5-API/java-docs/index.html" title="Api Doc"&gt;APIDoc&lt;/a&gt;，由于代码比较多，文章中仅仅贴出部分函数代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;doSeg&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;infile&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;outfile&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="kd"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// 切分一个大文件很消耗时间&lt;/span&gt;
        &lt;span class="c1"&gt;// 因此如果输出文件存在就抛出异常&lt;/span&gt;
        &lt;span class="c1"&gt;// 让用户确保不会产生覆盖文件&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;File&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outfile&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;exists&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt;
            &lt;span class="k"&gt;throw&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outfile&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; is existed&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;

        &lt;span class="n"&gt;LineIterator&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FileUtils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;lineIterator&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;File&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;infile&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;BufferedWriter&lt;/span&gt; &lt;span class="n"&gt;bw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

        &lt;span class="k"&gt;try&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;bw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;BufferedWriter&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;OutputStreamWriter&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;FileOutputStream&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outfile&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
            &lt;span class="n"&gt;StringBuilder&lt;/span&gt; &lt;span class="n"&gt;stringBuilder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;StringBuilder&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
            &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;hasNext&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
                &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;nextLine&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;trim&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;isEmpty&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;startsWith&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;URL::&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
                    &lt;span class="k"&gt;continue&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;startsWith&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;END::&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)){&lt;/span&gt;
                    &lt;span class="n"&gt;stringBuilder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;append&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;\n&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
                    &lt;span class="n"&gt;bw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;write&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stringBuilder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
                    &lt;span class="n"&gt;stringBuilder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;StringBuilder&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
                    &lt;span class="k"&gt;continue&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
                &lt;span class="o"&gt;}&lt;/span&gt;
                &lt;span class="c1"&gt;// seg line&lt;/span&gt;
                &lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;segTagger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;tag2Array&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
        &lt;span class="c1"&gt;// 对分词的结果去除停用词&lt;/span&gt;
                &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;baseWords&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stopWords&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;phraseDel&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
                &lt;span class="n"&gt;stringBuilder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;append&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Joiner&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;on&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;join&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;baseWords&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
                &lt;span class="n"&gt;stringBuilder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;append&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
            &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;finally&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;close&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bw&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
                &lt;span class="n"&gt;bw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;close&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
            &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;上述代码中通过&lt;code&gt;segTagger.tag2Array(line)&lt;/code&gt;分词，得到的分词结果是一个数组，再对分
词的结果去除停用词&lt;code&gt;stopWords.phraseDel(words)&lt;/code&gt;。这里的&lt;code&gt;segTagger&lt;/code&gt;和&lt;code&gt;stopWords&lt;/code&gt;
都是有&lt;a href="http://code.google.com/p/fudannlp/" title="FudanNLP"&gt;FudanNLP&lt;/a&gt;提供的类。在语料中，一篇文章的正文是&lt;code&gt;URL::&lt;/code&gt;和&lt;code&gt;END::&lt;/code&gt;
之间的部分，代码中逐行分词，然后把分词和去停用词的多行合并成一行，最后得到的一&amp;nbsp;篇文章就是一行。&lt;/p&gt;
&lt;h4&gt;3.2&amp;nbsp;去除低频词&lt;/h4&gt;
&lt;p&gt;通过分词和去除停用词之后，语料库中的词汇表规模（不同的词的数目）还是非常的大，
10万篇语料处理之后，词汇表有69万个词。这么大的词汇表对于后面我们计算特征值非常
不利，例如用最简单的&lt;code&gt;TF(term frequency)&lt;/code&gt;计算特征，要把每篇文章转换成一个词频向
量，而这个向量的长度就是我们的词汇表大小，如果这个表太大的话，会有很多词频为0，
直接导致数据稀疏问题，而且也会有很多噪声。从词汇表里面去除一些词的方法，最简单&amp;nbsp;的是采用词频过滤，即词频低于多少的词就忽略，下面描述具体做法。&lt;/p&gt;
&lt;p&gt;首先，我们要在整个训练集合上统计所有词的频率分布，这里用&lt;code&gt;JAVA&lt;/code&gt;的&lt;code&gt;HashMap&lt;/code&gt;就能实
现，以词为键，词出现的次数（词频）为值，整个语料库遍历之后，就得到每个词分别在
语料库里面出现多少次，最后对这个&lt;code&gt;HashMap&lt;/code&gt;做一个按值降序排序，越早出现的词那么词
频就越高。下图是我统计的一个累计词频分布图，累积地统计个词的出现次数占总次数的
比例，发现少部分词占有了大部分的词频。更多有意思的词频规律参考
&lt;a href="https://zh.wikipedia.org/wiki/%E9%BD%8A%E5%A4%AB%E5%AE%9A%E5%BE%8B"&gt;Zipf&amp;#8217;s law&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="zipflaw" src="https://wugh.github.io/images/NLP/zipfs-law.png" /&gt;&lt;/p&gt;
&lt;p&gt;本文最终取前3万个词频最高的词作为词汇表，这3万个词，总的出现次数已经占到总次数
的90%。当然从词汇表里面去除一些词还有更高级的做法，例如：计算每个词对整个语料库
的信息增益，取信息增益最高的多少个词作为词汇表，信息增益是什么，请参考&lt;a href="http://www.blogjava.net/zhenandaci/archive/2009/03/24/261701.html"&gt;这里
&lt;/a&gt;，具体实现
参考&lt;a href="http://www.cnblogs.com/zhangchaoyang/articles/2236475.html"&gt;这里&lt;/a&gt;；也可以
用&lt;a href="http://zh.wikipedia.org/wiki/TF-IDF" title="if idf"&gt;&lt;span class="caps"&gt;TF&lt;/span&gt;-&lt;span class="caps"&gt;IDF&lt;/span&gt;&lt;/a&gt;来度量词的重要性。&lt;/p&gt;
&lt;h3&gt;4&amp;nbsp;特征提取&lt;/h3&gt;
&lt;p&gt;特征提取的目地是把文本转换成特征向量，为后面的分类器训练做数据准备。最经典的方
法应该就是&lt;a href="http://zh.wikipedia.org/wiki/%E5%90%91%E9%87%8F%E7%A9%BA%E9%96%93%E6%A8%A1%E5%9E%8B" title="vector space model"&gt;向量空间模型
&lt;/a&gt;，更复杂的方法可以用
&lt;a href="http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" title="LDA"&gt;&lt;span class="caps"&gt;LDA&lt;/span&gt;&lt;/a&gt;等。下面分
别介绍向量空间模型和&lt;span class="caps"&gt;LDA&lt;/span&gt;。&lt;/p&gt;
&lt;h4&gt;4.1&amp;nbsp;向量空间模型&lt;/h4&gt;
&lt;p&gt;向量空间模型就是把词汇表的每个词都当作特征的一个维度，特征的取值是这个词在这篇
文章中出现的次数。现在假设词汇表有六个词，分别是：&lt;code&gt;上海 海关 建立 任务 提醒 机制&lt;/code&gt;
，下面示例如何把文章换转成一个向量，假设语料库里面有两篇文章
&lt;span class="math"&gt;\(d_1\)&lt;/span&gt;和&lt;span class="math"&gt;\(d_2\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
&amp;amp;d_1: 上海\quad{}海关\quad{}建立\quad{}任务\quad{}上海\cr
&amp;amp;d_2: 上海\quad{}建立\quad{}任务\quad{}提醒
\end{aligned} 
$$&lt;/div&gt;
&lt;p&gt;把词汇表的词依次记作: &lt;span class="math"&gt;\(w_1\ w_2\ w_3\ w_4\ w_5\ w_6\)&lt;/span&gt;，就可以统计文章中每个词出
现的次数。以&lt;span class="math"&gt;\(d_1\)&lt;/span&gt;为例子，在&lt;span class="math"&gt;\(d_1\)&lt;/span&gt;中，&lt;span class="math"&gt;\(w_1\)&lt;/span&gt;（上海）出现2次，&lt;span class="math"&gt;\(w_2\)&lt;/span&gt;出现1次，&lt;span class="math"&gt;\(w_3\)&lt;/span&gt;出
现1次，&lt;span class="math"&gt;\(w_4\)&lt;/span&gt;出现1次，&lt;span class="math"&gt;\(w_5,w_6\)&lt;/span&gt;出现0次，最终向量形式为：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
&amp;amp; v: w_1\quad{}w_2\quad{}w_3\quad{}w_4\quad{}w_5\quad{}w_6\cr
&amp;amp; d_1: 2\quad{}1\quad{}1\quad{}1\quad{}0\quad{}0\cr
&amp;amp; d_2: 1\quad{}0\quad{}1\quad{}1\quad{}1\quad{}0
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;这样就可以把每个文档都转换成一个词频向量，用这个词频向量就可以计算两个文档之间&amp;nbsp;的相似度，即求两个文档词频向量的余弦相似度，相似度越接近1代表越相似：&lt;/p&gt;
&lt;div class="math"&gt;$$
\cos{\theta} = \frac{\mathbf{d_1} \cdot \mathbf{d_2}}{\left\| \mathbf{d_1} \right\| \left \| \mathbf{d_2} \right\|}
$$&lt;/div&gt;
&lt;p&gt;当然这里的词频特征也可以用更为合理的&lt;code&gt;IF-IDF&lt;/code&gt;特征取代，具体请参考&lt;a href="http://zh.wikipedia.org/wiki/向量空間模型" title="VSM"&gt;这里
&lt;/a&gt;的计算过程。&lt;/p&gt;
&lt;h4&gt;4.2 用&lt;span class="caps"&gt;LDA&lt;/span&gt;提取特征(&lt;span class="caps"&gt;OPTIONAL&lt;/span&gt;)&lt;/h4&gt;
&lt;p&gt;其次我们可以使用&lt;span class="caps"&gt;LDA&lt;/span&gt;把每篇文档表示成主题的一个分布，然后用这个主题分布当作特征向
量来训练分类器。可以使用&lt;a href="http://jgibblda.sourceforge.net/" title="JGbibbsLDA"&gt;JGibbsLDA&lt;/a&gt;将训练语料转换成关于主题的分布。由于我们再预处理步骤已经把每篇预处
理好的文档变成一行，接下来我们就可以把训练语料转换成&lt;code&gt;JGibbsLDA&lt;/code&gt;需要的格式。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[M]
[document1]
[document2]
...
[documentM]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;[M]&lt;/code&gt;表示文档总数，没一行是一篇文档，&lt;code&gt;[documenti]&lt;/code&gt;表示语料库里面的第&lt;code&gt;i&lt;/code&gt;篇文档，
每篇文档总用有&lt;code&gt;Ni&lt;/code&gt;个词：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[documenti] = [wordi1] [wordi2] ... [wordiNi]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;用&lt;code&gt;JGibbsLDA&lt;/code&gt;训练得到的输出结果中，名为&lt;code&gt;&amp;lt;model_name&amp;gt;.theta&lt;/code&gt;的文件包含了每篇文档
的主题分布&lt;span class="math"&gt;\(p(topic_t|document_m)\)&lt;/span&gt;，一行是一个文档，每列是一个主题。&lt;/p&gt;
&lt;h3&gt;5&amp;nbsp;训练分类器&lt;/h3&gt;
&lt;p&gt;得到每个文档的特征向量之后，训练的过程可以采用各种各样的分类器，例如：朴素贝叶
斯、神经网络、&lt;span class="caps"&gt;SVM&lt;/span&gt;等。本文以&lt;span class="caps"&gt;SVM&lt;/span&gt;为例子简要描述一下分类过程，&lt;span class="caps"&gt;SVM&lt;/span&gt;可以使用
&lt;a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" title="LIBSVM"&gt;&lt;span class="caps"&gt;LIBSVM&lt;/span&gt;&lt;/a&gt;这个库。首先要把特征向量整理成&lt;a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" title="LIBSVM"&gt;&lt;span class="caps"&gt;LIBSVM&lt;/span&gt;&lt;/a&gt;的输入格式，然后&amp;nbsp;再进行训练。&lt;/p&gt;
&lt;h4&gt;5.1&amp;nbsp;特征格式转换&lt;/h4&gt;
&lt;p&gt;由于训练语料里面我们知道每篇文章的类别，因为我们是分文件存储每个类别的文档，可
以把10个类别打上标签&lt;code&gt;0~9&lt;/code&gt;，表示用0来表示1类新闻，1来表示另一种新闻，依此类推。
&lt;a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" title="LIBSVM"&gt;&lt;span class="caps"&gt;LIBSVM&lt;/span&gt;&lt;/a&gt;的输入格式如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;label&amp;gt; &amp;lt;index1&amp;gt;:&amp;lt;value1&amp;gt; &amp;lt;index2&amp;gt;:&amp;lt;value2&amp;gt; …
                      …
&amp;lt;label&amp;gt; &amp;lt;index1&amp;gt;:&amp;lt;value1&amp;gt; &amp;lt;index2&amp;gt;:&amp;lt;value2&amp;gt; …
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;假设我们上面举例的&lt;span class="math"&gt;\(d_1\)&lt;/span&gt;文档属于0这个标签，&lt;span class="math"&gt;\(d_2\)&lt;/span&gt;这个文档属于1这个标签，那么这两&amp;nbsp;篇文档可以表示成如下格式：&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
d_1: 2\quad{}1\quad{}1\quad{}1\quad{}0\quad{}0\cr
d_2: 1\quad{}0\quad{}1\quad{}1\quad{}1\quad{}0\cr
\Rightarrow\cr
d_1: 0\quad{}1:2\quad{}2:1\quad{}3:1\quad{}4:1\quad{}5:0\quad{}6:0\cr
d_2: 1\quad{}1:1\quad{}2:0\quad{}3:1\quad{}4:1\quad{}5:1\quad{}6:0\cr
\end{aligned}
$$&lt;/div&gt;
&lt;h4&gt;5.2&amp;nbsp;训练&lt;/h4&gt;
&lt;p&gt;训练过程直接使用&lt;a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" title="LIBSVM"&gt;&lt;span class="caps"&gt;LIBSVM&lt;/span&gt;&lt;/a&gt;的&lt;code&gt;C&lt;/code&gt;代码编译好的可执行文件&lt;code&gt;svm-train&lt;/code&gt;，训练之
前可以用&lt;code&gt;svm-scale&lt;/code&gt;对数据进行归一化，假设整理好的训练语料为&lt;code&gt;train.txt&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 归一化训练语料&lt;/span&gt;
&lt;span class="c1"&gt;# 把特征取值范围保存到train.range文件，方便对测试语料归一化&lt;/span&gt;
./svm-scale -s train.range train.txt &amp;gt; train.scale
&lt;span class="c1"&gt;# 训练svm模型&lt;/span&gt;
&lt;span class="c1"&gt;# type: C-SVC&lt;/span&gt;
&lt;span class="c1"&gt;# kernel type: radial basis function: exp(-gamma*|u-v|^2)&lt;/span&gt;
&lt;span class="c1"&gt;# gamma: 0.5&lt;/span&gt;
&lt;span class="c1"&gt;# C of C-SVC: 4&lt;/span&gt;
&lt;span class="c1"&gt;# 训练语料: train.scale&lt;/span&gt;
&lt;span class="c1"&gt;# 保存模型: train.scale.model&lt;/span&gt;
./svm-train -s &lt;span class="m"&gt;1&lt;/span&gt; -t &lt;span class="m"&gt;2&lt;/span&gt; -g 0.5 -c &lt;span class="m"&gt;4&lt;/span&gt; train.scale train.scale.model
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;6&amp;nbsp;测试分类器性能&lt;/h3&gt;
&lt;p&gt;测试的主要流程如下图，首先也要讲过训练过程的预处理和特征提取，再将特征向量转换
成&lt;a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" title="LIBSVM"&gt;&lt;span class="caps"&gt;LIBSVM&lt;/span&gt;&lt;/a&gt;需要的格式，最后用编译好的&lt;code&gt;svm-predict&lt;/code&gt;可执行文件进行分类，最
后取得的准确率为&lt;code&gt;0.8592%&lt;/code&gt;。
&lt;img alt="chartflow" src="https://wugh.github.io/images/NLP/text-classify-flow.png" /&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="文本分类"></category><category term="SVM"></category></entry><entry><title>hexo数学公式</title><link href="https://wugh.github.io/posts/2013/11/hexo-math-equation/" rel="alternate"></link><published>2013-11-12T12:20:30+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2013-11-12:posts/2013/11/hexo-math-equation/</id><summary type="html">&lt;p&gt;在使用&lt;a href="https://github.com/tommy351/hexo" title="Hexo"&gt;Hexo&lt;/a&gt;写博客的时候无法插入数学公
式，通过调研发现&lt;a href="http://www.mathjax.org/" title="MathJax"&gt;MathJax&lt;/a&gt;可以很好的在网页里&amp;nbsp;面显示数学公式，并且能够支持很多浏览器，下面分步骤描述生成公式的方法。&lt;/p&gt;
&lt;h3&gt;修改主题&lt;/h3&gt;
&lt;p&gt;MathJax的&lt;a href="http://docs.mathjax.org/en/latest/platforms/index.html"&gt;官方文档&lt;/a&gt;说只
要在每个页面的&lt;code&gt;head&lt;/code&gt;标签里面插入下面的&lt;code&gt;javascript&lt;/code&gt;代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt; &lt;span class="na"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;text/javascript&amp;quot;&lt;/span&gt;
   &lt;span class="na"&gt;src&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;接下来的步骤就是要把这个代码加入所有的文章页面里面，这里参考
&lt;a href="https://github.com/nuklly/hexo-theme-greyshade" title="hexo theme
greyshade"&gt;greyshade&lt;/a&gt;中对这段代码的处理方式，在主题目录&lt;code&gt;themes/light/layout/_partial/&lt;/code&gt;下
新建一个文件&lt;code&gt;mathjax.ejs&lt;/code&gt;，内容如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;&amp;lt;!-- mathjax config similar to math.stackexchange --&amp;gt;&lt;/span&gt;

&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt; &lt;span class="na"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;text/x-mathjax-config&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nx"&gt;MathJax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Hub&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Config&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="nx"&gt;tex2jax&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;inlineMath&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;$&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;$&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;\\(&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;\\)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
      &lt;span class="nx"&gt;processEscapes&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;});&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;

&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt; &lt;span class="na"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;text/x-mathjax-config&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nx"&gt;MathJax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Hub&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Config&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
      &lt;span class="nx"&gt;tex2jax&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nx"&gt;skipTags&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;script&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;noscript&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;style&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;textarea&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;pre&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;code&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;

&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt; &lt;span class="na"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;text/x-mathjax-config&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nx"&gt;MathJax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Hub&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Queue&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;all&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;MathJax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Hub&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;getAllJax&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;all&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;length&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nx"&gt;all&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="nx"&gt;SourceElement&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;parentNode&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;className&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; has-jax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;

&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt; &lt;span class="na"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;text/javascript&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;src&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;然后在&lt;code&gt;themes/light/layout/_partial/&lt;/code&gt;目录下的&lt;code&gt;head.ejs&lt;/code&gt;里面添加以下几行：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;&amp;lt;%&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theme&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mathjax&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="cp"&gt;%&amp;gt;&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="x"&gt;    &lt;/span&gt;&lt;span class="cp"&gt;&amp;lt;%-&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mathjax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="cp"&gt;%&amp;gt;&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="cp"&gt;&amp;lt;%&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="cp"&gt;%&amp;gt;&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这几行的意思是如果在主题的&lt;code&gt;_config.xml&lt;/code&gt;里面把&lt;code&gt;mathjax&lt;/code&gt;变量设置成&lt;code&gt;true&lt;/code&gt;那么就把
&lt;code&gt;mathjax.ejs&lt;/code&gt;包含进来，所以还需要在&lt;code&gt;themes/_config.xml&lt;/code&gt;里面添加：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;mathjax&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;配置完之后生成的网页里面都包含有&lt;code&gt;mathjax&lt;/code&gt;需要的一段&lt;code&gt;script&lt;/code&gt;。&lt;/p&gt;
&lt;h3&gt;生成公式&lt;/h3&gt;
&lt;p&gt;有些&lt;code&gt;MarkDown&lt;/code&gt;渲染器本来支持，例如
&lt;a href="https://github.com/gettalong/kramdown" title="kramdown"&gt;kramdown&lt;/a&gt;。但是这个是&lt;code&gt;Ruby&lt;/code&gt;
写的，&lt;code&gt;Hexo&lt;/code&gt;无法支持，&lt;code&gt;Hexo&lt;/code&gt;官方的&lt;code&gt;MarkDown&lt;/code&gt;渲染器也无法对公式进行处理，现在
还可以采用&lt;a href="https://github.com/wzpan/hexo-renderer-pandoc"&gt;hexo pandoc render&lt;/a&gt;&amp;nbsp;来进行渲染&lt;/p&gt;
&lt;h4&gt;行间公式&lt;/h4&gt;
&lt;p&gt;要产生行间公式就得用下面的代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="s"&gt;$&lt;/span&gt;&lt;span class="nb"&gt;E&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;mc^&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="s"&gt;$&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;现在举一个行间公式的例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;爱因斯坦提出了质能方程：&lt;span class="s"&gt;$&lt;/span&gt;&lt;span class="nb"&gt;E&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;mc^&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="s"&gt;$&lt;/span&gt;。他是一个伟大的物理学家。
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;爱因斯坦提出了质能方程：&lt;span class="math"&gt;\(E=mc^2\)&lt;/span&gt;。他是一个伟大的&amp;nbsp;物理学家。&lt;/p&gt;
&lt;h4&gt;方程组&lt;/h4&gt;
&lt;p&gt;我们可以用下面的一段代码来产生一个独立的方程组，由于一般&lt;code&gt;Markdown&lt;/code&gt;中&lt;code&gt;\\&lt;/code&gt;需要转义
，所以换行我们可以使用&lt;code&gt;\cr&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;以下是几个三角恒等式：
&lt;span class="sb"&gt;$$&lt;/span&gt;&lt;span class="nb"&gt;&lt;/span&gt;
&lt;span class="nv"&gt;\begin&lt;/span&gt;&lt;span class="nb"&gt;{align}&lt;/span&gt;
&lt;span class="nv"&gt;\sin&lt;/span&gt;&lt;span class="nb"&gt; &lt;/span&gt;&lt;span class="nv"&gt;\left&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;x&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;y&lt;/span&gt;&lt;span class="nv"&gt;\right&lt;/span&gt;&lt;span class="o"&gt;)=&lt;/span&gt;&lt;span class="nv"&gt;\sin&lt;/span&gt;&lt;span class="nb"&gt; x &lt;/span&gt;&lt;span class="nv"&gt;\cos&lt;/span&gt;&lt;span class="nb"&gt; y &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt; &lt;/span&gt;&lt;span class="nv"&gt;\cos&lt;/span&gt;&lt;span class="nb"&gt; x &lt;/span&gt;&lt;span class="nv"&gt;\sin&lt;/span&gt;&lt;span class="nb"&gt; y&lt;/span&gt;&lt;span class="nv"&gt;\cr&lt;/span&gt;&lt;span class="nb"&gt;&lt;/span&gt;
&lt;span class="nv"&gt;\cos&lt;/span&gt;&lt;span class="nb"&gt; &lt;/span&gt;&lt;span class="nv"&gt;\left&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;x&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;y&lt;/span&gt;&lt;span class="nv"&gt;\right&lt;/span&gt;&lt;span class="o"&gt;)=&lt;/span&gt;&lt;span class="nv"&gt;\cos&lt;/span&gt;&lt;span class="nb"&gt; x &lt;/span&gt;&lt;span class="nv"&gt;\cos&lt;/span&gt;&lt;span class="nb"&gt; y &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt; &lt;/span&gt;&lt;span class="nv"&gt;\sin&lt;/span&gt;&lt;span class="nb"&gt; x &lt;/span&gt;&lt;span class="nv"&gt;\sin&lt;/span&gt;&lt;span class="nb"&gt; y&lt;/span&gt;
&lt;span class="nv"&gt;\end&lt;/span&gt;&lt;span class="nb"&gt;{align}&lt;/span&gt;
&lt;span class="s"&gt;$$&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;以下是几个三角恒等式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
\sin \left(x+y\right)=\sin x \cos y + \cos x \sin y\cr
\cos \left(x+y\right)=\cos x \cos y - \sin x \sin y
\end{align}
$$&lt;/div&gt;
&lt;h3&gt;vim插入模板&lt;/h3&gt;
&lt;p&gt;上述方法插入公式的时候总是要时间去敲很多没有意义的&lt;code&gt;html&lt;/code&gt;标签，我们可以利用&lt;code&gt;vim&lt;/code&gt;
的&lt;a href="https://github.com/garbas/vim-snipmate" title="snipmate"&gt;vim-snipmate&lt;/a&gt;插件来做几个
模板加速我们的程序编写。
我写了两个&lt;code&gt;snippets&lt;/code&gt;来插入公式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;snippet $
    $${1:Inline}$${0}

snippet $$
    $$  &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Displayed&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;
    $$  
    &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;把这个&lt;code&gt;mkd.snippets&lt;/code&gt;文件放到&lt;code&gt;~/.vim/snippets/&lt;/code&gt;目录下面就可以生效，然后我们在编
辑&lt;code&gt;MarkDown&lt;/code&gt;文件的时候就可以通过&lt;code&gt;$&amp;lt;Tab&amp;gt;&lt;/code&gt;来触发一个行间公式模板，通过&lt;code&gt;$$&amp;lt;Tab&amp;gt;&lt;/code&gt;来&amp;nbsp;生成方程组模板。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="latex"></category><category term="vim"></category><category term="hexo"></category></entry><entry><title>BCM57780没有网络连接</title><link href="https://wugh.github.io/posts/2013/11/bcm57780-network-problem/" rel="alternate"></link><published>2013-11-10T00:16:36+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2013-11-10:posts/2013/11/bcm57780-network-problem/</id><summary type="html">&lt;h3&gt;问题描述&lt;/h3&gt;
&lt;p&gt;安装完Gentoo之后发现系统一直没有办法上网，通过&lt;code&gt;lspci&lt;/code&gt;找到自己的网卡之后，搜索问&amp;nbsp;题，发现通过下面几条命令之后才能够激活网卡：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# rmmod broadcom &lt;/span&gt;
&lt;span class="c1"&gt;# rmmod tg3 &lt;/span&gt;
&lt;span class="c1"&gt;# modprobe broadcom &lt;/span&gt;
&lt;span class="c1"&gt;# modprobe tg3 &lt;/span&gt;
&lt;span class="c1"&gt;# dhcpcd eth0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;问题解决&lt;/h3&gt;
&lt;p&gt;但是并不能永久解决问题，由上面看来问题应该出在内核那边，需要安装下面的方法重新&amp;nbsp;编译一下模块：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Device Drivers ---&amp;gt; 
     Network Device Support ---&amp;gt; 
          Ethernet driver support ---&amp;gt; 
               Broadcom devices 
                 &amp;lt;M&amp;gt; Broadcom Tigon3 support 
          PHY Device support and infrastrcutre ---&amp;gt; 
             &amp;lt;M&amp;gt; Drivers &lt;span class="k"&gt;for&lt;/span&gt; Broadcom PHYs
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;参考文章&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://forums.gentoo.org/viewtopic-t-925416-start-0.html"&gt;No Connection with &lt;span class="caps"&gt;BCM57780&lt;/span&gt; After&amp;nbsp;Installing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary><category term="内核编译"></category><category term="Gentoo"></category></entry><entry><title>Gentoo Portage使用技巧</title><link href="https://wugh.github.io/posts/2013/11/gentoo-portage-tips/" rel="alternate"></link><published>2013-11-09T10:06:01+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2013-11-09:posts/2013/11/gentoo-portage-tips/</id><summary type="html">&lt;p&gt;本文章记录一些使用Gentoo&amp;nbsp;Portage的技巧。&lt;/p&gt;
&lt;h3&gt;指定Slot安装软件&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Slot&lt;/code&gt;是在&lt;code&gt;Gentoo&lt;/code&gt;的包管理的一个重要功能，当一个软件有多个分支的时候Portage能够&amp;nbsp;保证不同的版本并存。例如，Python一般有多个版本&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ eix dev-lang/python
&lt;span class="o"&gt;[&lt;/span&gt;I&lt;span class="o"&gt;]&lt;/span&gt; dev-lang/python
     Available versions:  
        &lt;span class="o"&gt;(&lt;/span&gt;2.5&lt;span class="o"&gt;)&lt;/span&gt;   2.5.4-r4 ~2.5.4-r5
        &lt;span class="o"&gt;(&lt;/span&gt;2.6&lt;span class="o"&gt;)&lt;/span&gt;   2.6.8 ~2.6.8-r1
        &lt;span class="o"&gt;(&lt;/span&gt;2.7&lt;span class="o"&gt;)&lt;/span&gt;   2.7.3-r2 ~2.7.3-r3
        &lt;span class="o"&gt;(&lt;/span&gt;3.1&lt;span class="o"&gt;)&lt;/span&gt;   3.1.5 ~3.1.5-r1
        &lt;span class="o"&gt;(&lt;/span&gt;3.2&lt;span class="o"&gt;)&lt;/span&gt;   3.2.3 ~3.2.3-r1 ~3.2.3-r2
        &lt;span class="o"&gt;(&lt;/span&gt;3.3&lt;span class="o"&gt;)&lt;/span&gt;   **3.3.0 **3.3.0-r1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;上面的搜索出来的记过中括号中的就是slot号码，那么如何根据slot安装不同的软件呢，&amp;nbsp;下面将会解释这一点。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ emerge -pv dev-lang/python:3.3
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;通过上面的方式就可以安装Python3.3。&lt;/p&gt;
&lt;h3&gt;emerge特定版本的软件&lt;/h3&gt;
&lt;p&gt;使用&lt;code&gt;emerge&lt;/code&gt;搜索&lt;code&gt;gitolite&lt;/code&gt;，出现的结果很多，如果我们不想安装默认的版本那么就得&amp;nbsp;用特定的命令。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tom@nextzone ~ $ eix dev-vcs/gitolite
* dev-vcs/gitolite
     Available versions:  2.3.1 ~3.1 ~3.2 ~3.3 ~3.4 ~3.5 ~3.5.1 ~3.5.2 &lt;span class="o"&gt;{&lt;/span&gt;contrib tools vim-syntax&lt;span class="o"&gt;}&lt;/span&gt;
     Homepage:            http://github.com/sitaramc/gitolite
     Description:         Highly flexible server &lt;span class="k"&gt;for&lt;/span&gt; git directory version tracker
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;安装&lt;code&gt;3.5.2&lt;/code&gt;的方法如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;emerge --ask &lt;span class="o"&gt;=&lt;/span&gt;dev-vcs/gitolite-3.5.2
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;emerge的正则搜索&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;emerge --search "package name"&lt;/code&gt;是一个正常使用方法，如果待搜索的名字是以&lt;code&gt;%&lt;/code&gt;开头
那么代表这个是一个正则表达式。例如，&lt;code&gt;emerge --search "%^kde"&lt;/code&gt;会搜索出全部以
&lt;code&gt;kde&lt;/code&gt;开头的包，但是要注意的是，这样子搜索的时候不匹配软件包所在的类别，如果要匹
配类别，那么就需要加上&lt;code&gt;@&lt;/code&gt;：就可以搜索&lt;code&gt;dev-java&lt;/code&gt;这个类别下含有&lt;code&gt;jdk&lt;/code&gt;关键词的软件&amp;nbsp;包。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;guohuawu@cist-tux ~ $ emerge --search &lt;span class="s2"&gt;&amp;quot;%@^dev-java.*jdk&amp;quot;&lt;/span&gt;
Searching...    
&lt;span class="o"&gt;[&lt;/span&gt; Results &lt;span class="k"&gt;for&lt;/span&gt; search key : ^dev-java.*jdk &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt; Applications found : &lt;span class="m"&gt;11&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;

*  dev-java/apple-jdk-bin &lt;span class="o"&gt;[&lt;/span&gt; Masked &lt;span class="o"&gt;]&lt;/span&gt;
      Latest version available: 1.6.0
      Latest version installed: &lt;span class="o"&gt;[&lt;/span&gt; Not Installed &lt;span class="o"&gt;]&lt;/span&gt;
      Size of files: &lt;span class="m"&gt;0&lt;/span&gt; kB
      Homepage:      http://java.sun.com/j2se/1.6.0/
      Description:   Links to Apple&lt;span class="s1"&gt;&amp;#39;s version of Sun&amp;#39;&lt;/span&gt;s J2SE Development Kit
      License:       public-domain

*  dev-java/db4o-jdk11
      Latest version available: 7.4
      Latest version installed: &lt;span class="o"&gt;[&lt;/span&gt; Not Installed &lt;span class="o"&gt;]&lt;/span&gt;
      Size of files: &lt;span class="m"&gt;312&lt;/span&gt; kB
      Homepage:      http://www.db4o.com
      Description:   Core files &lt;span class="k"&gt;for&lt;/span&gt; the object database &lt;span class="k"&gt;for&lt;/span&gt; java
      License:       GPL-2
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;eix&lt;/h3&gt;
&lt;p&gt;上面提到用&lt;code&gt;emerge&lt;/code&gt;来搜索软件包，但是这个方法遇到的一个最大问题就是搜索非常慢，
用&lt;code&gt;eix&lt;/code&gt;可以解决这个问题。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root &lt;span class="c1"&gt;# emerge --ask eix&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这样以来同步Portage就变得非常简单：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root &lt;span class="c1"&gt;# eix-sync&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;通用&lt;code&gt;eix&lt;/code&gt;也支持正则搜索，而且搜索速度非常快&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;guohuawu@cist-dell ~ $ eix ^ack$
&lt;span class="o"&gt;[&lt;/span&gt;I&lt;span class="o"&gt;]&lt;/span&gt; sys-apps/ack
     Available versions:  1.96 2.12 &lt;span class="o"&gt;{&lt;/span&gt;test&lt;span class="o"&gt;}&lt;/span&gt;
     Installed versions:  2.12&lt;span class="o"&gt;(&lt;/span&gt;06:48:04 PM 05/24/2014&lt;span class="o"&gt;)(&lt;/span&gt;-test&lt;span class="o"&gt;)&lt;/span&gt;
     Homepage:            http://betterthangrep.com/ http://search.cpan.org/dist/ack/
     Description:         ack is a tool like grep, aimed at programmers with large trees of heterogeneous &lt;span class="nb"&gt;source&lt;/span&gt; code
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;下面举出一些常用的例子&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;eix kernel&lt;/code&gt;：搜索含有kernel关键词的包&lt;/li&gt;
&lt;li&gt;&lt;code&gt;eix -I kernel&lt;/code&gt;：从已经安装的包里面搜索&lt;/li&gt;
&lt;li&gt;&lt;code&gt;eix -S -c corba&lt;/code&gt;：搜索包的描述，并且用紧凑模式输出&lt;/li&gt;
&lt;li&gt;&lt;code&gt;eix -C -c app-officeext&lt;/code&gt;：搜索一个类别的包，，并且用紧凑模式输出&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更多的请参考&lt;a href="http://wiki.gentoo.org/wiki/Eix"&gt;Gentoo Wiki&lt;/a&gt;。&lt;/p&gt;</summary><category term="Portage"></category><category term="Gentoo"></category></entry><entry><title>Linux字体配置</title><link href="https://wugh.github.io/posts/2013/11/linux-fontconfig/" rel="alternate"></link><published>2013-11-09T09:54:23+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2013-11-09:posts/2013/11/linux-fontconfig/</id><summary type="html">&lt;h3&gt;fontconfig简介&lt;/h3&gt;
&lt;p&gt;Linux字体配置采用
&lt;a href="http://www.freedesktop.org/wiki/Software/fontconfig/"&gt;fontconfig&lt;/a&gt;来做字体渲染
，中文的&lt;a href="http://codex.wordpress.org.cn/Fontconfig%E7%94%A8%E6%88%B7%E6%89%8B%E5%86%8C"&gt;fontconfig手册
&lt;/a&gt;&amp;nbsp;参考这里，根据这个手册对Linux的字体进行简单的配置。&lt;/p&gt;
&lt;h3&gt;配置文件&lt;/h3&gt;
&lt;p&gt;根据自己的日常使用习惯，下面对我的fontconfig配置文件做一个描述。&lt;/p&gt;
&lt;h4&gt;dpi设定&lt;/h4&gt;
&lt;p&gt;dpi的详细设定参考这两篇文章，&lt;a href="http://xxb.is-programmer.com/posts/4260.html"&gt;dpi设置及sub-pixel次像素微调
&lt;/a&gt;和&lt;a href="http://www.linuxsir.org/bbs/thread266659.html"&gt;Linux 字体微调 - windows 效果
版&lt;/a&gt;两篇文章，Windows 7的默认dpi&amp;nbsp;是96，我的字体设定的dpi也采用96。参考前面两篇文章可以知道dpi的计算公式如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;屏幕 width = 287 mm = 28.7 cm，height = 215 mm = 21.5 cm
水平 dpi = 水平 resolution * 2.54 / width = 1024 * 2.54 / 28.7 = 90.6 
垂直 dpi = 垂直 resolution * 2.54 / height = 768 * 2.54 / 21.5 = 90.7
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;计算得到dpi就可以在fontconfig里面设定：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;match&lt;/span&gt; &lt;span class="na"&gt;target=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;pattern&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;edit&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dpi&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;mode=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;assign&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;double&amp;gt;&lt;/span&gt;96&lt;span class="nt"&gt;&amp;lt;/double&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/edit&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/match&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;抗锯齿&lt;/h4&gt;
&lt;p&gt;抗锯齿设定可以增加字体边缘的分辨率，配置如下&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;match&lt;/span&gt; &lt;span class="na"&gt;target=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;font&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;edit&lt;/span&gt; &lt;span class="na"&gt;mode=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;assign&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;antialias&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;bool&amp;gt;&lt;/span&gt;true&lt;span class="nt"&gt;&amp;lt;/bool&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/edit&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/match&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;字体微调&lt;/h4&gt;
&lt;p&gt;使用普通微调，TrueType微调指令会被freetype的字节码解释器所解释，对于那些有好的
微调指令的字体是一个好选择，然后选择的微调效果是&lt;code&gt;hintslight&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;match&lt;/span&gt; &lt;span class="na"&gt;target=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;font&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;edit&lt;/span&gt; &lt;span class="na"&gt;mode=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;assign&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;hinting&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;bool&amp;gt;&lt;/span&gt;true&lt;span class="nt"&gt;&amp;lt;/bool&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/edit&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/match&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;match&lt;/span&gt; &lt;span class="na"&gt;target=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;font&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;edit&lt;/span&gt; &lt;span class="na"&gt;mode=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;assign&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;hintstyle&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;const&amp;gt;&lt;/span&gt;hintslight&lt;span class="nt"&gt;&amp;lt;/const&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/edit&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/match&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;次像素渲染&lt;/h4&gt;
&lt;p&gt;次像素渲染能通过使用次像素有效地提高字体的水平（垂直）分辨率。注意在没有使用
&lt;a href="http://www.infinality.net/blog/"&gt;Infinality&lt;/a&gt;补丁的情况下autohint和subpixel
rendering不能一起使用。通常情况下现在的显示器使用宏红、绿、蓝（&lt;span class="caps"&gt;RGB&lt;/span&gt;）标准生产，
所以大多数情况下把次像素渲染类型设定为&lt;code&gt;RGB&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;match&lt;/span&gt; &lt;span class="na"&gt;target=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;font&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;edit&lt;/span&gt; &lt;span class="na"&gt;mode=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;assign&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;rgba&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;const&amp;gt;&lt;/span&gt;rgb&lt;span class="nt"&gt;&amp;lt;/const&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/edit&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/match&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;当使用次像素渲染的时候需要开启&lt;a href="http://www.freetype.org/freetype2/docs/reference/ft2-lcd_filtering.html"&gt;&lt;span class="caps"&gt;LCD&lt;/span&gt;
filter&lt;/a&gt;
（液晶过滤），这个选项的取值一般来说选&lt;code&gt;lcddefault&lt;/code&gt;就可以，完整取值请参考&amp;nbsp;fontconfig手册。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;match&lt;/span&gt; &lt;span class="na"&gt;target=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;font&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;edit&lt;/span&gt; &lt;span class="na"&gt;mode=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;assign&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;lcdfilter&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;const&amp;gt;&lt;/span&gt;lcddefault&lt;span class="nt"&gt;&amp;lt;/const&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/edit&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/match&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;字体替换&lt;/h3&gt;
&lt;p&gt;这里简要说明以下字体替换，替换的简洁写法是采用&lt;code&gt;alias&lt;/code&gt;标签。在&lt;code&gt;family&lt;/code&gt;标签里面写
好要替换的字体族名字，然后最后那个&lt;code&gt;prefer&lt;/code&gt;标签的意思是，在匹配的字体列表前面插
入一些列的字体名字。需要注意的是这个列表里面的字体应该有一些是你的系统里面有的&amp;nbsp;。&lt;/p&gt;
&lt;p&gt;Gentoo安装一些要用到的字体：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;emerge media-fonts/corefonts &lt;span class="c1"&gt;#ms core font&lt;/span&gt;
emerge media-fonts/dejavu &lt;span class="c1"&gt;#DejaVu fonts&lt;/span&gt;
emerge media-fonts/ttf-bitstream-vera &lt;span class="c1"&gt;#Bitstream Vera font family&lt;/span&gt;
emerge media-fonts/arphicfonts &lt;span class="c1"&gt;#ukai uming等字体&lt;/span&gt;
emerge wqy-bitmapfont wqy-microhei wqy-zenhei &lt;span class="c1"&gt;#文泉驿字体&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;还需要一些雅黑、宋体一类的字体，就可以自己从windows下复制，最后字体替换顺序如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;alias&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;serif&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;prefer&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;DejaVu Serif&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;Bitstream Vera Serif&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;Times New Roman&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;SimSun&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;AR PL New Sung&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;AR PL ShanHeiSun Uni&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/prefer&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/alias&amp;gt;&lt;/span&gt;

&lt;span class="nt"&gt;&amp;lt;alias&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;sans-serif&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;prefer&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;Arial&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;Open Sans&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;Microsoft YaHei&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;WenQuanYi Micro Hei&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/prefer&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/alias&amp;gt;&lt;/span&gt;

&lt;span class="nt"&gt;&amp;lt;alias&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;monospace&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;prefer&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;PowerlineSymbols&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;Inconsolata&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;WenQuanYi Zen Hei Mono&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;WenQuanYi Micro Hei Mono&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;family&amp;gt;&lt;/span&gt;Arial Unicode MS&lt;span class="nt"&gt;&amp;lt;/family&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/prefer&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/alias&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;部署配置文件&lt;/h3&gt;
&lt;p&gt;通过以上步骤就能获得一个fontconfig配置文件，文件比较长，就不再复制过来，可以从
我的&lt;a href="https://gist.github.com/wugh/7386376"&gt;gist&lt;/a&gt;下载，下载完之后把文件放到&lt;span class="caps"&gt;HOME&lt;/span&gt;
目录下，重新命名为&lt;code&gt;.fonts.conf&lt;/code&gt;，不过这种方法快要被fontconfig抛弃了，现在建议的
做法是把文件放到&lt;code&gt;~/.config/fontconfig/conf.d/&lt;/code&gt;下命名的时候在前面加个数字，表示&amp;nbsp;一个优先级，操作如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir -p ~/.config/fontconfig/conf.d/
mv fonts.conf ~/.config/fontconfig/conf.d/40-myfonts.conf
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;重启X&amp;nbsp;Server之后就能看到生效的字体效果。&lt;/p&gt;
&lt;h3&gt;参考文章&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://wiki.archlinux.org/index.php/Font_Configuration"&gt;Archlinux Font&amp;nbsp;Configuration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.freedesktop.org/software/fontconfig/fontconfig-user.html"&gt;fonts-conf&amp;nbsp;document&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://codex.wordpress.org.cn/Fontconfig用户手册"&gt;fontconfig中文文档&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary><category term="fontconfig"></category></entry><entry><title>解决Linux下mp3标签乱码</title><link href="https://wugh.github.io/posts/2013/11/linux-mp3-tag-problem/" rel="alternate"></link><published>2013-11-09T09:29:46+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2013-11-09:posts/2013/11/linux-mp3-tag-problem/</id><summary type="html">&lt;p&gt;在Linux下mp3标签出现乱码的原因主要是下载到的mp3都使用&lt;a href="http://zh.wikipedia.org/wiki/GB_2312"&gt;&lt;span class="caps"&gt;GB2312&lt;/span&gt;&lt;/a&gt;编码，然而Linux下使用的编码是&lt;a href="http://zh.wikipedia.org/wiki/UTF-8"&gt;&lt;span class="caps"&gt;UTF&lt;/span&gt;-8&lt;/a&gt;编码，所以只要对标签编码做一个转换就可以解决问题。&lt;/p&gt;
&lt;h3&gt;安装软件&lt;/h3&gt;
&lt;p&gt;下面以Gentoo为例子安装&lt;a href="https://code.google.com/p/mutagen/"&gt;Mutagen&lt;/a&gt;，其他的发行&amp;nbsp;版自行Google。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;emerge --ask media-libs/mutagen
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;转换当前目录下的所有mp3文件标签&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;find . -iname &lt;span class="s2"&gt;&amp;quot;*.mp3&amp;quot;&lt;/span&gt; -exec mid3iconv -e gbk &lt;span class="o"&gt;{}&lt;/span&gt; &lt;span class="se"&gt;\;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;转换当前目录下的所有ape文件标签&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;find . -iname &lt;span class="s2"&gt;&amp;quot;*.ape&amp;quot;&lt;/span&gt; -exec mid3iconv -e gbk &lt;span class="o"&gt;{}&lt;/span&gt; &lt;span class="se"&gt;\;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="mp3"></category><category term="乱码"></category></entry><entry><title>Linux 音频格式转换</title><link href="https://wugh.github.io/posts/2013/11/linux-audio-format-convert/" rel="alternate"></link><published>2013-11-09T09:15:30+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2013-11-09:posts/2013/11/linux-audio-format-convert/</id><summary type="html">&lt;h3&gt;需要的软件&lt;/h3&gt;
&lt;p&gt;在开始转换之前我们需要两个程序&lt;a href="http://lame.sourceforge.net/download.php"&gt;&lt;span class="caps"&gt;LAME&lt;/span&gt;&lt;/a&gt;
和&lt;a href="http://www.audiocoding.com/downloads.html"&gt;&lt;span class="caps"&gt;FAAD2&lt;/span&gt;&lt;/a&gt;，这两个包应该通过linux自带&amp;nbsp;的软件包管理器就可以安装。Gentoo下面安装方法如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;emerge --ask media-sound/lame
emerge --ask media-libs/faad2
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;批量转换&lt;/h3&gt;
&lt;h4&gt;m4a转换成mp3&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; i in *.m4a&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Converting: &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;i&lt;/span&gt;&lt;span class="p"&gt;%.m4a&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;.mp3&amp;quot;&lt;/span&gt;
    faad -o - &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$i&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; lame - &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;i&lt;/span&gt;&lt;span class="p"&gt;%.m4a&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;.mp3&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;flac转mp3&lt;/h4&gt;
&lt;p&gt;另外可以使用flac转mp3，需要以下三个命令flac lame&amp;nbsp;id3，然后使用以下脚本&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#! /bin/sh&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; a in *.flac&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="nv"&gt;OUTF&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;a&lt;/span&gt;&lt;span class="p"&gt;%.flac&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.mp3

    &lt;span class="nv"&gt;ARTIST&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;metaflac &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$a&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; --show-tag&lt;span class="o"&gt;=&lt;/span&gt;ARTIST &lt;span class="p"&gt;|&lt;/span&gt; sed s/.*&lt;span class="o"&gt;=&lt;/span&gt;//g&lt;span class="k"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;TITLE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;metaflac &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$a&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; --show-tag&lt;span class="o"&gt;=&lt;/span&gt;TITLE &lt;span class="p"&gt;|&lt;/span&gt; sed s/.*&lt;span class="o"&gt;=&lt;/span&gt;//g&lt;span class="k"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;ALBUM&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;metaflac &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$a&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; --show-tag&lt;span class="o"&gt;=&lt;/span&gt;ALBUM &lt;span class="p"&gt;|&lt;/span&gt; sed s/.*&lt;span class="o"&gt;=&lt;/span&gt;//g&lt;span class="k"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;GENRE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;metaflac &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$a&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; --show-tag&lt;span class="o"&gt;=&lt;/span&gt;GENRE &lt;span class="p"&gt;|&lt;/span&gt; sed s/.*&lt;span class="o"&gt;=&lt;/span&gt;//g&lt;span class="k"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;TRACKNUMBER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;metaflac &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$a&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; --show-tag&lt;span class="o"&gt;=&lt;/span&gt;TRACKNUMBER &lt;span class="p"&gt;|&lt;/span&gt; sed s/.*&lt;span class="o"&gt;=&lt;/span&gt;//g&lt;span class="k"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;DATE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;metaflac &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$a&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; --show-tag&lt;span class="o"&gt;=&lt;/span&gt;DATE &lt;span class="p"&gt;|&lt;/span&gt; sed s/.*&lt;span class="o"&gt;=&lt;/span&gt;//g&lt;span class="k"&gt;)&lt;/span&gt;

    flac -c -d &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$a&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; lame -m j -q &lt;span class="m"&gt;0&lt;/span&gt; --vbr-new -V &lt;span class="m"&gt;0&lt;/span&gt; -s 44.1 - &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$OUTF&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    id3 -t &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$TITLE&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; -T &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;TRACKNUMBER&lt;/span&gt;&lt;span class="k"&gt;:-&lt;/span&gt;&lt;span class="nv"&gt;0&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; -a &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$ARTIST&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; -A &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$ALBUM&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; -y &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$DATE&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; -g &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;GENRE&lt;/span&gt;&lt;span class="k"&gt;:-&lt;/span&gt;&lt;span class="nv"&gt;12&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$OUTF&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="格式转换"></category></entry><entry><title>gentoo wifi 共享</title><link href="https://wugh.github.io/posts/2013/11/gentoo-wifi-share/" rel="alternate"></link><published>2013-11-09T00:57:27+08:00</published><author><name>Guohua Wu</name></author><id>tag:wugh.github.io,2013-11-09:posts/2013/11/gentoo-wifi-share/</id><summary type="html">&lt;h3&gt;简介&lt;/h3&gt;
&lt;p&gt;本文主要描述如何在&lt;code&gt;Gentoo&lt;/code&gt;下利用无线网卡做一个&lt;code&gt;wifi&lt;/code&gt;热点来给手机提供无线网络访
问。目前在&lt;code&gt;Windows&lt;/code&gt;下有&lt;a href="http://www.connectify.me/"&gt;connectify&lt;/a&gt;来共享网络，同样的在&lt;code&gt;Linux&lt;/code&gt;下也有&lt;a href="http://linuxwireless.org/en/users/Documentation/hostapd"&gt;hostapd&lt;/a&gt;。
由于在Windows下基本就是一键配置热点，但是Linux配置起来比较麻烦，所以本文在这里&amp;nbsp;记录一下配置过程。&lt;/p&gt;
&lt;h3&gt;软件安装&lt;/h3&gt;
&lt;p&gt;需要安装一些软件&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;emerge -a hostapd
emerge -a dnsmasq
emerge -a iptables
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;配置&lt;/h3&gt;
&lt;p&gt;以下描述各个软件的配置说明。&lt;/p&gt;
&lt;h4&gt;网卡参数&lt;/h4&gt;
&lt;p&gt;修改文件&lt;code&gt;/etc/conf.d/net&lt;/code&gt;：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;modules_wlan0&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;!iwconfig !wpa_supplicant&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;config_wlan0&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;192.168.0.1/24&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;网卡配置好之后，要加入默认启动。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; /etc/init.d
ln -s net.lo net.wlan0
/etc/init.d/net.wlan0 start
rc-update add net.wlan0 default
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;hostapd配置&lt;/h4&gt;
&lt;p&gt;修改&lt;code&gt;hostapd&lt;/code&gt;配置文件&lt;code&gt;/etc/hostapd/hostapd.conf&lt;/code&gt;：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;interface&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;wlan0
&lt;span class="c1"&gt;#bridge=br0                         (optional, if you want bridging remove the #)&lt;/span&gt;
&lt;span class="nv"&gt;driver&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;nl80211
&lt;span class="nv"&gt;ssid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;MyNetwork     &lt;span class="c1"&gt;#热点名称&lt;/span&gt;
&lt;span class="nv"&gt;channel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1
&lt;span class="nv"&gt;hw_mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;g
&lt;span class="nv"&gt;wpa&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;3
&lt;span class="nv"&gt;wpa_passphrase&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Your passphrase  &lt;span class="c1"&gt;#热点密码&lt;/span&gt;
&lt;span class="nv"&gt;wpa_key_mgmt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;WPA-PSK
&lt;span class="nv"&gt;wpa_pairwise&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;TKIP
&lt;span class="nv"&gt;rsn_pairwise&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;CCMP
&lt;span class="nv"&gt;macaddr_acl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0
&lt;span class="nv"&gt;auth_algs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1
&lt;span class="nv"&gt;ignore_broadcast_ssid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0
&lt;span class="nv"&gt;logger_syslog&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;-1
&lt;span class="nv"&gt;logger_syslog_level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;2
&lt;span class="nv"&gt;logger_stdout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;-1
&lt;span class="nv"&gt;logger_stdout_level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1
&lt;span class="nv"&gt;debug&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0
&lt;span class="nv"&gt;dump_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/tmp/hostapd.dump
&lt;span class="nv"&gt;ctrl_interface&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/var/run/hostapd
&lt;span class="nv"&gt;ctrl_interface_group&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0
&lt;span class="nv"&gt;accept_mac_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/etc/hostapd/hostapd.accept
&lt;span class="nv"&gt;deny_mac_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/etc/hostapd/hostapd.deny
&lt;span class="nv"&gt;auth_algs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;把hostapd加入默认启动。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/etc/init.d/hostapd start
rc-update add hostapd default
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;dnsmasq配置&lt;/h4&gt;
&lt;p&gt;修改&lt;code&gt;dnsmasq&lt;/code&gt;配置文件&lt;code&gt;/etc/dnsmasq.conf&lt;/code&gt;：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;interface&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;wlan0
bind-interfaces
dhcp-range&lt;span class="o"&gt;=&lt;/span&gt;192.168.0.50,192.168.0.150,12h
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;dnsmasq用来分配&lt;span class="caps"&gt;IP&lt;/span&gt;，所以要启动这个服务。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/etc/init.d/dnsmasq start
rc-update add dnsmasq default
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;iptables配置&lt;/h4&gt;
&lt;p&gt;iptables用来设置包的转发规则，以&lt;span class="caps"&gt;NAT&lt;/span&gt;方式配置&lt;span class="caps"&gt;AP&lt;/span&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;iptables -F
iptables -t nat -F
iptables -A FORWARD -i wlan0 -s 192.168.0.0/255.255.0.0 -j ACCEPT
iptables -A FORWARD -i eth0 -d 192.168.0.0/255.255.0.0 -j ACCEPT
iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &amp;gt; /proc/sys/net/ipv4/ip_forward
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;保存规则，并把iptable默认启动&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/etc/init.d/iptables save
rc-update add iptables default
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;总结&lt;/h3&gt;
&lt;p&gt;以上这些东西配置后之后，我们的手机应该就可以连上的点的wifi热点。&lt;/p&gt;
&lt;h3&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://en.gentoo-wiki.com/wiki/Atheros_Ath5k/Ath9k_Wireless_Access_Point#Without_Ethernet_Bridging"&gt;Atheros Ath5k/Ath9k Wireless Access&amp;nbsp;Point&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://en.gentoo-wiki.com/wiki/Wireless/Access_point"&gt;Wireless/Access&amp;nbsp;point&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary><category term="hostapd"></category><category term="ap"></category><category term="Gentoo"></category></entry></feed>