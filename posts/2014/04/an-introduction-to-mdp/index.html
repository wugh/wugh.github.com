<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>MDP入门 &mdash; Life in a Nutshell</title>
  <meta name="author" content="Guohua Wu">

  <link href="https://wugh.github.io/feeds/main.atom.xml" type="application/atom+xml" rel="alternate"
        title="Life in a Nutshell Atom Feed" />





  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="https://wugh.github.io/favicon.png" rel="icon">

  <link href="https://wugh.github.io/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.useso.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.useso.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="https://wugh.github.io/">Life in a Nutshell</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="https://wugh.github.io/feeds/main.atom.xml" rel="subscribe-atom">Atom</a></li>
</ul>


<ul class="main-navigation">
    <li><a href="/archives.html">Archives</a></li>
      <li><a href="https://wugh.github.io/pages/about-me.html">About&nbsp;Me</a></li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">MDP入门</h1>
    <p class="meta">
<time datetime="2014-04-25T19:19:36+08:00" pubdate>2014-04-25(Fri)</time>    </p>
</header>

  <div class="entry-content"><h3>介绍</h3>
<p><span class="caps">MDP</span>（Markov Decision
Process）由5元组构成<span class="math">\(MDP(S,A,{P_{sa}},\gamma,R)\)</span>，具体的&nbsp;参数介绍如下：</p>
<ul>
<li><span class="math">\(S\)</span>：状态集合</li>
<li><span class="math">\(A\)</span>：动作集合</li>
<li><span class="math">\(P_{sa}\)</span>：状态转移概率分布，<span class="math">\(P_{sa}(s')\)</span>表示在<span class="math">\(s\)</span>状态下采取
  <span class="math">\(s\)</span>动作，转移到<span class="math">\(s'\)</span>的概率，<span class="math">\(P_{sa}(s')\geq0\)</span></li>
<li><span class="math">\(\gamma\)</span>：折扣系数取值范围<span class="math">\(0\leq\gamma\le1\)</span></li>
<li><span class="math">\(R\)</span>：回报函数，<span class="math">\(R:S\mapsto&nbsp;\mathbb{R}\)</span></li>
</ul>
<p>下面举一个例子来说明<span class="caps">MDP</span>的参数，假设一个机器人在如所描述的
网格中走动，灰色代表障碍物，当机器人走到<span class="math">\((4,3)\)</span>位置将获得<span class="math">\(+1\)</span>的回报，走到
<span class="math">\((4,2)\)</span>位置回报为<span class="math">\(-1\)</span>。</p>
<p><img alt="MDP简单例子" src="https://wugh.github.io/images/ML/simple-pomdp.png" />
<img alt="动作执行受噪声干扰" src="https://wugh.github.io/images/ML/noisy.png" /></p>
<p>如果用<span class="caps">MDP</span>来描述这个例子，那么<span class="math">\(S\)</span>就有<span class="math">\(11\)</span>中取值，机器人可能处在除了障碍物的其他
位置；<span class="math">\(A\)</span>就有<span class="math">\(4\)</span>种取值，机器人可以往<span class="math">\({N,S,E,W}\)</span>四个方向走；假设现在处于<span class="math">\([3,1]\)</span>
位置，采取动作<span class="math">\(N\)</span>（虽然命令机器人向前走，但是由于噪声的影响，可能机器人会向左
或者向右走，如），假设<span class="math">\(P_{[3,1]N}\)</span>分布如下
<span class="math">\(P_{[3,1]N}([3,2])=0.8\)</span>，<span class="math">\(P_{[3,1]N}([2,1])=0.1\)</span>，<span class="math">\(P_{[3,1]N}([4,1])=0.1\)</span>，
<span class="math">\(P_{[3,1]N}([1,1])=0\)</span>等（除了相邻的位置，其他位置都无法到达，所以为<span class="math">\(0\)</span>）；回报
函数<span class="math">\(R\)</span>，<span class="math">\(R([4,2])=-1\)</span>，<span class="math">\(R([4,3])=+1\)</span>，对于其他位置而言<span class="math">\(R(s)=-0.02\)</span>，因为当机器&nbsp;人每走动一步都需要消耗电量，所以对于中间状态回报是一个比较小的负数。</p>
<p>对于的描述，状态的变化过程如下描述，假设<span class="math">\(0\)</span>时刻状态是<span class="math">\(s_0\)</span>
，然后选择一个动作<span class="math">\(a_0\)</span>，根据<span class="math">\(s\_1 \thicksim P_{s_0a_0}\)</span>分布选择目标状态<span class="math">\(s_1\)</span>，再选择
动作<span class="math">\(a_1\)</span>，根据<span class="math">\(s\_2 \thicksim P_{s_1a_1}\)</span>选择目标状态<span class="math">\(s_2\)</span>，依此类推状态序列。
对于这个状态变化序列，可以计算总的回报值（Total&nbsp;Payoff）。</p>
<p>初始状态是<span class="math">\(s_0\)</span>的总回报定义如下，<span class="math">\(0\leq \gamma \le 1\)</span>：
</p>
<div class="math">$$R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots
    \label{eq:totalpayoff}$$</div>
<p>
总的回报是当前的回报，加上未来的回报，但是距离当前越远回报值权重越小，为了使得
总的回报值最大，我们需要选择一组最优动作序列<span class="math">\((a_0,a_1,\dots)\)</span>使得总回报的期望最大：</p>
<div class="math">$$E[R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots]
  \label{eq:expectedpayoff}$$</div>
<p>最后还需要引入一个定义<span class="math">\(\pi\)</span>：策略<span class="math">\(\pi\)</span>指的是，在给定状态选择一个动作，映射关系
为：<span class="math">\(S\mapsto A\)</span>。</p>
<p>所以选择一个最优的动作序列，就是说要找到一个最优的<span class="math">\(\pi\)</span>，对于
能够求解出如的最优<span class="math">\(\pi\)</span>，下面的章节会解释如何求解<span class="math">\(\pi\)</span>。</p>
<h3><span class="caps">MDP</span>求解</h3>
<p>本节我们需要定义几个辅助变量：<span class="math">\(V^{\pi}\)</span>，<span class="math">\(V^*\)</span>和<span class="math">\(\pi^*\)</span>，下面将逐步介绍&nbsp;定义。</p>
<h4><span class="math">\(V^\pi\)</span></h4>
<p>对于任意<span class="math">\(\pi\)</span>都可以定义一个值函数<span class="math">\(V^{\pi}\)</span>（映射关系是<span class="math">\(S\mapsto \mathbb{R}\)</span>）
，<span class="math">\(V^{\pi}\)</span>指的是从状态<span class="math">\(s\)</span>开始并执行策略<span class="math">\(\pi\)</span>之后所得到的总回报值的期望：</p>
<div class="math">$$V^{\pi}=E\Big[R(s_0) + \gamma R(s_1) + \dots|\pi, s_0=s\Big]
    \label{eq:vpi}$$</div>
<p>下面是一个具体的例子，如是一个<span class="math">\(\pi\)</span>，是与之
对应的<span class="math">\(V^\pi\)</span>，实际上这个策略<span class="math">\(\pi\)</span>并不是非常好，因为对于很多状态执行该策略
后趋向于走到<span class="math">\(-1\)</span>位置而不是<span class="math">\(+1\)</span>位置。中下面两行执行的动作
使得机器人偏向走到<span class="math">\(-1\)</span>位置，所以他们的总回报的期望值是负数，对于最上面一行偏向
于走到<span class="math">\(+1\)</span>位置，所以总回报都是正的。所以对于下两行的位置这个策略非常差，但是&nbsp;对于最上面那行这个策略就显得不错。</p>
<p><img alt="其中一个pi" src="https://wugh.github.io/images/ML/one-pi.png" />
<img alt="pi对应的V" src="https://wugh.github.io/images/ML/v-pi.png" /></p>
<p>下面要对<span class="math">\(V^\pi\)</span>做一个推导使得<span class="math">\(V\pi\)</span>更容易计算，这里假设当前状态<span class="math">\(s\)</span>会转移到状态
<span class="math">\(s'\)</span>。中的<span class="math">\(P_{s\pi(s)}(s')\)</span>描述的是<span class="math">\(s\)</span>状态下采取一个动作
（这个动作由<span class="math">\(\pi(s)\)</span>来确定）转移到<span class="math">\(s'\)</span>状态的概率分布，因此式子中的求和描述的就
是一个求期望的过程，总回报值的期望是当前回报加上未来回报值的期望，
也称作贝尔曼方程（Bellman’s&nbsp;Equations）。</p>
<div class="math">$$\begin{align}
  V^\pi(s) &amp;= E\Big[R(s_0) + \gamma \Big(R(s_1) + \gamma R(s_2) + \dots\Big)\Big|\pi,s_0=s\Big] \cr
  &amp;= R(s) + \gamma \sum_{s'}{P_{s\pi(s)}(s')V^\pi(s')}
\label{eq:bellman}
\end{align}$$</div>
<p>对于的例子，如果针对每个状态都写出方程，
那么就可以得到<span class="math">\(11\)</span>个线性方程组，并且有<span class="math">\(11\)</span>个未知数（每个状态都有一个<span class="math">\(V^pi(s)\)</span>），
可以通过求解这个方程组得到<span class="math">\(V^\pi\)</span>。按照的策略，我们可以计算
<span class="math">\([3,1]\)</span>位置的<span class="math">\(V^\pi\)</span>： </p>
<div class="math">$$\begin{split}
  V^\pi([3,1]) = &amp;R([3,1]) + \cr
  &amp; \gamma[0.8V\pi([3,2]) + 0.1V\pi([4,1]) + 0.1V\pi([2,1])]
\end{split}$$</div>
<h4><span class="math">\(V^*\)</span> 和 <span class="math">\(\pi^*\)</span></h4>
<p>最优值函数<span class="math">\(V^*(s)\)</span>定义如下：</p>
<p><span class="math">\(V^*(s)\)</span>是最优值函数，值得是找到一个<span class="math">\(\pi\)</span>使得对于所有的状态<span class="math">\(V^\pi(s)\)</span>最大。</p>
<div class="math">\begin{equation}
V^*(s) = \max_x V^\pi(s)
    \label{eq:vstar}
\end{equation}</div>
<p>集合和可以得到<span class="math">\(V^*\)</span>的贝尔曼方程：</p>
<div class="math">\begin{equation}
V^*(s) = R(s) + \max_a \gamma \sum_{s'}{P_{sa}(s')V^*(s')} 
  \label{eq:vstarbellman}
\end{equation}</div>
<p>根据最优值函数的贝尔曼方程，把中的常数项<span class="math">\(R(s)\)</span>和
常数系数<span class="math">\(\gamma\)</span>去掉（处于状态<span class="math">\(s\)</span>时，对于所有的<span class="math">\(a\)</span>这两个系数都相等），
就可以得到最优策略<span class="math">\(\pi^*(s)\)</span>的求解公式：</p>
<div class="math">\begin{equation}
\DeclareMathOperator*{\argmax}{arg\,max}
\pi^*(s) = \argmax_a \sum_{s'}{P_{sa}(s')V^*(s')} 
  \label{eq:bestpi}
\end{equation}</div>
<p>由可以看出<span class="math">\(\pi^*(s)\)</span>其实依赖于<span class="math">\(V^*(s)\)</span>，所以现在的主要目标是要想办法求解
<span class="math">\(V^*(s)\)</span>。根据的定义，最直接的方法就是穷举所有的<span class="math">\(\pi\)</span>，但是穷举的情况会非常多
，例如有<span class="math">\(11\)</span>个状态，<span class="math">\(4\)</span>个动作那么就有<span class="math">\(4^{11}\)</span>种组合，搜索空间呈指数增长，不大
合理，下面将介绍值迭代（Value Iteration）和策略迭代（Policy Iteration）方法来
求解<span class="math">\(V^*(s)\)</span>。</p>
<p>算法所描述的是值迭代的过程，初始化时对于所有的<span class="math">\(s\)</span>对应的
<span class="math">\(V(s)\)</span>为<span class="math">\(0\)</span>，接着对于每个<span class="math">\(V(s)\)</span>，这里的<span class="math">\(V(s)\)</span>有两种更新方式。</p>
<p><img alt="值迭代" src="https://wugh.github.io/images/ML/mdp-vi.png" /></p>
<p>第一种是对于所有的状态计算出式子右边的部分，然后同时更新所有的<span class="math">\(V(s)\)</span>,这种称作
同步更新（Synchronous Update）；另一种叫做异步更新（Asynchronous
Update），假设我们按照固定的状态顺序更新<span class="math">\(V(s)\)</span>，那么首先会更新第1个状态
的<span class="math">\(V(s)\)</span>，接着是第2个状态的<span class="math">\(V(s)\)</span>、第3个状态的<span class="math">\(V(s)\)</span>、第4个状态的<span class="math">\(V(s)\)</span>
，如果在更新第5个状态的<span class="math">\(V(s)\)</span>用到的<span class="math">\(V(s')\)</span>恰好是第1、2、3、4状态的，
那么我们使用的<span class="math">\(V(s')\)</span>是前面几次迭代更新的版本。两种方法中异步更新会
收敛地稍微快一点，值迭代会使得<span class="math">\(V(s)\)</span>不断地向<span class="math">\(V^*(s)\)</span>接近，如
是最后求解出来的<span class="math">\(V^*(s)\)</span>。</p>
<p><img alt="左图是\(V^*\)且\gamma=.99，右图是对应的\(\pi^*\)" src="https://wugh.github.io/images/ML/bestv-and-pi.png" /></p>
<p>求解出<span class="math">\(V^*(s)\)</span>之后，根据就可以计算<span class="math">\(\pi^*(s)\)</span>，
下面举一个例子计算<span class="math">\(\pi([3,1])\)</span>的最优策略，结合，可以
计算出采取各个动作的未来总回报的期望，假设机器人碰到墙壁之后会回到
原来的位置，所以机器人向<span class="math">\(E\)</span>走的时候有<span class="math">\(0.1\)</span>的可能性会碰到墙壁然后又
返回到<span class="math">\([3,1]\)</span>位置。 </p>
<div class="math">\begin{aligned}
    E: &amp; \sum_{s'}{P_{sa}(s')V^*(s')} = 0.8*0.75 + 0.1*0.69 + 0.1*0.71 = 0.74\cr
    N: &amp; \sum_{s'}{P_{sa}(s')V^*(s')} = 0.8*0.69 + 0.1*0.75 + 0.1*0.49 = 0.676\cr
    W: &amp; \sum_{s'}{P_{sa}(s')V^*(s')} = 0.8*0.49 + 0.1*0.69 + 0.1*0.71 = 0.532\cr
    S: &amp; \sum_{s'}{P_{sa}(s')V^*(s')} = 0.8*0.71 + 0.1*0.75 + 0.1*0.49 = 0.692
\end{aligned}</div>
<p>对比<span class="math">\(4\)</span>个方向的未来总回报的期望值之后，发现采取<span class="math">\(E\)</span>动作之后得到的值最大，
所以在<span class="math">\([3,1]\)</span>位置会采取动作<span class="math">\(E\)</span>。对每个状态都计算最优动作之后就可以得到如&nbsp;所示的结果。</p>
<p>描述完值迭代之后，下面简单描述一下策略迭代求解<span class="math">\(V^*(s)\)</span>，策略迭代会使得最后
<span class="math">\(V(s)\)</span>趋近于<span class="math">\(V^*(s)\)</span>并且<span class="math">\(\pi(s)\)</span>趋近于<span class="math">\(\pi^*(s)\)</span>。</p>
<p><img alt="策略迭代" src="https://wugh.github.io/images/ML/mdp-pi.png" /></p>
<p>当状态数量少的时候可以采用策略迭代，因为这时候求解贝尔曼方程比较快速，但是当
状态数非常多，例如有100万个状态，那么求解贝尔曼方程的代价可能会太大，就应该&nbsp;考虑使用值迭代。</p>
<p>这里还需要讨论一下如何求解<span class="math">\(P_{sa}\)</span>，一般来说可以用最大似然估计来估计。</p>
<div class="math">\begin{equation}
\begin{split}
    P_{sa}(s') &amp;= \frac{\text{在}s\text{状态下采取动作a到达状态}s'\text{的次数}}
    {\text{在}s\text{状态下采取动作}a\text{的次数}} \cr
    &amp;\Big(\text{如果得到}\frac{0}{0}\text{的情况就用}\frac{1}{|s|}\text{替换}\Big)
  \end{split}
  \label{eq:psa}
\end{equation}</div>
<h4>求解过程总结</h4>
<p>把上文提到的求解<span class="math">\(V^*(s)\)</span>、<span class="math">\(\pi^*(s)\)</span>和<span class="math">\(P_{sa}\)</span>的方法结合起来就可以构成一个完整
的求解<span class="caps">MDP</span>的方法。首先采取策略<span class="math">\(\pi\)</span>之后可以观测到一些状态转移的数据，用这些
数据来重新估计<span class="math">\(P_{sa}\)</span>，接着用值迭代的方式来求解当前<span class="math">\(\pi\)</span>和<span class="math">\(P_{sa}\)</span>前提
下的<span class="math">\(V^*(s)\)</span>（值迭代的初始<span class="math">\(V(s)\)</span>可以使用上一轮迭代的<span class="math">\(V^*(s)\)</span>），
最后再利用这个<span class="math">\(V^*(s)\)</span>来更新<span class="math">\(\pi\)</span>。
<img alt="整个过程" src="https://wugh.github.io/images/ML/put-together.png" /></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        Guohua Wu
    </span>
  </span>
<time datetime="2014-04-25T19:19:36+08:00" pubdate>2014-04-25(Fri)</time>  <span class="categories">
    <a class='category' href='https://wugh.github.io/category/machine-learning.html'>Machine Learning</a>
  </span>
  <span class="categories">
    <a class="category" href="https://wugh.github.io/tag/mdp.html">MDP</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
  </section>
</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/05/cs224d-notes5-recusive-neural-networks/">CS224d笔记5——递归神经网络（Recusive Neural Network, RNN）</a>
      </li>
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/03/cs224d-notes4-recurrent-neural-networks-continue/">CS224d笔记4续——RNN隐藏层计算之GRU和LSTM</a>
      </li>
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/03/cs224d-notes4-recurrent-neural-networks/">CS224d笔记4——语言模型和循环神经网络（Recurrent Neural Network, RNN）</a>
      </li>
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/03/cs224d-notes3-neural-networks-and-backward-propagation/">CS224d笔记3——神经网络</a>
      </li>
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/02/cs224d-notes2-softmax-classification-and-window-classification/">CS224d笔记2——Softmax分类和窗口分类</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="https://wugh.github.io/category/linux.html">Linux</a></li>
        <li><a href="https://wugh.github.io/category/machine-learning.html">Machine Learning</a></li>
        <li><a href="https://wugh.github.io/category/nlp.html">NLP</a></li>
        <li><a href="https://wugh.github.io/category/web.html">Web</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="https://wugh.github.io/tag/pelican.html">Pelican</a>,    <a href="https://wugh.github.io/tag/lu-yin.html">录音</a>,    <a href="https://wugh.github.io/tag/pulse.html">pulse</a>,    <a href="https://wugh.github.io/tag/vim.html">vim</a>,    <a href="https://wugh.github.io/tag/ap.html">ap</a>,    <a href="https://wugh.github.io/tag/mp3.html">mp3</a>,    <a href="https://wugh.github.io/tag/mdp.html">MDP</a>,    <a href="https://wugh.github.io/tag/fontconfig.html">fontconfig</a>,    <a href="https://wugh.github.io/tag/lstm.html">LSTM</a>,    <a href="https://wugh.github.io/tag/gru.html">GRU</a>,    <a href="https://wugh.github.io/tag/jinja.html">Jinja</a>,    <a href="https://wugh.github.io/tag/hexo.html">hexo</a>,    <a href="https://wugh.github.io/tag/nei-he-bian-yi.html">内核编译</a>,    <a href="https://wugh.github.io/tag/portage.html">Portage</a>,    <a href="https://wugh.github.io/tag/di-gui-shen-jing-wang-luo.html">递归神经网络</a>,    <a href="https://wugh.github.io/tag/hou-xiang-chuan-bo.html">后向传播</a>,    <a href="https://wugh.github.io/tag/luan-ma.html">乱码</a>,    <a href="https://wugh.github.io/tag/word2vec.html">word2vec</a>,    <a href="https://wugh.github.io/tag/rnn.html">RNN</a>,    <a href="https://wugh.github.io/tag/gentoo.html">Gentoo</a>,    <a href="https://wugh.github.io/tag/xun-huan-shen-jing-wang-luo.html">循环神经网络</a>,    <a href="https://wugh.github.io/tag/wen-ben-fen-lei.html">文本分类</a>,    <a href="https://wugh.github.io/tag/yu-yan-mo-xing.html">语言模型</a>,    <a href="https://wugh.github.io/tag/shen-jing-wang-luo.html">神经网络</a>,    <a href="https://wugh.github.io/tag/latex.html">latex</a>,    <a href="https://wugh.github.io/tag/svm.html">SVM</a>,    <a href="https://wugh.github.io/tag/hostapd.html">hostapd</a>,    <a href="https://wugh.github.io/tag/qing-gan-fen-xi.html">情感分析</a>,    <a href="https://wugh.github.io/tag/ge-shi-zhuan-huan.html">格式转换</a>,    <a href="https://wugh.github.io/tag/maxent.html">MaxEnt</a>,    <a href="https://wugh.github.io/tag/filter.html">Filter</a>,    <a href="https://wugh.github.io/tag/bpsuan-fa.html">bp算法</a>,    <a href="https://wugh.github.io/tag/shen-du-xue-xi.html">深度学习</a>,    <a href="https://wugh.github.io/tag/softmax.html">softmax</a>,    <a href="https://wugh.github.io/tag/ju-fa-fen-xi.html">句法分析</a>,    <a href="https://wugh.github.io/tag/chuang-kou-fen-lei.html">窗口分类</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://wugh.github.io/feeds/main.atom.xml" type="application/atom+xml" rel="alternate">Atom</a></li>
            <li><a href="https://github.com/wugh" target="_blank">GitHub</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2013&ndash;2016  Guohua Wu &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="https://wugh.github.io/theme/js/modernizr-2.0.js"></script>
  <script src="https://wugh.github.io/theme/js/ender.js"></script>
  <script src="https://wugh.github.io/theme/js/octopress.js" type="text/javascript"></script>
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-52495737-1', 'auto');

    ga('send', 'pageview');
    </script>
  <script type="text/javascript">
    var disqus_shortname = 'guohuasblog';
    var disqus_identifier = '/posts/2014/04/an-introduction-to-mdp/';
    var disqus_url = 'https://wugh.github.io/posts/2014/04/an-introduction-to-mdp/';
    var disqus_title = '<span class="caps">MDP</span>入门';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>