<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Learning to Rank入门 &mdash; Life in a Nutshell</title>
  <meta name="author" content="Guohua Wu">

  <link href="https://wugh.github.io/feeds/main.atom.xml" type="application/atom+xml" rel="alternate"
        title="Life in a Nutshell Atom Feed" />





  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="https://wugh.github.io/favicon.png" rel="icon">

  <link href="https://wugh.github.io/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.useso.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.useso.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="https://wugh.github.io/">Life in a Nutshell</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="https://wugh.github.io/feeds/main.atom.xml" rel="subscribe-atom">Atom</a></li>
</ul>


<ul class="main-navigation">
    <li><a href="/archives.html">Archives</a></li>
      <li><a href="https://wugh.github.io/pages/about-me.html">About&nbsp;Me</a></li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Learning to Rank入门</h1>
    <p class="meta">
<time datetime="2016-10-20T09:37:00+08:00" pubdate>2016-10-20(Thu)</time>    </p>
</header>

  <div class="entry-content"><p>下面主要是阅读刘铁岩老师的《Learning to Rank for Information Retrieval》的一些
学习笔记。Learning to Ranking指的是根据训练数据用机器学习模型来解决排序问题。很
多<span class="caps">IR</span>问题都可以看成是排序问题，例如文档检索、协同过滤、关键词抽取、定义查找，本
文主要以文档检索为例子来描述Learning to&nbsp;Rank。</p>
<h2>传统<span class="caps">IR</span>模型</h2>
<p>传统的<span class="caps">IR</span>模型可以分成两类，一类是query-dependent，另一类是query-independent。</p>
<p>query-dependent指的是检索的时候考虑query和doc之间的关心，衡量这个关系的不同方法
产生了不同的模型,&nbsp;考虑doc和query的相关性:</p>
<ol>
<li>Boolean model: 对doc中的term做倒排，然后query是一个boolean表达式，对取回来的&nbsp;文档集合做交集并集等等</li>
<li>Vector space model: 把doc和query都表示在一个统一的欧拉空间（每个term一个维度
   ），每一个维度的值一般可以是<span class="caps">TF</span>-<span class="caps">IDF</span>，然后计算cosine距离</li>
<li><span class="caps">LSI</span>: 根据term和doc构建一个矩阵，矩阵里面的值可以是<span class="caps">TF</span>-<span class="caps">IDF</span>，然后进行矩阵分解，
   得到term和doc的向量表示，新来一个query之后转换成低维空间的向量计算和doc的&nbsp;cosine相似度</li>
<li><span class="caps">BM25</span>:&nbsp;计算query和doc相关性分的经典方法</li>
<li>基于语言模型的方法:&nbsp;利用语言模型预估P(q|d)的概率</li>
</ol>
<p>不考虑doc和query的相关性的时候，更重要的是如何考虑一个文档的质量，可以用PageRank&nbsp;衡量
</p>
<div class="math">$$PR(d_u)=\sum_{d_v\in B_u}\frac{PR(d_v)}{U(d_v)}$$</div>
<h2><span class="caps">IR</span>基于位置的评估方法</h2>
<p>对于一个query给定m个对应的文档<span class="math">\(\left\{d_j\right\}_{j=1}^m\)</span>，有三种方式给出文档&nbsp;和query的关系:</p>
<ol>
<li>给出每个doc和query的相关性分，一般有两级和多级的表示方法，相关性之间可以比较</li>
<li>对于某个query给出文档之间的偏序关系，<span class="math">\(l_{u,v}=1\)</span>表示u比v更相关</li>
<li>直接给出这个query下m篇文档的排列顺序</li>
</ol>
<p>评估方法:</p>
<ol>
<li><span class="caps">MRR</span> (Mean reciprocal rank): 对于query来说第一个相关文档的位置<span class="math">\(r(q)\)</span>，那么<span class="math">\(1/r(q)\)</span>就是这个query的<span class="caps">MRR</span></li>
<li><span class="caps">MAP</span> (Mean Average precision): 首先P(k)指的是排序列表中topk个位置的精度，对每个query求AveP取平均得到<span class="caps">MAP</span>
<div class="math">$$P(k)=\frac{\mbox{topk中相关的文档个数}}{k}$$</div>
<div class="math">$$\operatorname{AveP} = \frac{\sum_{k=1}^n (P(k) \times \operatorname{rel}(k))}{\mbox{number of relevant documents}}$$</div>
</li>
<li><span class="caps">NDCG</span> (Normalised Discounted cumulative gain): 给定排序列表后p位置的<span class="caps">DCG</span>如下
<div class="math">$$ \mathrm{DCG_{p}} = rel_{1} + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}i}. $$</div>
<span class="caps">NDGC</span>指的对<span class="math">\(DCG_p\)</span>进行了归一化，把<span class="math">\(DCG_p\)</span>除以了完全正确排序时的<span class="math">\(DCG_p\)</span>
<div class="math">$$ \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG{p}}. $$</div>
</li>
<li><span class="caps">RC</span> (Rank correlation): <span class="caps">RC</span>给出了两个完整排序列表的不一致性, 例如可以用
<a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient">Kendall rank correlation coefficient</a>衡量</li>
</ol>
<p>这些评估方法都是以单个query为基础来评估的，并且每个query在评估文档排列时都需要&nbsp;考虑文档所处的位置。</p>
<h2>Learning to&nbsp;Rank</h2>
<p>文中把利用机器学习技术来解决排序问题的方法称为Learning to Rank。一般的做法是给
定query和doc抽取特征和真是label，特征包括doc本身的特征(PageRank值，点击率等等)
，以及query和doc之间的相关性特征(<span class="caps">BM25</span>等)，然后用机器学习方法去学习如何从特征空
间映射到label的空间。这些机器学习方法大体上可以分成三类：pointwise，pairwise，&nbsp;listwise。下面主要谈谈自己对这三种方法的理解，具体的参见参考文献。</p>
<h3>pointwise</h3>
<p>pointwise方法比较简单粗暴，把当个doc和query的特征看成输入，然后以这个pair是否相
关为目标来训练数据。关心单个文档和特定query的相关性。最后按照模型的相关性得分输&nbsp;出给文档排序，得到query下doc的排序。</p>
<p>pointwise按照模型输出空间的不同又可以分成三类：回归，分类，ordinal regression（
类似分类，但是在这种回归的学习目标是学习到目标空间的几个有序划分点，然后每一段代表&nbsp;一个分类）。</p>
<p>回归方法最简单的理解就是把问题看成一个线性回归问题，那么对于输入向量<span class="math">\(x\)</span>可以用以下<span class="math">\(f(x)\)</span>拟合相关分:
</p>
<div class="math">$$f_w(x) = w_0 + w_1x_1 + w_2x_2 + \cdots + w_nx_n$$</div>
<p>
损失函数可以采用square loss,&nbsp;y为真是相关性分：
</p>
<div class="math">$$L(f(x), y) = (y-f(x))^2$$</div>
<p>看成分类问题时可以把是否相关看成两个分类，如果数据是多分类标注时就看成n个分类来学习，
采用logistics regression，最大熵，<span class="caps">SVM</span>都可以很好的解决该分类问题。</p>
<p>pointwise存在的问题:</p>
<ol>
<li>训练过程只考虑了单篇文档，没有学习到两篇文档的相对关系</li>
<li>没有考虑到<span class="caps">IR</span>中评估性能的两个方面，没有考虑到一个query对应多个文档、loss函数&nbsp;没有体现文档在排序列表中的位置</li>
</ol>
<h3>pairwise</h3>
<p>pairwise方法解决了pointwise方法存在的第一个问题（训练过程考虑文档的相对顺序，即&nbsp;在这个query下某个doc1和doc2的相关性谁强）。关心特定query下两个文档之间的相对顺序。</p>
<p>pairwise的学习目标是要减少误分类的文档对，即数据给定的方式是<span class="math">\((q, d_1, d_2)\)</span>，然后
如果<span class="math">\(d_1\)</span>比<span class="math">\(d_2\)</span>相关那么label为1，否则label为-1。</p>
<p>pairwise的中有些方法的方法感觉可以分成两种类型，一种是hypothesis就是建模两个文档
之间的相对关系，假定两个文档<span class="math">\(x_u\)</span>和<span class="math">\(x_v\)</span>那么，<span class="math">\(h(x_u,x_v)\)</span>直接建模这两个文档的相对关系；
第二种模式是hypothesis建模还是单个文档的分，然后再损失函数里面考虑两个文档的相对关系，
假设<span class="math">\(f(x_u)\)</span>和<span class="math">\(f(x_v)\)</span>分别为两个文档的分数，然后如果<span class="math">\(f(x_u)&gt;f(x_v)\)</span>表示u比v相关。&nbsp;下面主要针对第二种模式给出一个经典算法RankNet。其他经典方法还有RankSVM。</p>
<p>在RankNet用<span class="math">\(P_{u,v}\)</span>表示<span class="math">\(x_u\)</span>比<span class="math">\(x_v\)</span>更相关的概率，对于特定query <span class="math">\(q\)</span>如果文档
<span class="math">\(x_u\)</span>比<span class="math">\(x_v\)</span>更相关，那么我们要学习<span class="math">\(\bar{y}_{u,v}=1\)</span>，相反的时候我们希望
<span class="math">\(\bar{y}_{u,v}=0\)</span>。基于上面描述的评分函数<span class="math">\(f\)</span>，<span class="math">\(P\)</span>可以写成如下形式：
</p>
<div class="math">$$P_{u,v}(f) = \frac{\exp(f(x_u)-f(x_v))}{1+\exp(f(x_u)-f(x_v))}$$</div>
<p>可以用交叉熵来作为损失函数:
</p>
<div class="math">$$L(f;x_u,x_v,y_{u,v}) = -y_{u,v}\log{P_{u,v}}(f)-(1-y_{u,v})\log{(1-{P_{u,v}}(f))}$$</div>
<p>评分函数可以用神经网络来建模，交叉熵可以求导，直接把梯度传递给神经网络就可以完成整个训练过程。</p>
<p>虽然pairwise在考虑了文档之间的相对顺序，但是还没有把文档在ranklist中的绝对位置考虑到损失里面，&nbsp;而且没有考虑到多个文档同属于一个query这种性质。</p>
<h3>listwise</h3>
<p>listwise方法可以分成两种，一种模型的输出空间是文档列表中每个文档的相关性分数，&nbsp;这里介绍softrank；另一种是建模整个文档列表的排列顺序，这里介绍ListNet。</p>
<p>listwise方法的最大优势是损失函数直接近似于<span class="caps">IR</span>的评估指标（<span class="caps">MAP</span>，<span class="caps">NDGC</span>等），<span class="caps">IR</span>的评估&nbsp;指标一般非连续不可导，一般采用以下的替代方案:</p>
<ol>
<li>用连续可微的指标来逼近<span class="caps">IR</span>指标, softrank,&nbsp;apprank</li>
<li>bound <span class="caps">IR</span>指标, <span class="math">\(SVM^{\mbox{map}}\)</span>, <span class="math">\(SVM^{\mbox{ndgc}}\)</span>,&nbsp;PermuRank</li>
<li>优化complex objectives, AdaRank,&nbsp;RankGP</li>
</ol>
<p>SoftRank在文档排序分中引入随机性，用<span class="caps">NDCG</span>的期望来逼近原始的<span class="caps">NDCG</span>分数。假设给定query <span class="math">\(q\)</span>，
对应的文档集合是<span class="math">\(\mathbf{x}={x_j}_{j=1}^m\)</span>，<span class="math">\(x_j\)</span>的得分<span class="math">\(s_j\)</span>不再当成一个确定值来用，而是
当做一个随机变量，<span class="math">\(s_j\)</span>是一个均值为<span class="math">\(f(x_j)\)</span>方差为<span class="math">\(\sigma_s\)</span>（这个<span class="math">\(f\)</span>可以用神经网络建模，
<span class="math">\(sigma\)</span>是超参数）:
</p>
<div class="math">$$p(s_j)=N(s_j|f(x_j),\sigma_s^2)$$</div>
<p>
文档<span class="math">\(x_u\)</span>排的比<span class="math">\(x_v\)</span>靠前的概率可以描述如下:
</p>
<div class="math">$$p_{u,v}=\int_0^\inf N(s|f(x_u)-f(x_v),2\sigma_s^2)ds$$</div>
<p>
有了<span class="math">\(p_{u,v}\)</span>之后可以用迭代的方式表示用特定文档处于排序列表某个位置的概率。假设
<span class="math">\(x_j\)</span>已经存在于列表中，此时需要加入<span class="math">\(x_u\)</span>，那么如果<span class="math">\(x_u\)</span>比<span class="math">\(x_j\)</span>靠前那么加入之后<span class="math">\(x_j\)</span>位置后移一位，
否则<span class="math">\(x_j\)</span>位置不变。<span class="math">\(p_j^u(r)\)</span>表示<span class="math">\(x_u\)</span>加入后<span class="math">\(x_j\)</span>处于<span class="math">\(r\)</span>位置的概率:
</p>
<div class="math">$$p_j^u(r)=p_j^{(u-1)}(r-1)p_{u,j}+p_j^{(u-1)}(r)(1-p_{u,j})$$</div>
<p>
SoftRank通过计算<span class="caps">NDCG</span>@m的期望来作为目标函数，又称之为SoftNDCG，通过最大化目标函数来学习<span class="math">\(f\)</span>:
</p>
<div class="math">$$\mbox{SoftNDCG}\triangleq \frac{1}{Z_m}\sum_{j=1}^m(2^{y_i}-1)\sum_{r=0}^{m-1}\eta(r)p_j(r)$$</div>
<p>ListNet首先根据评分函数给文档打分，<span class="math">\(\mathbf{s}=\{s_j\}_{j=1}^m\)</span>其中<span class="math">\(s_j=f(x_j\)</span>
，然后根据Luce model来定义任意文档排列<span class="math">\(\pi\)</span>的概率:
</p>
<div class="math">$$P(\pi|\mathbf{s})=\Pi_{j=1}^m\frac{\varphi(s_{\pi^{-1}(j)})}{\sum_{u=j}^m \varphi(s_{\pi^{-1}(u)})}$$</div>
<p>
<span class="math">\(\pi^{-1}(j)\)</span>表示文档排列<span class="math">\(\pi\)</span>中第j位置的文档，<span class="math">\(\varphi\)</span>是转换函数可以是sigmoid、线性或者指数。
上述建模好了当前评分函数<span class="math">\(f\)</span>下任意排列的概率分数，我们还需要给出真实的任意排列概率分数<span class="math">\(P_y(\pi)\)</span>，
（如果真实的文档列表是按照相关性分数给出，那么可以直接用Luce model给出真实分布；如果给出的仅仅是个
排序好的列表那么可以用delta函数给出分布，或者把位置转换成一个分数之后再利用Luce model）。通过
计算两个分布的<span class="caps">KL</span>散度，可以建模损失函数:
</p>
<div class="math">$$L(f;\mathbf{x},\pi_y)=D_{\mathrm{KL}}(P(\pi|\varphi(f(w,\mathbf{x})))\| P_y(\pi))$$</div>
<p>
ListNet在训练过程中要计算的<span class="caps">KL</span>散度计算复杂度和<span class="math">\(m\)</span>呈指数关系，因为需要考虑所有排列。ListNet的
原始文献中进一步基于top-k luce model给出top-k版本的<span class="caps">KL</span>散度，把计算复杂度降低到<span class="math">\(m\)</span>的多项式复杂度。</p>
<p>基于ListNet，提出了ListMLE，基本思想和ListNet很像。根据评分函数用luce model建模排列&nbsp;的概率分布，把优化目标改成了最大化真实排列的最大似然概率:
</p>
<div class="math">$$L(f;\mathbf{x},\pi_y)=-\log P(\pi_y|\varphi(f(w,\mathbf{x})))$$</div>
<p>
ListMLE的训练数据需要给定query对应的整个排序列表，如果数据按pairwise或者pairwise方式给出&nbsp;要想办法转换成一个完整列表。</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        Guohua Wu
    </span>
  </span>
<time datetime="2016-10-20T09:37:00+08:00" pubdate>2016-10-20(Thu)</time>  <span class="categories">
    <a class='category' href='https://wugh.github.io/category/nlp.html'>NLP</a>
  </span>
  <span class="categories">
    <a class="category" href="https://wugh.github.io/tag/learning-to-rank.html">Learning to Rank</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
  </section>
</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/10/learning-to-rank-an-introduction/">Learning to Rank入门</a>
      </li>
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/05/cs224d-notes5-recusive-neural-networks/">CS224d笔记5——递归神经网络（Recusive Neural Network, RNN）</a>
      </li>
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/03/cs224d-notes4-recurrent-neural-networks-continue/">CS224d笔记4续——RNN隐藏层计算之GRU和LSTM</a>
      </li>
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/03/cs224d-notes4-recurrent-neural-networks/">CS224d笔记4——语言模型和循环神经网络（Recurrent Neural Network, RNN）</a>
      </li>
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/03/cs224d-notes3-neural-networks-and-backward-propagation/">CS224d笔记3——神经网络</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="https://wugh.github.io/category/linux.html">Linux</a></li>
        <li><a href="https://wugh.github.io/category/machine-learning.html">Machine Learning</a></li>
        <li><a href="https://wugh.github.io/category/nlp.html">NLP</a></li>
        <li><a href="https://wugh.github.io/category/web.html">Web</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="https://wugh.github.io/tag/pelican.html">Pelican</a>,    <a href="https://wugh.github.io/tag/lu-yin.html">录音</a>,    <a href="https://wugh.github.io/tag/learning-to-rank.html">Learning to Rank</a>,    <a href="https://wugh.github.io/tag/pulse.html">pulse</a>,    <a href="https://wugh.github.io/tag/vim.html">vim</a>,    <a href="https://wugh.github.io/tag/ap.html">ap</a>,    <a href="https://wugh.github.io/tag/mp3.html">mp3</a>,    <a href="https://wugh.github.io/tag/lstm.html">LSTM</a>,    <a href="https://wugh.github.io/tag/fontconfig.html">fontconfig</a>,    <a href="https://wugh.github.io/tag/shen-du-xue-xi.html">深度学习</a>,    <a href="https://wugh.github.io/tag/gru.html">GRU</a>,    <a href="https://wugh.github.io/tag/jinja.html">Jinja</a>,    <a href="https://wugh.github.io/tag/mdp.html">MDP</a>,    <a href="https://wugh.github.io/tag/nei-he-bian-yi.html">内核编译</a>,    <a href="https://wugh.github.io/tag/portage.html">Portage</a>,    <a href="https://wugh.github.io/tag/di-gui-shen-jing-wang-luo.html">递归神经网络</a>,    <a href="https://wugh.github.io/tag/hou-xiang-chuan-bo.html">后向传播</a>,    <a href="https://wugh.github.io/tag/luan-ma.html">乱码</a>,    <a href="https://wugh.github.io/tag/word2vec.html">word2vec</a>,    <a href="https://wugh.github.io/tag/rnn.html">RNN</a>,    <a href="https://wugh.github.io/tag/gentoo.html">Gentoo</a>,    <a href="https://wugh.github.io/tag/xun-huan-shen-jing-wang-luo.html">循环神经网络</a>,    <a href="https://wugh.github.io/tag/wen-ben-fen-lei.html">文本分类</a>,    <a href="https://wugh.github.io/tag/yu-yan-mo-xing.html">语言模型</a>,    <a href="https://wugh.github.io/tag/shen-jing-wang-luo.html">神经网络</a>,    <a href="https://wugh.github.io/tag/latex.html">latex</a>,    <a href="https://wugh.github.io/tag/svm.html">SVM</a>,    <a href="https://wugh.github.io/tag/hexo.html">hexo</a>,    <a href="https://wugh.github.io/tag/hostapd.html">hostapd</a>,    <a href="https://wugh.github.io/tag/qing-gan-fen-xi.html">情感分析</a>,    <a href="https://wugh.github.io/tag/ge-shi-zhuan-huan.html">格式转换</a>,    <a href="https://wugh.github.io/tag/maxent.html">MaxEnt</a>,    <a href="https://wugh.github.io/tag/filter.html">Filter</a>,    <a href="https://wugh.github.io/tag/bpsuan-fa.html">bp算法</a>,    <a href="https://wugh.github.io/tag/softmax.html">softmax</a>,    <a href="https://wugh.github.io/tag/ju-fa-fen-xi.html">句法分析</a>,    <a href="https://wugh.github.io/tag/chuang-kou-fen-lei.html">窗口分类</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://wugh.github.io/feeds/main.atom.xml" type="application/atom+xml" rel="alternate">Atom</a></li>
            <li><a href="https://github.com/wugh" target="_blank">GitHub</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2013&ndash;2016  Guohua Wu &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="https://wugh.github.io/theme/js/modernizr-2.0.js"></script>
  <script src="https://wugh.github.io/theme/js/ender.js"></script>
  <script src="https://wugh.github.io/theme/js/octopress.js" type="text/javascript"></script>
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-52495737-1', 'auto');

    ga('send', 'pageview');
    </script>
  <script type="text/javascript">
    var disqus_shortname = 'guohuasblog';
    var disqus_identifier = '/posts/2016/10/learning-to-rank-an-introduction/';
    var disqus_url = 'https://wugh.github.io/posts/2016/10/learning-to-rank-an-introduction/';
    var disqus_title = 'Learning to&nbsp;Rank入门';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>