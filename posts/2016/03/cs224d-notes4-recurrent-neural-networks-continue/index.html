<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>CS224d笔记4续——RNN隐藏层计算之GRU和LSTM &mdash; Life in a Nutshell</title>
  <meta name="author" content="Guohua Wu">

  <link href="https://wugh.github.io/feeds/main.atom.xml" type="application/atom+xml" rel="alternate"
        title="Life in a Nutshell Atom Feed" />





  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="https://wugh.github.io/favicon.png" rel="icon">

  <link href="https://wugh.github.io/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.useso.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.useso.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="https://wugh.github.io/">Life in a Nutshell</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="https://wugh.github.io/feeds/main.atom.xml" rel="subscribe-atom">Atom</a></li>
</ul>


<ul class="main-navigation">
    <li><a href="/archives.html">Archives</a></li>
      <li><a href="https://wugh.github.io/pages/about-me.html">About&nbsp;Me</a></li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">CS224d笔记4续——RNN隐藏层计算之GRU和LSTM</h1>
    <p class="meta">
<time datetime="2016-03-13T21:22:00+08:00" pubdate>2016-03-13(Sun)</time>    </p>
</header>

  <div class="entry-content"><p>本篇文章主要介绍两种<span class="caps">RNN</span>的隐藏层信息计算方法<span class="caps">GRU</span>（Gated Recurrent Units）和
<span class="caps">LSTM</span>（Long-Short-Term-Memories），这两种隐藏层的计算方法通过引入门（Gate）
的机制来解决<span class="caps">RNN</span>的梯度消失问题，从而学习到长距离依赖。</p>
<p>这里说的隐藏层计算方法指的是如何计算下个时刻的隐藏层信息，标准<span class="caps">RNN</span>中计算&nbsp;方法是：
</p>
<div class="math">$$h_t=f(W^{(hh)}h_{t-1}+W^{(hx)}x_t)$$</div>
<p>
而<span class="caps">LSTM</span>和<span class="caps">GRU</span>可以理解为计算<span class="math">\(h_t\)</span>的另一种方法。</p>
<h2><span class="caps">LSTM</span></h2>
<p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">这篇文章</a>详细 地解释了<span class="caps">LSTM</span>各个门的物理含义，
以及<span class="caps">LSTM</span>计算隐藏层的方法，这里简要的进行总结，
下图是<span class="caps">LSTM</span>网络的示意图，图中各个符号的含义参考<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">原文</a>：
<img alt="LSTM RNN" src="https://wugh.github.io/images/NLP/rnn-lstm-chain.png" style="display:block;margin:0 auto" /></p>
<p>某个时刻<span class="caps">LSTM</span>计算隐藏层的方法参考下图：
<img alt="LSTM 单元" src="https://wugh.github.io/images/NLP/rnn-lstm-unit-detail.png" style="display:block;margin:0 auto" /></p>
<p><span class="caps">LSTM</span>最核心的部分是<code>cell state</code>，即图中的<span class="math">\(c_t\)</span>。<span class="math">\(c_t\)</span>的信息贯穿整个<span class="caps">LSTM</span>，
在整个前向传播的过程中只在<span class="math">\(c_t\)</span>上进行一些简单的线性操作，通过门来控制
<span class="math">\(c_t\)</span>中信息的增减。<span class="caps">LSTM</span>中的门是通过一个sigmoid层来实现的，门输出的数值在
0~1之间，然后把门的取值向量和目标数据对应维相乘就可以达到控制数据流通的
效果。<span class="caps">LSTM</span>中有三个门，分别是<code>forget gate</code>、<code>input gate</code>和<code>output gate</code>，
这三个门的计算方法公式一样，都是根据<span class="math">\(x_t\)</span>和<span class="math">\(h_{t-1}\)</span>来计算，&nbsp;区别在于权重矩阵和偏置不同。</p>
<ol>
<li>首先是<code>forget gate</code><span class="math">\(f_t\)</span>，这个门主要控制要从<code>cell state</code>中忘记哪些信息，计算方法如下:
<div class="math">$$f_t=\sigma(W^fx_t+U^fh_{t-1})$$</div>
</li>
<li>其次是<code>input gate</code><span class="math">\(i_t\)</span>，这个门控制当前时刻的新信息（candidate hidden layer）有哪些需要添加进<code>cell state</code>中，计算方法如下:
<div class="math">$$i_t=\sigma(W^ix_t+U^ih_{t-1})$$</div>
</li>
<li>上一步提到的当前时刻新信息的计算方法如下：
<div class="math">$$\tilde{c}_t=\tanh(W^cx_t+U^ch_{t-1})$$</div>
</li>
<li>然后<span class="math">\(t\)</span>时刻<code>cell state</code>中的信息就变成<span class="math">\(c_{t-1}\)</span>中的部分信息再叠加上<span class="math">\(\tilde{c}_t\)</span>中的部分信息，计算方法如下：
<div class="math">$$c_t=f_t\circ c_{t-1} + i_t\circ\tilde{c}_t$$</div>
</li>
<li>最后还需要根据<span class="math">\(t\)</span>时刻的<code>cell state</code>输出<span class="math">\(h_t\)</span>，通过<code>output gate</code>来控制<code>cell state</code>中的哪些信息需要
输出，<code>output gate</code>的计算方法如下：
<div class="math">$$o_t=\sigma(W^ox_t+U^oh_{t-1})$$</div>
将<code>cell state</code>中的信息经过一个<span class="math">\(\tanh\)</span>层之后然后经过<code>output gate</code>过滤得到<span class="math">\(h_t\)</span>：
<div class="math">$$h_t=o_t\circ\tanh(c_t)$$</div>
</li>
</ol>
<p>如果我们把<span class="caps">LSTM</span>的<code>forget gate</code>全部置0（总是忘记之前的信息），<code>input gate</code>全部
置1，<code>output gate</code>全部置1（把<code>cell state</code>中的信息全部输出），这样<span class="caps">LSTM</span>就变成一个标准的<span class="caps">RNN</span>。
<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">上文提到的文章</a>中还提到一些<span class="caps">LSTM</span>的变种，比如根据<span class="math">\(h_{t-1}\)</span>、<span class="math">\(x_t\)</span>和<span class="math">\(c_t\)</span>来计算门信息。</p>
<h2><span class="caps">GRU</span></h2>
<p><span class="caps">GRU</span>可以看成是<span class="caps">LSTM</span>的变种，<span class="caps">GRU</span>把<span class="caps">LSTM</span>中的<code>forget gate</code>和<code>input gate</code>用<code>update gate</code>来替代。
把<code>cell state</code>和隐状态<span class="math">\(h_t\)</span>进行合并，在计算当前时刻新信息的方法和<span class="caps">LSTM</span>有所不同。
下图是<span class="caps">GRU</span>更新<span class="math">\(h_t\)</span>的过程：
<img alt="GRU" src="https://wugh.github.io/images/NLP/rnn-gru-unit.png" style="display:block;margin:0 auto" />&nbsp;具体更新过程如下：</p>
<ol>
<li>首先介绍<span class="caps">GRU</span>的两个门，分别是<code>reset gate</code><span class="math">\(r_t\)</span>和<code>update gate</code><span class="math">\(z_t\)</span>，计算方法和<span class="caps">LSTM</span>中
门的计算方法一致：
<div class="math">$$\begin{align*}
r_t&amp;=\sigma(W^rx_t+U^rh_{t-1})\\
z_t&amp;=\sigma(W^zx_t+U^zh_{t-1})
\end{align*}$$</div>
</li>
<li>其次是计算候选隐藏层（candidate hidden layer）<span class="math">\(\tilde{h}_t\)</span>，这个候选隐藏层
和<span class="caps">LSTM</span>中的<span class="math">\(\tilde{c}_t\)</span>是类似，可以看成是当前时刻的新信息，其中<span class="math">\(r_t\)</span>用来控制需要
保留多少之前的记忆，如果<span class="math">\(r_t\)</span>为0，那么<span class="math">\(\tilde{h}_t\)</span>只包含当前词的信息：
<div class="math">$$\tilde{h}_t=\tanh(Wx_t+r_tUh_{t-1})$$</div>
</li>
<li>最后<span class="math">\(z_t\)</span>控制需要从前一时刻的隐藏层<span class="math">\(h_{t-1}\)</span>中遗忘多少信息，需要加入多少当前
时刻的隐藏层信息<span class="math">\(\tilde{h}_t\)</span>，最后得到<span class="math">\(h_t\)</span>，直接得到最后输出的隐藏层信息，
这里与<span class="caps">LSTM</span>的区别是<span class="caps">GRU</span>中没有<code>output gate</code>：
<div class="math">$$h_t=z_t\circ h_{t-1} + (1-z_t)\circ \tilde{h}_t$$</div>
</li>
</ol>
<p>如果<code>reset gate</code>接近0，那么之前的隐藏层信息就会丢弃，允许模型丢弃一些和未来无关
的信息；<code>update gate</code>控制当前时刻的隐藏层输出<span class="math">\(h_t\)</span>需要保留多少之前的隐藏层信息，
若<span class="math">\(z_t\)</span>接近1相当于我们之前把之前的隐藏层信息拷贝到当前时刻，可以学习长距离依赖。
一般来说那些具有短距离依赖的单元<code>reset gate</code>比较活跃（如果<span class="math">\(r_t\)</span>为1，而<span class="math">\(z_t\)</span>为0
那么相当于变成了一个标准的<span class="caps">RNN</span>，能处理短距离依赖），具有长距离依赖的单元<code>update gate</code>比较活跃。</p>
<p><span class="caps">LSTM</span>和<span class="caps">RNN</span>的<code>thenao</code>实现可以参考<a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/">这篇文章</a>。</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        Guohua Wu
    </span>
  </span>
<time datetime="2016-03-13T21:22:00+08:00" pubdate>2016-03-13(Sun)</time>  <span class="categories">
    <a class='category' href='https://wugh.github.io/category/nlp.html'>NLP</a>
  </span>
  <span class="categories">
    <a class="category" href="https://wugh.github.io/tag/shen-jing-wang-luo.html">神经网络</a>,    <a class="category" href="https://wugh.github.io/tag/rnn.html">RNN</a>,    <a class="category" href="https://wugh.github.io/tag/xun-huan-shen-jing-wang-luo.html">循环神经网络</a>,    <a class="category" href="https://wugh.github.io/tag/shen-du-xue-xi.html">深度学习</a>,    <a class="category" href="https://wugh.github.io/tag/gru.html">GRU</a>,    <a class="category" href="https://wugh.github.io/tag/lstm.html">LSTM</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
  </section>
</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/03/cs224d-notes4-recurrent-neural-networks-continue/">CS224d笔记4续——RNN隐藏层计算之GRU和LSTM</a>
      </li>
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/03/cs224d-notes4-recurrent-neural-networks/">CS224d笔记4——语言模型和循环神经网络（Recurrent Neural Network, RNN）</a>
      </li>
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/03/cs224d-notes3-neural-networks-and-backward-propagation/">CS224d笔记3——神经网络</a>
      </li>
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/02/cs224d-notes2-softmax-classification-and-window-classification/">CS224d笔记2——Softmax分类和窗口分类</a>
      </li>
      <li class="post">
          <a href="https://wugh.github.io/posts/2016/02/cs224d-notes1-word2vec/">CS224d笔记1——word2vec</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="https://wugh.github.io/category/linux.html">Linux</a></li>
        <li><a href="https://wugh.github.io/category/machine-learning.html">Machine Learning</a></li>
        <li><a href="https://wugh.github.io/category/nlp.html">NLP</a></li>
        <li><a href="https://wugh.github.io/category/web.html">Web</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="https://wugh.github.io/tag/pelican.html">Pelican</a>,    <a href="https://wugh.github.io/tag/lu-yin.html">录音</a>,    <a href="https://wugh.github.io/tag/pulse.html">pulse</a>,    <a href="https://wugh.github.io/tag/vim.html">vim</a>,    <a href="https://wugh.github.io/tag/ap.html">ap</a>,    <a href="https://wugh.github.io/tag/mp3.html">mp3</a>,    <a href="https://wugh.github.io/tag/mdp.html">MDP</a>,    <a href="https://wugh.github.io/tag/fontconfig.html">fontconfig</a>,    <a href="https://wugh.github.io/tag/lstm.html">LSTM</a>,    <a href="https://wugh.github.io/tag/gru.html">GRU</a>,    <a href="https://wugh.github.io/tag/jinja.html">Jinja</a>,    <a href="https://wugh.github.io/tag/hexo.html">hexo</a>,    <a href="https://wugh.github.io/tag/nei-he-bian-yi.html">内核编译</a>,    <a href="https://wugh.github.io/tag/portage.html">Portage</a>,    <a href="https://wugh.github.io/tag/shen-du-xue-xi.html">深度学习</a>,    <a href="https://wugh.github.io/tag/hou-xiang-chuan-bo.html">后向传播</a>,    <a href="https://wugh.github.io/tag/luan-ma.html">乱码</a>,    <a href="https://wugh.github.io/tag/word2vec.html">word2vec</a>,    <a href="https://wugh.github.io/tag/rnn.html">RNN</a>,    <a href="https://wugh.github.io/tag/gentoo.html">Gentoo</a>,    <a href="https://wugh.github.io/tag/xun-huan-shen-jing-wang-luo.html">循环神经网络</a>,    <a href="https://wugh.github.io/tag/wen-ben-fen-lei.html">文本分类</a>,    <a href="https://wugh.github.io/tag/yu-yan-mo-xing.html">语言模型</a>,    <a href="https://wugh.github.io/tag/shen-jing-wang-luo.html">神经网络</a>,    <a href="https://wugh.github.io/tag/latex.html">latex</a>,    <a href="https://wugh.github.io/tag/svm.html">SVM</a>,    <a href="https://wugh.github.io/tag/hostapd.html">hostapd</a>,    <a href="https://wugh.github.io/tag/ge-shi-zhuan-huan.html">格式转换</a>,    <a href="https://wugh.github.io/tag/maxent.html">MaxEnt</a>,    <a href="https://wugh.github.io/tag/filter.html">Filter</a>,    <a href="https://wugh.github.io/tag/bpsuan-fa.html">bp算法</a>,    <a href="https://wugh.github.io/tag/softmax.html">softmax</a>,    <a href="https://wugh.github.io/tag/chuang-kou-fen-lei.html">窗口分类</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://wugh.github.io/feeds/main.atom.xml" type="application/atom+xml" rel="alternate">Atom</a></li>
            <li><a href="https://github.com/wugh" target="_blank">GitHub</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2013&ndash;2016  Guohua Wu &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="https://wugh.github.io/theme/js/modernizr-2.0.js"></script>
  <script src="https://wugh.github.io/theme/js/ender.js"></script>
  <script src="https://wugh.github.io/theme/js/octopress.js" type="text/javascript"></script>
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-52495737-1', 'auto');

    ga('send', 'pageview');
    </script>
  <script type="text/javascript">
    var disqus_shortname = 'guohuasblog';
    var disqus_identifier = '/posts/2016/03/cs224d-notes4-recurrent-neural-networks-continue/';
    var disqus_url = 'https://wugh.github.io/posts/2016/03/cs224d-notes4-recurrent-neural-networks-continue/';
    var disqus_title = 'CS224d笔记4续——<span class="caps">RNN</span>隐藏层计算之<span class="caps">GRU</span>和<span class="caps">LSTM</span>';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>